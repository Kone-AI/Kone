diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..82e245b
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,74 @@
+# Server Configuration
+PORT=3000
+DEBUG_MODE=false
+MAINTENANCE_MODE=false
+MAINTENANCE_MESSAGE="Service is temporarily under maintenance."
+
+# Rate Limiting
+DISABLE_RATE_LIMIT=false
+RATE_LIMIT_REQUESTS=60
+RATE_LIMIT_WINDOW=60000
+
+# Request Limits
+MAX_TOKENS=350000
+MAX_REQUEST_SIZE=15mb
+MAX_CONCURRENT=50
+REQUEST_TIMEOUT=120000
+
+# Health Checks
+ENABLE_HEALTH_CHECKS=true
+HEALTH_CHECK_INTERVAL=7200  # In seconds (default: 2 hours)
+HEALTH_CHECK_DELAY=3600     # In milliseconds (delay between model tests)
+HEALTH_CHECK_MIN_WORDS=2    # Minimum words for valid response
+
+# API Providers
+# Google
+GOOGLE_API_KEY=
+GOOGLE_API_KEY_BACKUP1=
+GOOGLE_API_KEY_BACKUP2=
+
+# HuggingChat   BROKEN
+ENABLE_HUGGINGCHAT=true
+HUGGINGCHAT_EMAIL=
+HUGGINGCHAT_PASSWORD=
+
+# Mistral
+ENABLE_MISTRAL=true
+MISTRAL_API_KEY=
+
+# Groq
+ENABLE_GROQ=true
+GROQ_API_KEY=
+GROQ_API_KEY_BACKUP1=
+
+# Together.ai
+ENABLE_TOGETHER=true
+TOGETHER_API_KEY=
+
+# Glama.ai  BROKEN
+ENABLE_GLAMA=true
+GLAMA_API_KEY=
+
+# HackClub
+ENABLE_HACKCLUB=true
+HACKCLUB_API_KEY=
+
+# Optional Providers (Disabled by default)
+ENABLE_OPENROUTER=false
+ENABLE_OPENAI=false
+ENABLE_CLAUDE=false
+ENABLE_DEEPSEEK=false
+ENABLE_PERPLEXITY=false
+ENABLE_CEREBRAS=false
+ENABLE_FIREWORKS=false
+ENABLE_DEEPINFRA=false
+ENABLE_COPILOT_MORE=false
+COPILOT_MORE_API_URL=https://web.archive.org/web/20250201202656/https://github.com/jjleng/copilot-more
+
+# Access Control
+REQUIRE_API_KEY=false
+ALLOWED_ORIGINS=*
+
+# Logging
+ENABLE_LOGGING=true
+LOG_LEVEL=info  # debug, info, warn, error
\ No newline at end of file
diff --git a/.gitignore b/.gitignore
index fc4e774..27335ad 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,133 +1,39 @@
-# Logs
-logs
-*.log
-npm-debug.log*
-yarn-debug.log*
-yarn-error.log*
-lerna-debug.log*
-.pnpm-debug.log*
-
-# Diagnostic reports (https://nodejs.org/api/report.html)
-report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json
-
-# Runtime data
-pids
-*.pid
-*.seed
-*.pid.lock
-
-# Directory for instrumented libs generated by jscoverage/JSCover
-lib-cov
-
-# Coverage directory used by tools like istanbul
-coverage
-*.lcov
-
-# nyc test coverage
-.nyc_output
-
-# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
-.grunt
-
-# Bower dependency directory (https://bower.io/)
-bower_components
-
-# node-waf configuration
-.lock-wscript
-
-# Compiled binary addons (https://nodejs.org/api/addons.html)
-build/Release
-
-# Dependency directories
+# Dependencies
 node_modules/
-jspm_packages/
-
-# Snowpack dependency directory (https://snowpack.dev/)
-web_modules/
-
-# TypeScript cache
-*.tsbuildinfo
-
-# Optional npm cache directory
-.npm
-
-# Optional eslint cache
-.eslintcache
-
-# Optional stylelint cache
-.stylelintcache
-
-# Microbundle cache
-.rpt2_cache/
-.rts2_cache_cjs/
-.rts2_cache_es/
-.rts2_cache_umd/
+.pnp
+.pnp.js
 
-# Optional REPL history
-.node_repl_history
-
-# Output of 'npm pack'
-*.tgz
-
-# Yarn Integrity file
-.yarn-integrity
-
-# dotenv environment variable files
+# Environment variables
 .env
-.env.development.local
-.env.test.local
-.env.production.local
 .env.local
+.env.*.local
 
-# parcel-bundler cache (https://parceljs.org/)
-.cache
-.parcel-cache
-
-# Next.js build output
-.next
-out
-
-# Nuxt.js build / generate output
-.nuxt
-dist
-
-# Gatsby files
-.cache/
-# Comment in the public line in if your project uses Gatsby and not Next.js
-# https://nextjs.org/blog/next-9-1#public-directory-support
-# public
-
-# vuepress build output
-.vuepress/dist
-
-# vuepress v2.x temp and cache directory
-.temp
-.cache
-
-# Docusaurus cache and generated files
-.docusaurus
-
-# Serverless directories
-.serverless/
-
-# FuseBox cache
-.fusebox/
-
-# DynamoDB Local files
-.dynamodb/
-
-# TernJS port file
-.tern-port
-
-# Stores VSCode versions used for testing VSCode extensions
-.vscode-test
-
-# yarn v2
-.yarn/cache
-.yarn/unplugged
-.yarn/build-state.yml
-.yarn/install-state.gz
-.pnp.*
-
+# Debug logs
+npm-debug.log*
+yarn-debug.log*
+yarn-error.log*
 
+# Data and logs
+data/
+logs/
+
+# Build output
+dist/
+build/
+.next/
+out/
+
+# IDE and editor files
+.idea/
+.vscode/
+*.swp
+*.swo
+.DS_Store
+
+# Project specific
+login_cache
+datalogs
+memory-bank
+src/huggingface_login_cache
+*.com.txt
 .history
\ No newline at end of file
diff --git a/.roomodes b/.roomodes
new file mode 100644
index 0000000..0feaa00
--- /dev/null
+++ b/.roomodes
@@ -0,0 +1,9 @@
+{
+  "customModes": [{
+    "slug": "research-bot",
+    "name": "Research Bot",
+    "roleDefinition": "AI research assistant that can browse the web, read files, run commands, and use MCP tools to gather and synthesize information",
+    "groups": ["read", "edit", "browser", "command", "mcp"],
+    "customInstructions": "Focus on comprehensive research with citations"
+  }]
+}
\ No newline at end of file
diff --git a/README.md b/README.md
index 88a60c1..03a3c14 100644
--- a/README.md
+++ b/README.md
@@ -1,107 +1,83 @@
-# HackClub API Compatibility Layer
+# Sylph API
 
-An OpenAI API-compatible proxy for HackClub AI that allows you to use OpenAI clients with HackClub's free AI service.
+Free API access to AI language models for students and developers in developing countries. Seamlessly integrate with multiple LLMs through a unified, OpenAI-compatible interface.
 
-## How It Works
+A hosted version is available at [ai.minoa.cat](https://ai.minoa.cat)
 
-This proxy server acts as a middleware between your application and HackClub's AI service, translating requests and responses between OpenAI's API format and HackClub's API format.
+## Features
 
-```mermaid
-graph LR
-    Client[Client Application] -->|OpenAI Format Request| Proxy[Proxy Server]
-    Proxy -->|HackClub Format Request| HackClub[HackClub AI API]
-    HackClub -->|HackClub Format Response| Proxy
-    Proxy -->|OpenAI Format Response| Client
-```
-
-## API Endpoints
-
-- `GET /models` - Get list of available models (OpenAI compatible endpoint)
-- `GET /v1/models` - Get list of available models (OpenAI v1 compatible endpoint)
-- `POST /chat/completions` - Create chat completions (main endpoint)
-- `POST /v1/chat/completions` - Create chat completions (OpenAI v1 compatible endpoint)
-- `GET /test-response` - Return a test response to check client compatibility
+- Free access - no credit card required
+- OpenAI-compatible API endpoints
+- Access to multiple model providers
+- Automatic failover between providers
+- Real-time health monitoring
+- Privacy-focused with no data collection
 
-## Getting Started
+## Setup Guide
 
-### Local Development
+1. Clone the repository:
+```bash
+git clone https://github.com/m1noa/Sylph
+cd Sylph
+```
 
-1. Clone the repository
 2. Install dependencies:
-   ```
-   npm install
-   ```
-3. Start the server:
-   ```
-   npm start
-   ```
-4. Visit `http://localhost:3000` for Swagger UI documentation
-
-### Vercel Deployment
-
-This project is ready to deploy to Vercel:
-
-1. Fork this repository
-2. Connect it to your Vercel account
-3. Deploy
-
-The `vercel.json` file is already configured for serverless deployment.
-
-## Usage Example
-
-Using fetch:
-
-```javascript
-fetch('https://ai.minoa.cat/chat/completions', {
-  method: 'POST',
-  headers: {
-    'Content-Type': 'application/json',
-  },
-  body: JSON.stringify({
-    messages: [
-      {
-        role: 'user',
-        content: 'Tell me a joke!'
-      }
-    ]
-  })
-})
-.then(response => response.json())
-.then(data => console.log(data));
+```bash
+npm install
+```
+
+3. Configure environment:
+```bash
+cp .env.example .env
+# Edit .env with your configuration
 ```
 
-Using OpenAI's Node.js client:
+4. Start the server:
+```bash
+# Development
+npm run dev
 
-```javascript
-import OpenAI from 'openai';
+# Production
+npm start
+```
 
-const openai = new OpenAI({
-  baseURL: 'https://ai.minoa.cat',
-  apiKey: 'dummy-key' // API key is ignored but required by the client
-});
+The server will start on port 3000 by default. You can change this in your .env file.
 
-async function main() {
-  const completion = await openai.chat.completions.create({
-    messages: [{ role: 'user', content: 'Tell me a joke!' }],
-  });
+## Using the API
 
-  console.log(completion.choices[0].message.content);
-}
+Basic example:
 
-main();
+```bash
+curl https://ai.minoa.cat/v1/chat/completions \
+  -H "Content-Type: application/json" \
+  -d '{
+    "model": "hackclub/llama-3.3-70b-versatile",
+    "messages": [{"role": "user", "content": "Hello!"}]
+  }'
 ```
 
-## Important Notes
+For full documentation and available models, visit:
+- [API Documentation](https://ai.minoa.cat/docs)
+- [Model List](https://ai.minoa.cat/models)
 
-- HackClub AI's service is for non-commercial, educational purposes only
-- The proxy doesn't add any authentication - it's just as open as HackClub's API
-- The actual model used is dynamically detected from HackClub's homepage
+## Contributing
 
-## License
+Contributions are greatly appreciated! To contribute:
+
+1. Fork the repository
+2. Create a feature branch
+3. Make your changes
+4. Submit a pull request
 
-This project is licensed under the terms of the LICENSE file included in this repository.
+Please ensure your code follows the existing style and includes appropriate tests.
 
-## Credits
+## Support
+
+If you find Sylph helpful, consider:
+- Supporting the project at [ai.minoa.cat/donate](https://ai.minoa.cat/donate)
+- Sharing with others who might benefit
+- Contributing code improvements
+
+## License
 
-- Powered by [ai.hackclub.com](https://ai.hackclub.com)
-- Created by [minoa.cat](https://bio.minoa.cat)
+MIT License - See [LICENSE](LICENSE) for details.
diff --git a/docs.json b/docs.json
deleted file mode 100644
index 7fb1b04..0000000
--- a/docs.json
+++ /dev/null
@@ -1,320 +0,0 @@
-{
-  "openapi": "3.0.0",
-  "info": {
-    "title": "HackClub AI API Proxy",
-    "description": "OpenAI API compatible proxy for HackClub AI\n\nPowered by [ai.hackclub.com](https://ai.hackclub.com)\nProxy created by [Minoa.cat](https://minoa.cat)",
-    "version": "1.0.1"
-  },
-  "servers": [
-    {
-      "url": "https://ai.minoa.cat",
-      "description": "The API server",
-      "variables": {
-        "host": {
-          "default": "ai.minoa.cat",
-          "description": "The deployment host"
-        }
-      }
-    },
-    {
-      "url": "http://localhost:3000",
-      "description": "Local development server"
-    }
-  ],
-  "security": [
-    {
-      "ApiKeyAuth": []
-    }
-  ],
-  "components": {
-    "securitySchemes": {
-      "ApiKeyAuth": {
-        "type": "apiKey",
-        "in": "header",
-        "name": "Authorization",
-        "description": "API key is optional and will be ignored"
-      }
-    }
-  },
-  "paths": {
-    "/models": {
-      "get": {
-        "summary": "Get available models",
-        "description": "Returns a list of available models in OpenAI format",
-        "responses": {
-          "200": {
-            "description": "Successful response",
-            "content": {
-              "application/json": {
-                "example": {
-                  "object": "list",
-                  "data": [
-                    {
-                      "id": "llama-3.3-70b-versatile",
-                      "object": "model",
-                      "created": 1677652288,
-                      "owned_by": "hackclub",
-                      "permission": [],
-                      "root": "llama-3.3-70b-versatile",
-                      "parent": null
-                    }
-                  ]
-                }
-              }
-            }
-          }
-        }
-      }
-    },
-    "/v1/models": {
-      "get": {
-        "summary": "Get available models (OpenAI v1 endpoint)",
-        "description": "OpenAI API v1 compatibility endpoint for listing available models",
-        "responses": {
-          "200": {
-            "description": "Successful response",
-            "content": {
-              "application/json": {
-                "example": {
-                  "object": "list",
-                  "data": [
-                    {
-                      "id": "llama-3.3-70b-versatile",
-                      "object": "model",
-                      "created": 1677652288,
-                      "owned_by": "hackclub",
-                      "permission": [],
-                      "root": "llama-3.3-70b-versatile",
-                      "parent": null
-                    }
-                  ]
-                }
-              }
-            }
-          }
-        }
-      }
-    },
-    "/chat/completions": {
-      "post": {
-        "summary": "Create chat completion",
-        "description": "Creates a completion for the chat message",
-        "requestBody": {
-          "required": true,
-          "content": {
-            "application/json": {
-              "schema": {
-                "type": "object",
-                "properties": {
-                  "messages": {
-                    "type": "array",
-                    "description": "List of messages comprising the conversation so far",
-                    "items": {
-                      "type": "object",
-                      "required": ["role", "content"],
-                      "properties": {
-                        "role": {
-                          "type": "string",
-                          "enum": ["system", "user", "assistant"],
-                          "description": "The role of the message author"
-                        },
-                        "content": {
-                          "type": "string",
-                          "description": "The content of the message"
-                        }
-                      }
-                    }
-                  },
-                  "model": {
-                    "type": "string",
-                    "description": "ID of the model to use (will be ignored, but accepted for compatibility)"
-                  },
-                  "temperature": {
-                    "type": "number",
-                    "format": "float",
-                    "minimum": 0,
-                    "maximum": 2,
-                    "description": "Controls randomness. Higher values make output more random, lower values make it more deterministic"
-                  },
-                  "max_tokens": {
-                    "type": "integer",
-                    "minimum": 1,
-                    "description": "Maximum number of tokens to generate"
-                  },
-                  "api_key": {
-                    "type": "string",
-                    "description": "Optional API key (will be ignored, but accepted for compatibility)"
-                  }
-                }
-              },
-              "example": {
-                "messages": [
-                  {
-                    "role": "user",
-                    "content": "Tell me a joke!"
-                  }
-                ]
-              }
-            }
-          }
-        },
-        "responses": {
-          "200": {
-            "description": "Successful response",
-            "content": {
-              "application/json": {
-                "example": {
-                  "id": "chatcmpl-123abc",
-                  "object": "chat.completion",
-                  "created": 1677652288,
-                  "model": "gpt-3.5-turbo",
-                  "choices": [{
-                    "index": 0,
-                    "message": {
-                      "role": "assistant",
-                      "content": "Why don't programmers like nature? It has too many bugs!"
-                    },
-                    "finish_reason": "stop"
-                  }],
-                  "usage": {
-                    "prompt_tokens": 10,
-                    "completion_tokens": 20,
-                    "total_tokens": 30
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-    },
-    "/v1/chat/completions": {
-      "post": {
-        "summary": "Create chat completion (OpenAI v1 compatibility)",
-        "description": "OpenAI API v1 compatibility endpoint, identical to /chat/completions",
-        "requestBody": {
-          "required": true,
-          "content": {
-            "application/json": {
-              "schema": {
-                "type": "object",
-                "properties": {
-                  "messages": {
-                    "type": "array",
-                    "description": "List of messages comprising the conversation so far",
-                    "items": {
-                      "type": "object",
-                      "required": ["role", "content"],
-                      "properties": {
-                        "role": {
-                          "type": "string",
-                          "enum": ["system", "user", "assistant"],
-                          "description": "The role of the message author"
-                        },
-                        "content": {
-                          "type": "string",
-                          "description": "The content of the message"
-                        }
-                      }
-                    }
-                  },
-                  "model": {
-                    "type": "string",
-                    "description": "ID of the model to use (will be ignored, but accepted for compatibility)"
-                  },
-                  "temperature": {
-                    "type": "number",
-                    "format": "float",
-                    "minimum": 0,
-                    "maximum": 2,
-                    "description": "Controls randomness. Higher values make output more random, lower values make it more deterministic"
-                  },
-                  "max_tokens": {
-                    "type": "integer",
-                    "minimum": 1,
-                    "description": "Maximum number of tokens to generate"
-                  },
-                  "api_key": {
-                    "type": "string",
-                    "description": "Optional API key (will be ignored, but accepted for compatibility)"
-                  }
-                }
-              },
-              "example": {
-                "model": "gpt-3.5-turbo",
-                "messages": [
-                  {
-                    "role": "user",
-                    "content": "Tell me a joke!"
-                  }
-                ]
-              }
-            }
-          }
-        },
-        "responses": {
-          "200": {
-            "description": "Successful response",
-            "content": {
-              "application/json": {
-                "example": {
-                  "id": "chatcmpl-123abc",
-                  "object": "chat.completion",
-                  "created": 1677652288,
-                  "model": "gpt-3.5-turbo",
-                  "choices": [{
-                    "index": 0,
-                    "message": {
-                      "role": "assistant",
-                      "content": "Why don't programmers like nature? It has too many bugs!"
-                    },
-                    "finish_reason": "stop"
-                  }],
-                  "usage": {
-                    "prompt_tokens": 10,
-                    "completion_tokens": 20,
-                    "total_tokens": 30
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-    },
-    "/test-response": {
-      "get": {
-        "summary": "Test endpoint",
-        "description": "Returns a test response in OpenAI format to verify client compatibility",
-        "responses": {
-          "200": {
-            "description": "Successful response",
-            "content": {
-              "application/json": {
-                "example": {
-                  "id": "chatcmpl-123abc",
-                  "object": "chat.completion",
-                  "created": 1677652288,
-                  "model": "llama-3.3-70b-versatile",
-                  "choices": [{
-                    "index": 0,
-                    "message": {
-                      "role": "assistant",
-                      "content": "This is a test response in standard OpenAI format. Your client should be able to parse and display this message properly."
-                    },
-                    "finish_reason": "stop"
-                  }],
-                  "usage": {
-                    "prompt_tokens": 0,
-                    "completion_tokens": 25,
-                    "total_tokens": 25
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-    }
-  }
-}
diff --git a/huggingface.md b/huggingface.md
new file mode 100644
index 0000000..5a505d0
--- /dev/null
+++ b/huggingface.md
@@ -0,0 +1,415 @@
+## Functions
+
+<dl>
+<dt><a href="#intialize">intialize()</a> â‡’ <code>Promise.&lt;void&gt;</code></dt>
+<dd><p>Initializes the ChatBot instance.</p>
+</dd>
+<dt><a href="#switchModel">switchModel(value)</a></dt>
+<dd><p>Switches the current model for the chat.</p>
+</dd>
+<dt><a href="#listAvilableModels">listAvilableModels()</a> â‡’ <code>Array.&lt;Model&gt;</code></dt>
+<dd><p>Lists available models that can be used with the chat.</p>
+</dd>
+<dt><a href="#listAvilableSesson">listAvilableSesson()</a> â‡’ <code>Array.&lt;Sesson&gt;</code></dt>
+<dd><p>Lists available sessions for the chat.</p>
+</dd>
+<dt><a href="#showCurrentModel">showCurrentModel()</a> â‡’ <code>Model</code> | <code>null</code></dt>
+<dd><p>Returns the currently selected model for the chat.</p>
+</dd>
+<dt><a href="#getRemoteConversations">getRemoteConversations()</a> â‡’ <code>Promise.&lt;Array.&lt;Sesson&gt;&gt;</code></dt>
+<dd><p>Fetches remote conversations from a server.</p>
+</dd>
+<dt><a href="#getRemoteLlms">getRemoteLlms()</a> â‡’ <code>Promise.&lt;Array.&lt;Model&gt;&gt;</code></dt>
+<dd><p>Fetches remote LLMs from a server.</p>
+</dd>
+<dt><a href="#getNewChat">getNewChat()</a> â‡’ <code>Promise.&lt;Conversation&gt;</code></dt>
+<dd><p>Initializes a new chat conversation.</p>
+</dd>
+<dt><a href="#chat">chat(text, currentConversionID, options)</a> â‡’ <code>Promise.&lt;ChatResponse&gt;</code></dt>
+<dd><p>Initiates a chat with the provided text.</p>
+</dd>
+<dt><a href="#getConversationHistory">getConversationHistory()</a> â‡’ <code>Promise.&lt;Conversation&gt;</code></dt>
+<dd><p>get the details of current conversation</p>
+</dd>
+</dl>
+
+<a name="intialize"></a>
+
+## intialize() â‡’ <code>Promise.&lt;void&gt;</code>
+Initializes the ChatBot instance.
+
+**Kind**: global function  
+<a name="switchModel"></a>
+
+## switchModel(value)
+Switches the current model for the chat.
+
+**Kind**: global function  
+**Throws**:
+
+- <code>Error</code> If the provided model ID is not a string or if the model is not found.
+
+
+| Param | Type | Description |
+| --- | --- | --- |
+| value | <code>string</code> | The ID of the model to switch to. |
+
+<a name="listAvilableModels"></a>
+
+## listAvilableModels() â‡’ <code>Array.&lt;Model&gt;</code>
+Lists available models that can be used with the chat.
+
+**Kind**: global function  
+**Returns**: <code>Array.&lt;Model&gt;</code> - An array of available model names.  
+<a name="listAvilableSesson"></a>
+
+## listAvilableSesson() â‡’ <code>Array.&lt;Sesson&gt;</code>
+Lists available sessions for the chat.
+
+**Kind**: global function  
+**Returns**: <code>Array.&lt;Sesson&gt;</code> - An array of available sessions.  
+<a name="showCurrentModel"></a>
+
+## showCurrentModel() â‡’ <code>Model</code> \| <code>null</code>
+Returns the currently selected model for the chat.
+
+**Kind**: global function  
+**Returns**: <code>Model</code> \| <code>null</code> - The current model.  
+<a name="getRemoteConversations"></a>
+
+## getRemoteConversations() â‡’ <code>Promise.&lt;Array.&lt;Sesson&gt;&gt;</code>
+Fetches remote conversations from a server.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;Array.&lt;Sesson&gt;&gt;</code> - A promise that resolves to an array of fetched conversations.  
+**Throws**:
+
+- <code>Error</code> If the server response is not successful.
+
+<a name="getRemoteLlms"></a>
+
+## getRemoteLlms() â‡’ <code>Promise.&lt;Array.&lt;Model&gt;&gt;</code>
+Fetches remote LLMs from a server.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;Array.&lt;Model&gt;&gt;</code> - A promise that resolves to an array of fetched conversations.  
+**Throws**:
+
+- <code>Error</code> If the server response is not successful.
+
+<a name="getNewChat"></a>
+
+## getNewChat() â‡’ <code>Promise.&lt;Conversation&gt;</code>
+Initializes a new chat conversation.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;Conversation&gt;</code> - The conversation ID of the new chat.  
+**Throws**:
+
+- <code>Error</code> If the creation of a new conversation fails.
+
+<a name="chat"></a>
+
+## chat(text, currentConversionID, options) â‡’ <code>Promise.&lt;ChatResponse&gt;</code>
+Initiates a chat with the provided text.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;ChatResponse&gt;</code> - An object containing conversation details.  
+**Throws**:
+
+- <code>Error</code> If there is an issue with the chat request.
+
+
+| Param | Type | Description |
+| --- | --- | --- |
+| text | <code>string</code> | The user's input text or prompt. |
+| currentConversionID | <code>string</code> | The conversation ID for the current chat. |
+| options | <code>ChatOptions</code> |  |
+
+<a name="getConversationHistory"></a>
+
+## getConversationHistory() â‡’ <code>Promise.&lt;Conversation&gt;</code>
+get the details of current conversation
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;Conversation&gt;</code> - A Promise that return conversation details  
+**Throws**:
+
+- <code>Error</code> If there is an api error
+
+
+
+
+## Functions
+
+<dl>
+<dt><a href="#parseCookies">parseCookies()</a> â‡’ <code>string</code></dt>
+<dd><p>Parses cookies into a formatted string.</p>
+</dd>
+<dt><a href="#get">get(url, _parms)</a> â‡’ <code>Promise.&lt;AxiosResponse&gt;</code></dt>
+<dd><p>Sends an HTTP GET request.</p>
+</dd>
+<dt><a href="#post">post(url, data, _headers)</a> â‡’ <code>Promise.&lt;AxiosResponse&gt;</code></dt>
+<dd><p>Sends an HTTP POST request.</p>
+</dd>
+<dt><a href="#refreshCookies">refreshCookies(response)</a></dt>
+<dd><p>Refreshes cookies based on the response headers.</p>
+</dd>
+<dt><a href="#signinWithEmail">signinWithEmail()</a></dt>
+<dd><p>Attempts to sign in with the provided email and password.</p>
+</dd>
+<dt><a href="#getAuthUrl">getAuthUrl()</a> â‡’ <code>Promise.&lt;string&gt;</code></dt>
+<dd><p>Retrieves the authentication URL for a chat.</p>
+</dd>
+<dt><a href="#getCrpf">getCrpf(input)</a> â‡’ <code>string</code> | <code>null</code></dt>
+<dd><p>Extracts CSRF token from a string.</p>
+</dd>
+<dt><a href="#grantAuth">grantAuth(url)</a> â‡’ <code>Promise.&lt;number&gt;</code></dt>
+<dd><p>Grants authorization by following redirects.</p>
+</dd>
+<dt><a href="#login">login(cache_path)</a> â‡’ <code>Promise.&lt;string&gt;</code></dt>
+<dd><p>Initiates the login process.</p>
+</dd>
+<dt><a href="#cacheLogin">cacheLogin(path)</a></dt>
+<dd><p>Caches login data to a file.</p>
+</dd>
+<dt><a href="#loadLoginCache">loadLoginCache(path)</a> â‡’ <code>Promise.&lt;string&gt;</code></dt>
+<dd><p>Loads cached login data from a file.</p>
+</dd>
+</dl>
+
+<a name="parseCookies"></a>
+
+## parseCookies() â‡’ <code>string</code>
+Parses cookies into a formatted string.
+
+**Kind**: global function  
+**Returns**: <code>string</code> - A formatted string containing parsed cookies.  
+<a name="get"></a>
+
+## get(url, _parms) â‡’ <code>Promise.&lt;AxiosResponse&gt;</code>
+Sends an HTTP GET request.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;AxiosResponse&gt;</code> - A Promise that resolves to the HTTP response.  
+
+| Param | Type | Description |
+| --- | --- | --- |
+| url | <code>string</code> | The URL to send the GET request to. |
+| _parms | <code>Record.&lt;string, any&gt;</code> | Optional query parameters for the request. |
+
+<a name="post"></a>
+
+## post(url, data, _headers) â‡’ <code>Promise.&lt;AxiosResponse&gt;</code>
+Sends an HTTP POST request.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;AxiosResponse&gt;</code> - A Promise that resolves to the HTTP response.  
+
+| Param | Type | Description |
+| --- | --- | --- |
+| url | <code>string</code> | The URL to send the POST request to. |
+| data | <code>Record.&lt;string, any&gt;</code> | Data to include in the request body. |
+| _headers | <code>Record.&lt;string, any&gt;</code> | Optional additional headers for the request. |
+
+<a name="refreshCookies"></a>
+
+## refreshCookies(response)
+Refreshes cookies based on the response headers.
+
+**Kind**: global function  
+
+| Param | Type | Description |
+| --- | --- | --- |
+| response | <code>AxiosResponse</code> | The HTTP response to extract cookies from. |
+
+<a name="signinWithEmail"></a>
+
+## signinWithEmail()
+Attempts to sign in with the provided email and password.
+
+**Kind**: global function  
+**Throws**:
+
+- <code>Error</code> If the sign-in fails.
+
+<a name="getAuthUrl"></a>
+
+## getAuthUrl() â‡’ <code>Promise.&lt;string&gt;</code>
+Retrieves the authentication URL for a chat.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;string&gt;</code> - A Promise that resolves to the authentication URL.  
+**Throws**:
+
+- <code>Error</code> If the URL retrieval fails.
+
+<a name="getCrpf"></a>
+
+## getCrpf(input) â‡’ <code>string</code> \| <code>null</code>
+Extracts CSRF token from a string.
+
+**Kind**: global function  
+**Returns**: <code>string</code> \| <code>null</code> - The extracted CSRF token or null if not found.  
+
+| Param | Type | Description |
+| --- | --- | --- |
+| input | <code>string</code> | The input string containing CSRF information. |
+
+<a name="grantAuth"></a>
+
+## grantAuth(url) â‡’ <code>Promise.&lt;number&gt;</code>
+Grants authorization by following redirects.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;number&gt;</code> - A Promise that resolves to a status code.  
+**Throws**:
+
+- <code>Error</code> If the authorization process fails.
+
+
+| Param | Type | Description |
+| --- | --- | --- |
+| url | <code>string</code> | The URL to grant authorization for. |
+
+<a name="login"></a>
+
+## login(cache_path) â‡’ <code>Promise.&lt;string&gt;</code>
+Initiates the login process.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;string&gt;</code> - A Promise that resolves to the parsed cookies.  
+**Throws**:
+
+- <code>Error</code> If the login process fails.
+
+
+| Param | Type | Description |
+| --- | --- | --- |
+| cache_path | <code>string</code> | Optional path for caching login data. |
+
+<a name="cacheLogin"></a>
+
+## cacheLogin(path)
+Caches login data to a file.
+
+**Kind**: global function  
+
+| Param | Type | Description |
+| --- | --- | --- |
+| path | <code>string</code> | The path where login data will be cached. |
+
+<a name="loadLoginCache"></a>
+
+## loadLoginCache(path) â‡’ <code>Promise.&lt;string&gt;</code>
+Loads cached login data from a file.
+
+**Kind**: global function  
+**Returns**: <code>Promise.&lt;string&gt;</code> - A Promise that resolves to the cached login data.  
+
+| Param | Type | Description |
+| --- | --- | --- |
+| path | <code>string</code> | The path to the cached login data file. |
+
+
+
+
+
+**Deprecation Notice**
+
+> The versions 2.x and lower are deprecated please use latest.
+ 
+# Huggingface chat api 
+A simple api for hugging face chat with login caching.
+
+## Installation
+
+Current stable release (`4.x`) 
+> Added tools support ðŸŽ‰
+
+```sh
+npm i huggingface-chat
+``` 
+
+
+## Example usage 
+```js
+
+import { Login ,ChatBot} from "huggingface-chat";
+
+const EMAIL = "email"
+const PASSWD = "password"
+const cachePath = "./login_cache/"
+const signin = new Login(EMAIL, PASSWD)
+const res = await signin.login(cachePath) // default path is ./login_cache/
+const chat = new ChatBot(res) // res is cookies which is required for subsequent aip calls
+
+await chat.intialize()
+
+const models = chat.listAvilableModels()
+console.log(models)
+
+
+const sessons = chat.listAvilableSesson()
+console.log(sessons)
+
+// more info : https://huggingface.co/chat/models
+let currentModel = chat.showCurrentModel()
+console.log(currentModel)
+
+
+chat.switchModel("microsoft/Phi-3.5-mini-instruct")
+
+currentModel = chat.showCurrentModel()
+console.log(currentModel)
+
+const currentChat = await chat.getNewChat("you are a drunk person") // optional if you want to set a system prompt
+console.log(currentChat)
+
+const tools = await chat.getToolList("1") // for the sake of not overloading the api the tools need to be called when needed also pass the page number more info : https://huggingface.co/chat/tools
+console.log(tools)
+
+let data  = await chat.chat("take screenshoot of this website : google.com", undefined, {
+	tools:["000000000000000000000001","66e99753cb638fb7e2342da5"], // pass the tools id tools[0].id
+	rawResponse:true
+}); 
+
+let  reader  =  data.stream.getReader();
+while (true) {
+	const  {  done,  value  }  =  await  reader.read();
+	if (done) break;  // The streaming has ended.
+	process.stdout.write(value)
+}
+
+
+data = await chat.chat("what is my name"); 
+let response = await data.completeResponsePromise() //non streaming response 
+console.log(response)
+
+data = await chat.chat("what is my name", sessons[0].id); // using existing sessons
+response = await data.completeResponsePromise()
+console.log(response)
+
+
+
+```
+
+
+>Note: Supported in node 18.x and higher.
+
+>Note: In case the package stops working there is most likely a change in hugging face api, if possible please report it and update the package to latest if available.
+
+## Documentations
+
+Full API documentations of both classes can be found here [Chat](./docs/chat.md) [Login](./docs/login.md)
+
+
+## Contributions
+
+- If you happen to see missing feature or a bug, feel free to open an [issue](https://github.com/rahulsushilsharma/huggingface-chat/issues).
+- Pull requests are welcomed too!
+
+## License
+
+[MIT](LICENSE.md)
+
diff --git a/keys.txt b/keys.txt
new file mode 100644
index 0000000..0ca72ed
--- /dev/null
+++ b/keys.txt
@@ -0,0 +1,11 @@
+bla
+blabbllalbla
+blalblllblalbla
+blahblahblahbhablablalbla
+heh
+meow
+123345656677876543256789654323
+57689y0
+4235w6e7r8uyhgfr
+mroe
+meowmeowmew
\ No newline at end of file
diff --git a/package-lock.json b/package-lock.json
index 4812c5d..767d7e6 100644
--- a/package-lock.json
+++ b/package-lock.json
@@ -1,23 +1,26 @@
 {
-  "name": "hackclub-api-proxy",
+  "name": "sylph-api",
   "version": "1.0.0",
   "lockfileVersion": 3,
   "requires": true,
   "packages": {
     "": {
-      "name": "hackclub-api-proxy",
+      "name": "sylph-api",
       "version": "1.0.0",
+      "license": "MIT",
       "dependencies": {
+        "cheerio": "^1.0.0-rc.12",
         "cors": "^2.8.5",
-        "express": "^4.18.2",
-        "node-fetch": "^2.6.9",
+        "dotenv": "^16.0.3",
+        "express": "^4.21.2",
+        "express-rate-limit": "^6.11.2",
+        "huggingface-chat": "latest",
+        "morgan": "^1.10.0",
+        "openai": "^4.0.0",
         "swagger-ui-express": "^4.6.2"
       },
       "devDependencies": {
         "nodemon": "^2.0.22"
-      },
-      "engines": {
-        "node": ">=14.0.0"
       }
     },
     "node_modules/@scarf/scarf": {
@@ -27,6 +30,37 @@
       "hasInstallScript": true,
       "license": "Apache-2.0"
     },
+    "node_modules/@types/node": {
+      "version": "18.19.84",
+      "resolved": "https://registry.npmjs.org/@types/node/-/node-18.19.84.tgz",
+      "integrity": "sha512-ACYy2HGcZPHxEeWTqowTF7dhXN+JU1o7Gr4b41klnn6pj2LD6rsiGqSZojMdk1Jh2ys3m76ap+ae1vvE4+5+vg==",
+      "license": "MIT",
+      "dependencies": {
+        "undici-types": "~5.26.4"
+      }
+    },
+    "node_modules/@types/node-fetch": {
+      "version": "2.6.12",
+      "resolved": "https://registry.npmjs.org/@types/node-fetch/-/node-fetch-2.6.12.tgz",
+      "integrity": "sha512-8nneRWKCg3rMtF69nLQJnOYUcbafYeFSjqkw3jCRLsqkWFlHaoQrr5mXmofFGOx3DKn7UfmBMyov8ySvLRVldA==",
+      "license": "MIT",
+      "dependencies": {
+        "@types/node": "*",
+        "form-data": "^4.0.0"
+      }
+    },
+    "node_modules/abort-controller": {
+      "version": "3.0.0",
+      "resolved": "https://registry.npmjs.org/abort-controller/-/abort-controller-3.0.0.tgz",
+      "integrity": "sha512-h8lQ8tacZYnR3vNQTgibj+tODHI5/+l06Au2Pcriv/Gmet0eaj4TwWH41sO9wnHDiQsEj19q0drzdWdeAHtweg==",
+      "license": "MIT",
+      "dependencies": {
+        "event-target-shim": "^5.0.0"
+      },
+      "engines": {
+        "node": ">=6.5"
+      }
+    },
     "node_modules/accepts": {
       "version": "1.3.8",
       "resolved": "https://registry.npmjs.org/accepts/-/accepts-1.3.8.tgz",
@@ -40,6 +74,18 @@
         "node": ">= 0.6"
       }
     },
+    "node_modules/agentkeepalive": {
+      "version": "4.6.0",
+      "resolved": "https://registry.npmjs.org/agentkeepalive/-/agentkeepalive-4.6.0.tgz",
+      "integrity": "sha512-kja8j7PjmncONqaTsB8fQ+wE2mSU2DJ9D4XKoJ5PFWIdRMa6SLSN1ff4mOr4jCbfRSsxR4keIiySJU0N9T5hIQ==",
+      "license": "MIT",
+      "dependencies": {
+        "humanize-ms": "^1.2.1"
+      },
+      "engines": {
+        "node": ">= 8.0.0"
+      }
+    },
     "node_modules/anymatch": {
       "version": "3.1.3",
       "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-3.1.3.tgz",
@@ -60,6 +106,23 @@
       "integrity": "sha512-PCVAQswWemu6UdxsDFFX/+gVeYqKAod3D3UVm91jHwynguOwAvYPhx8nNlM++NqRcK6CxxpUafjmhIdKiHibqg==",
       "license": "MIT"
     },
+    "node_modules/asynckit": {
+      "version": "0.4.0",
+      "resolved": "https://registry.npmjs.org/asynckit/-/asynckit-0.4.0.tgz",
+      "integrity": "sha512-Oei9OH4tRh0YqU3GxhX79dM/mwVgvbZJaSNaRk+bshkj0S5cfHcgYakreBjrHwatXKbz+IoIdYLxrKim2MjW0Q==",
+      "license": "MIT"
+    },
+    "node_modules/axios": {
+      "version": "1.8.4",
+      "resolved": "https://registry.npmjs.org/axios/-/axios-1.8.4.tgz",
+      "integrity": "sha512-eBSYY4Y68NNlHbHBMdeDmKNtDgXWhQsJcGqzO3iLUM0GraQFSS9cVgPX5I9b3lbdFKyYoAEGAZF1DwhTaljNAw==",
+      "license": "MIT",
+      "dependencies": {
+        "follow-redirects": "^1.15.6",
+        "form-data": "^4.0.0",
+        "proxy-from-env": "^1.1.0"
+      }
+    },
     "node_modules/balanced-match": {
       "version": "1.0.2",
       "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
@@ -67,6 +130,24 @@
       "dev": true,
       "license": "MIT"
     },
+    "node_modules/basic-auth": {
+      "version": "2.0.1",
+      "resolved": "https://registry.npmjs.org/basic-auth/-/basic-auth-2.0.1.tgz",
+      "integrity": "sha512-NF+epuEdnUYVlGuhaxbbq+dvJttwLnGY+YixlXlME5KpQ5W3CnXA5cVTneY3SPbPDRkcjMbifrwmFYcClgOZeg==",
+      "license": "MIT",
+      "dependencies": {
+        "safe-buffer": "5.1.2"
+      },
+      "engines": {
+        "node": ">= 0.8"
+      }
+    },
+    "node_modules/basic-auth/node_modules/safe-buffer": {
+      "version": "5.1.2",
+      "resolved": "https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.1.2.tgz",
+      "integrity": "sha512-Gd2UZBJDkXlY7GbJxfsE8/nvKkUEU1G38c1siN6QP6a9PT9MmHB8GnpscSmMJSoF8LOIrt8ud/wPtojys4G6+g==",
+      "license": "MIT"
+    },
     "node_modules/binary-extensions": {
       "version": "2.3.0",
       "resolved": "https://registry.npmjs.org/binary-extensions/-/binary-extensions-2.3.0.tgz",
@@ -104,6 +185,24 @@
         "npm": "1.2.8000 || >= 1.4.16"
       }
     },
+    "node_modules/body-parser/node_modules/iconv-lite": {
+      "version": "0.4.24",
+      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz",
+      "integrity": "sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==",
+      "license": "MIT",
+      "dependencies": {
+        "safer-buffer": ">= 2.1.2 < 3"
+      },
+      "engines": {
+        "node": ">=0.10.0"
+      }
+    },
+    "node_modules/boolbase": {
+      "version": "1.0.0",
+      "resolved": "https://registry.npmjs.org/boolbase/-/boolbase-1.0.0.tgz",
+      "integrity": "sha512-JZOSA7Mo9sNGB8+UjSgzdLtokWAky1zbztM3WRLCbZ70/3cTANmQmOdR7y2g+J0e2WXywy1yS468tY+IruqEww==",
+      "license": "ISC"
+    },
     "node_modules/brace-expansion": {
       "version": "1.1.11",
       "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.11.tgz",
@@ -166,6 +265,48 @@
         "url": "https://github.com/sponsors/ljharb"
       }
     },
+    "node_modules/cheerio": {
+      "version": "1.0.0",
+      "resolved": "https://registry.npmjs.org/cheerio/-/cheerio-1.0.0.tgz",
+      "integrity": "sha512-quS9HgjQpdaXOvsZz82Oz7uxtXiy6UIsIQcpBj7HRw2M63Skasm9qlDocAM7jNuaxdhpPU7c4kJN+gA5MCu4ww==",
+      "license": "MIT",
+      "dependencies": {
+        "cheerio-select": "^2.1.0",
+        "dom-serializer": "^2.0.0",
+        "domhandler": "^5.0.3",
+        "domutils": "^3.1.0",
+        "encoding-sniffer": "^0.2.0",
+        "htmlparser2": "^9.1.0",
+        "parse5": "^7.1.2",
+        "parse5-htmlparser2-tree-adapter": "^7.0.0",
+        "parse5-parser-stream": "^7.1.2",
+        "undici": "^6.19.5",
+        "whatwg-mimetype": "^4.0.0"
+      },
+      "engines": {
+        "node": ">=18.17"
+      },
+      "funding": {
+        "url": "https://github.com/cheeriojs/cheerio?sponsor=1"
+      }
+    },
+    "node_modules/cheerio-select": {
+      "version": "2.1.0",
+      "resolved": "https://registry.npmjs.org/cheerio-select/-/cheerio-select-2.1.0.tgz",
+      "integrity": "sha512-9v9kG0LvzrlcungtnJtpGNxY+fzECQKhK4EGJX2vByejiMX84MFNQw4UxPJl3bFbTMw+Dfs37XaIkCwTZfLh4g==",
+      "license": "BSD-2-Clause",
+      "dependencies": {
+        "boolbase": "^1.0.0",
+        "css-select": "^5.1.0",
+        "css-what": "^6.1.0",
+        "domelementtype": "^2.3.0",
+        "domhandler": "^5.0.3",
+        "domutils": "^3.0.1"
+      },
+      "funding": {
+        "url": "https://github.com/sponsors/fb55"
+      }
+    },
     "node_modules/chokidar": {
       "version": "3.6.0",
       "resolved": "https://registry.npmjs.org/chokidar/-/chokidar-3.6.0.tgz",
@@ -191,6 +332,18 @@
         "fsevents": "~2.3.2"
       }
     },
+    "node_modules/combined-stream": {
+      "version": "1.0.8",
+      "resolved": "https://registry.npmjs.org/combined-stream/-/combined-stream-1.0.8.tgz",
+      "integrity": "sha512-FQN4MRfuJeHf7cBbBMJFXhKSDq+2kAArBlmRBvcvFE5BB1HZKXtSFASDhdlz9zOYwxh8lDdnvmMOe/+5cdoEdg==",
+      "license": "MIT",
+      "dependencies": {
+        "delayed-stream": "~1.0.0"
+      },
+      "engines": {
+        "node": ">= 0.8"
+      }
+    },
     "node_modules/concat-map": {
       "version": "0.0.1",
       "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz",
@@ -247,6 +400,34 @@
         "node": ">= 0.10"
       }
     },
+    "node_modules/css-select": {
+      "version": "5.1.0",
+      "resolved": "https://registry.npmjs.org/css-select/-/css-select-5.1.0.tgz",
+      "integrity": "sha512-nwoRF1rvRRnnCqqY7updORDsuqKzqYJ28+oSMaJMMgOauh3fvwHqMS7EZpIPqK8GL+g9mKxF1vP/ZjSeNjEVHg==",
+      "license": "BSD-2-Clause",
+      "dependencies": {
+        "boolbase": "^1.0.0",
+        "css-what": "^6.1.0",
+        "domhandler": "^5.0.2",
+        "domutils": "^3.0.1",
+        "nth-check": "^2.0.1"
+      },
+      "funding": {
+        "url": "https://github.com/sponsors/fb55"
+      }
+    },
+    "node_modules/css-what": {
+      "version": "6.1.0",
+      "resolved": "https://registry.npmjs.org/css-what/-/css-what-6.1.0.tgz",
+      "integrity": "sha512-HTUrgRJ7r4dsZKU6GjmpfRK1O76h97Z8MfS1G0FozR+oF2kG6Vfe8JE6zwrkbxigziPHinCJ+gCPjA9EaBDtRw==",
+      "license": "BSD-2-Clause",
+      "engines": {
+        "node": ">= 6"
+      },
+      "funding": {
+        "url": "https://github.com/sponsors/fb55"
+      }
+    },
     "node_modules/debug": {
       "version": "2.6.9",
       "resolved": "https://registry.npmjs.org/debug/-/debug-2.6.9.tgz",
@@ -256,6 +437,15 @@
         "ms": "2.0.0"
       }
     },
+    "node_modules/delayed-stream": {
+      "version": "1.0.0",
+      "resolved": "https://registry.npmjs.org/delayed-stream/-/delayed-stream-1.0.0.tgz",
+      "integrity": "sha512-ZySD7Nf91aLB0RxL4KGrKHBXl7Eds1DAmEdcoVawXnLD7SDhpNgtuII2aAkg7a7QS41jxPSZ17p4VdGnMHk3MQ==",
+      "license": "MIT",
+      "engines": {
+        "node": ">=0.4.0"
+      }
+    },
     "node_modules/depd": {
       "version": "2.0.0",
       "resolved": "https://registry.npmjs.org/depd/-/depd-2.0.0.tgz",
@@ -275,6 +465,73 @@
         "npm": "1.2.8000 || >= 1.4.16"
       }
     },
+    "node_modules/dom-serializer": {
+      "version": "2.0.0",
+      "resolved": "https://registry.npmjs.org/dom-serializer/-/dom-serializer-2.0.0.tgz",
+      "integrity": "sha512-wIkAryiqt/nV5EQKqQpo3SToSOV9J0DnbJqwK7Wv/Trc92zIAYZ4FlMu+JPFW1DfGFt81ZTCGgDEabffXeLyJg==",
+      "license": "MIT",
+      "dependencies": {
+        "domelementtype": "^2.3.0",
+        "domhandler": "^5.0.2",
+        "entities": "^4.2.0"
+      },
+      "funding": {
+        "url": "https://github.com/cheeriojs/dom-serializer?sponsor=1"
+      }
+    },
+    "node_modules/domelementtype": {
+      "version": "2.3.0",
+      "resolved": "https://registry.npmjs.org/domelementtype/-/domelementtype-2.3.0.tgz",
+      "integrity": "sha512-OLETBj6w0OsagBwdXnPdN0cnMfF9opN69co+7ZrbfPGrdpPVNBUj02spi6B1N7wChLQiPn4CSH/zJvXw56gmHw==",
+      "funding": [
+        {
+          "type": "github",
+          "url": "https://github.com/sponsors/fb55"
+        }
+      ],
+      "license": "BSD-2-Clause"
+    },
+    "node_modules/domhandler": {
+      "version": "5.0.3",
+      "resolved": "https://registry.npmjs.org/domhandler/-/domhandler-5.0.3.tgz",
+      "integrity": "sha512-cgwlv/1iFQiFnU96XXgROh8xTeetsnJiDsTc7TYCLFd9+/WNkIqPTxiM/8pSd8VIrhXGTf1Ny1q1hquVqDJB5w==",
+      "license": "BSD-2-Clause",
+      "dependencies": {
+        "domelementtype": "^2.3.0"
+      },
+      "engines": {
+        "node": ">= 4"
+      },
+      "funding": {
+        "url": "https://github.com/fb55/domhandler?sponsor=1"
+      }
+    },
+    "node_modules/domutils": {
+      "version": "3.2.2",
+      "resolved": "https://registry.npmjs.org/domutils/-/domutils-3.2.2.tgz",
+      "integrity": "sha512-6kZKyUajlDuqlHKVX1w7gyslj9MPIXzIFiz/rGu35uC1wMi+kMhQwGhl4lt9unC9Vb9INnY9Z3/ZA3+FhASLaw==",
+      "license": "BSD-2-Clause",
+      "dependencies": {
+        "dom-serializer": "^2.0.0",
+        "domelementtype": "^2.3.0",
+        "domhandler": "^5.0.3"
+      },
+      "funding": {
+        "url": "https://github.com/fb55/domutils?sponsor=1"
+      }
+    },
+    "node_modules/dotenv": {
+      "version": "16.4.7",
+      "resolved": "https://registry.npmjs.org/dotenv/-/dotenv-16.4.7.tgz",
+      "integrity": "sha512-47qPchRCykZC03FhkYAhrvwU4xDBFIj1QPqaarj6mdM/hgUzfPHcpkHJOn3mJAufFeeAxAzeGsr5X0M4k6fLZQ==",
+      "license": "BSD-2-Clause",
+      "engines": {
+        "node": ">=12"
+      },
+      "funding": {
+        "url": "https://dotenvx.com"
+      }
+    },
     "node_modules/dunder-proto": {
       "version": "1.0.1",
       "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz",
@@ -304,6 +561,31 @@
         "node": ">= 0.8"
       }
     },
+    "node_modules/encoding-sniffer": {
+      "version": "0.2.0",
+      "resolved": "https://registry.npmjs.org/encoding-sniffer/-/encoding-sniffer-0.2.0.tgz",
+      "integrity": "sha512-ju7Wq1kg04I3HtiYIOrUrdfdDvkyO9s5XM8QAj/bN61Yo/Vb4vgJxy5vi4Yxk01gWHbrofpPtpxM8bKger9jhg==",
+      "license": "MIT",
+      "dependencies": {
+        "iconv-lite": "^0.6.3",
+        "whatwg-encoding": "^3.1.1"
+      },
+      "funding": {
+        "url": "https://github.com/fb55/encoding-sniffer?sponsor=1"
+      }
+    },
+    "node_modules/entities": {
+      "version": "4.5.0",
+      "resolved": "https://registry.npmjs.org/entities/-/entities-4.5.0.tgz",
+      "integrity": "sha512-V0hjH4dGPh9Ao5p0MoRY6BVqtwCjhz6vI5LT8AJ55H+4g9/4vbHx1I54fS0XuclLhDHArPQCiMjDxjaL8fPxhw==",
+      "license": "BSD-2-Clause",
+      "engines": {
+        "node": ">=0.12"
+      },
+      "funding": {
+        "url": "https://github.com/fb55/entities?sponsor=1"
+      }
+    },
     "node_modules/es-define-property": {
       "version": "1.0.1",
       "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz",
@@ -334,6 +616,21 @@
         "node": ">= 0.4"
       }
     },
+    "node_modules/es-set-tostringtag": {
+      "version": "2.1.0",
+      "resolved": "https://registry.npmjs.org/es-set-tostringtag/-/es-set-tostringtag-2.1.0.tgz",
+      "integrity": "sha512-j6vWzfrGVfyXxge+O0x5sh6cvxAog0a/4Rdd2K36zCMV5eJ+/+tOAngRO8cODMNWbVRdVlmGZQL2YS3yR8bIUA==",
+      "license": "MIT",
+      "dependencies": {
+        "es-errors": "^1.3.0",
+        "get-intrinsic": "^1.2.6",
+        "has-tostringtag": "^1.0.2",
+        "hasown": "^2.0.2"
+      },
+      "engines": {
+        "node": ">= 0.4"
+      }
+    },
     "node_modules/escape-html": {
       "version": "1.0.3",
       "resolved": "https://registry.npmjs.org/escape-html/-/escape-html-1.0.3.tgz",
@@ -349,6 +646,15 @@
         "node": ">= 0.6"
       }
     },
+    "node_modules/event-target-shim": {
+      "version": "5.0.1",
+      "resolved": "https://registry.npmjs.org/event-target-shim/-/event-target-shim-5.0.1.tgz",
+      "integrity": "sha512-i/2XbnSz/uxRCU6+NdVJgKWDTM427+MqYbkQzD321DuCQJUqOuJKIA0IM2+W2xtYHdKOmZ4dR6fExsd4SXL+WQ==",
+      "license": "MIT",
+      "engines": {
+        "node": ">=6"
+      }
+    },
     "node_modules/express": {
       "version": "4.21.2",
       "resolved": "https://registry.npmjs.org/express/-/express-4.21.2.tgz",
@@ -395,6 +701,18 @@
         "url": "https://opencollective.com/express"
       }
     },
+    "node_modules/express-rate-limit": {
+      "version": "6.11.2",
+      "resolved": "https://registry.npmjs.org/express-rate-limit/-/express-rate-limit-6.11.2.tgz",
+      "integrity": "sha512-a7uwwfNTh1U60ssiIkuLFWHt4hAC5yxlLGU2VP0X4YNlyEDZAqF4tK3GD3NSitVBrCQmQ0++0uOyFOgC2y4DDw==",
+      "license": "MIT",
+      "engines": {
+        "node": ">= 14"
+      },
+      "peerDependencies": {
+        "express": "^4 || ^5"
+      }
+    },
     "node_modules/fill-range": {
       "version": "7.1.1",
       "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz",
@@ -426,6 +744,60 @@
         "node": ">= 0.8"
       }
     },
+    "node_modules/follow-redirects": {
+      "version": "1.15.9",
+      "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.9.tgz",
+      "integrity": "sha512-gew4GsXizNgdoRyqmyfMHyAmXsZDk6mHkSxZFCzW9gwlbtOW44CDtYavM+y+72qD/Vq2l550kMF52DT8fOLJqQ==",
+      "funding": [
+        {
+          "type": "individual",
+          "url": "https://github.com/sponsors/RubenVerborgh"
+        }
+      ],
+      "license": "MIT",
+      "engines": {
+        "node": ">=4.0"
+      },
+      "peerDependenciesMeta": {
+        "debug": {
+          "optional": true
+        }
+      }
+    },
+    "node_modules/form-data": {
+      "version": "4.0.2",
+      "resolved": "https://registry.npmjs.org/form-data/-/form-data-4.0.2.tgz",
+      "integrity": "sha512-hGfm/slu0ZabnNt4oaRZ6uREyfCj6P4fT/n6A1rGV+Z0VdGXjfOhVUpkn6qVQONHGIFwmveGXyDs75+nr6FM8w==",
+      "license": "MIT",
+      "dependencies": {
+        "asynckit": "^0.4.0",
+        "combined-stream": "^1.0.8",
+        "es-set-tostringtag": "^2.1.0",
+        "mime-types": "^2.1.12"
+      },
+      "engines": {
+        "node": ">= 6"
+      }
+    },
+    "node_modules/form-data-encoder": {
+      "version": "1.7.2",
+      "resolved": "https://registry.npmjs.org/form-data-encoder/-/form-data-encoder-1.7.2.tgz",
+      "integrity": "sha512-qfqtYan3rxrnCk1VYaA4H+Ms9xdpPqvLZa6xmMgFvhO32x7/3J/ExcTd6qpxM0vH2GdMI+poehyBZvqfMTto8A==",
+      "license": "MIT"
+    },
+    "node_modules/formdata-node": {
+      "version": "4.4.1",
+      "resolved": "https://registry.npmjs.org/formdata-node/-/formdata-node-4.4.1.tgz",
+      "integrity": "sha512-0iirZp3uVDjVGt9p49aTaqjk84TrglENEDuqfdlZQ1roC9CWlPk6Avf8EEnZNcAqPonwkG35x4n3ww/1THYAeQ==",
+      "license": "MIT",
+      "dependencies": {
+        "node-domexception": "1.0.0",
+        "web-streams-polyfill": "4.0.0-beta.3"
+      },
+      "engines": {
+        "node": ">= 12.20"
+      }
+    },
     "node_modules/forwarded": {
       "version": "0.2.0",
       "resolved": "https://registry.npmjs.org/forwarded/-/forwarded-0.2.0.tgz",
@@ -552,6 +924,21 @@
         "url": "https://github.com/sponsors/ljharb"
       }
     },
+    "node_modules/has-tostringtag": {
+      "version": "1.0.2",
+      "resolved": "https://registry.npmjs.org/has-tostringtag/-/has-tostringtag-1.0.2.tgz",
+      "integrity": "sha512-NqADB8VjPFLM2V0VvHUewwwsw0ZWBaIdgo+ieHtK3hasLz4qeCRjYcqfB6AQrBggRKppKF8L52/VqdVsO47Dlw==",
+      "license": "MIT",
+      "dependencies": {
+        "has-symbols": "^1.0.3"
+      },
+      "engines": {
+        "node": ">= 0.4"
+      },
+      "funding": {
+        "url": "https://github.com/sponsors/ljharb"
+      }
+    },
     "node_modules/hasown": {
       "version": "2.0.2",
       "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
@@ -564,6 +951,25 @@
         "node": ">= 0.4"
       }
     },
+    "node_modules/htmlparser2": {
+      "version": "9.1.0",
+      "resolved": "https://registry.npmjs.org/htmlparser2/-/htmlparser2-9.1.0.tgz",
+      "integrity": "sha512-5zfg6mHUoaer/97TxnGpxmbR7zJtPwIYFMZ/H5ucTlPZhKvtum05yiPK3Mgai3a0DyVxv7qYqoweaEd2nrYQzQ==",
+      "funding": [
+        "https://github.com/fb55/htmlparser2?sponsor=1",
+        {
+          "type": "github",
+          "url": "https://github.com/sponsors/fb55"
+        }
+      ],
+      "license": "MIT",
+      "dependencies": {
+        "domelementtype": "^2.3.0",
+        "domhandler": "^5.0.3",
+        "domutils": "^3.1.0",
+        "entities": "^4.5.0"
+      }
+    },
     "node_modules/http-errors": {
       "version": "2.0.0",
       "resolved": "https://registry.npmjs.org/http-errors/-/http-errors-2.0.0.tgz",
@@ -580,13 +986,34 @@
         "node": ">= 0.8"
       }
     },
+    "node_modules/huggingface-chat": {
+      "version": "4.0.1",
+      "resolved": "https://registry.npmjs.org/huggingface-chat/-/huggingface-chat-4.0.1.tgz",
+      "integrity": "sha512-QTC0MQmRECAvZxA0CZLye6F/NHsZ3KFJtsWzKtqlWGcKQXqMY4otG0GBiddlMfYUhL8S7YezpUVyXkgBMeLGcw==",
+      "license": "ISC",
+      "dependencies": {
+        "axios": "^1.5.0"
+      },
+      "engines": {
+        "node": ">=18.17"
+      }
+    },
+    "node_modules/humanize-ms": {
+      "version": "1.2.1",
+      "resolved": "https://registry.npmjs.org/humanize-ms/-/humanize-ms-1.2.1.tgz",
+      "integrity": "sha512-Fl70vYtsAFb/C06PTS9dZBo7ihau+Tu/DNCk/OyHhea07S+aeMWpFFkUaXRa8fI+ScZbEI8dfSxwY7gxZ9SAVQ==",
+      "license": "MIT",
+      "dependencies": {
+        "ms": "^2.0.0"
+      }
+    },
     "node_modules/iconv-lite": {
-      "version": "0.4.24",
-      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz",
-      "integrity": "sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==",
+      "version": "0.6.3",
+      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.6.3.tgz",
+      "integrity": "sha512-4fCk79wshMdzMp2rH06qWrJE4iolqLhCUH+OiuIgU++RB0+94NlDL81atO7GX55uUKueo0txHNtvEyI6D7WdMw==",
       "license": "MIT",
       "dependencies": {
-        "safer-buffer": ">= 2.1.2 < 3"
+        "safer-buffer": ">= 2.1.2 < 3.0.0"
       },
       "engines": {
         "node": ">=0.10.0"
@@ -742,6 +1169,34 @@
         "node": "*"
       }
     },
+    "node_modules/morgan": {
+      "version": "1.10.0",
+      "resolved": "https://registry.npmjs.org/morgan/-/morgan-1.10.0.tgz",
+      "integrity": "sha512-AbegBVI4sh6El+1gNwvD5YIck7nSA36weD7xvIxG4in80j/UoK8AEGaWnnz8v1GxonMCltmlNs5ZKbGvl9b1XQ==",
+      "license": "MIT",
+      "dependencies": {
+        "basic-auth": "~2.0.1",
+        "debug": "2.6.9",
+        "depd": "~2.0.0",
+        "on-finished": "~2.3.0",
+        "on-headers": "~1.0.2"
+      },
+      "engines": {
+        "node": ">= 0.8.0"
+      }
+    },
+    "node_modules/morgan/node_modules/on-finished": {
+      "version": "2.3.0",
+      "resolved": "https://registry.npmjs.org/on-finished/-/on-finished-2.3.0.tgz",
+      "integrity": "sha512-ikqdkGAAyf/X/gPhXGvfgAytDZtDbr+bkNUJ0N9h5MI/dmdgCs3l6hoHrcUv41sRKew3jIwrp4qQDXiK99Utww==",
+      "license": "MIT",
+      "dependencies": {
+        "ee-first": "1.1.1"
+      },
+      "engines": {
+        "node": ">= 0.8"
+      }
+    },
     "node_modules/ms": {
       "version": "2.0.0",
       "resolved": "https://registry.npmjs.org/ms/-/ms-2.0.0.tgz",
@@ -757,6 +1212,25 @@
         "node": ">= 0.6"
       }
     },
+    "node_modules/node-domexception": {
+      "version": "1.0.0",
+      "resolved": "https://registry.npmjs.org/node-domexception/-/node-domexception-1.0.0.tgz",
+      "integrity": "sha512-/jKZoMpw0F8GRwl4/eLROPA3cfcXtLApP0QzLmUT/HuPCZWyB7IY9ZrMeKw2O/nFIqPQB3PVM9aYm0F312AXDQ==",
+      "funding": [
+        {
+          "type": "github",
+          "url": "https://github.com/sponsors/jimmywarting"
+        },
+        {
+          "type": "github",
+          "url": "https://paypal.me/jimmywarting"
+        }
+      ],
+      "license": "MIT",
+      "engines": {
+        "node": ">=10.5.0"
+      }
+    },
     "node_modules/node-fetch": {
       "version": "2.7.0",
       "resolved": "https://registry.npmjs.org/node-fetch/-/node-fetch-2.7.0.tgz",
@@ -833,6 +1307,18 @@
         "node": ">=0.10.0"
       }
     },
+    "node_modules/nth-check": {
+      "version": "2.1.1",
+      "resolved": "https://registry.npmjs.org/nth-check/-/nth-check-2.1.1.tgz",
+      "integrity": "sha512-lqjrjmaOoAnWfMmBPL+XNnynZh2+swxiX3WUE0s4yEHI6m+AwrK2UZOimIRl3X/4QctVqS8AiZjFqyOGrMXb/w==",
+      "license": "BSD-2-Clause",
+      "dependencies": {
+        "boolbase": "^1.0.0"
+      },
+      "funding": {
+        "url": "https://github.com/fb55/nth-check?sponsor=1"
+      }
+    },
     "node_modules/object-assign": {
       "version": "4.1.1",
       "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz",
@@ -866,6 +1352,82 @@
         "node": ">= 0.8"
       }
     },
+    "node_modules/on-headers": {
+      "version": "1.0.2",
+      "resolved": "https://registry.npmjs.org/on-headers/-/on-headers-1.0.2.tgz",
+      "integrity": "sha512-pZAE+FJLoyITytdqK0U5s+FIpjN0JP3OzFi/u8Rx+EV5/W+JTWGXG8xFzevE7AjBfDqHv/8vL8qQsIhHnqRkrA==",
+      "license": "MIT",
+      "engines": {
+        "node": ">= 0.8"
+      }
+    },
+    "node_modules/openai": {
+      "version": "4.90.0",
+      "resolved": "https://registry.npmjs.org/openai/-/openai-4.90.0.tgz",
+      "integrity": "sha512-YCuHMMycqtCg1B8G9ezkOF0j8UnBWD3Al/zYaelpuXwU1yhCEv+Y4n9G20MnyGy6cH4GsFwOMrgstQ+bgG1PtA==",
+      "license": "Apache-2.0",
+      "dependencies": {
+        "@types/node": "^18.11.18",
+        "@types/node-fetch": "^2.6.4",
+        "abort-controller": "^3.0.0",
+        "agentkeepalive": "^4.2.1",
+        "form-data-encoder": "1.7.2",
+        "formdata-node": "^4.3.2",
+        "node-fetch": "^2.6.7"
+      },
+      "bin": {
+        "openai": "bin/cli"
+      },
+      "peerDependencies": {
+        "ws": "^8.18.0",
+        "zod": "^3.23.8"
+      },
+      "peerDependenciesMeta": {
+        "ws": {
+          "optional": true
+        },
+        "zod": {
+          "optional": true
+        }
+      }
+    },
+    "node_modules/parse5": {
+      "version": "7.2.1",
+      "resolved": "https://registry.npmjs.org/parse5/-/parse5-7.2.1.tgz",
+      "integrity": "sha512-BuBYQYlv1ckiPdQi/ohiivi9Sagc9JG+Ozs0r7b/0iK3sKmrb0b9FdWdBbOdx6hBCM/F9Ir82ofnBhtZOjCRPQ==",
+      "license": "MIT",
+      "dependencies": {
+        "entities": "^4.5.0"
+      },
+      "funding": {
+        "url": "https://github.com/inikulin/parse5?sponsor=1"
+      }
+    },
+    "node_modules/parse5-htmlparser2-tree-adapter": {
+      "version": "7.1.0",
+      "resolved": "https://registry.npmjs.org/parse5-htmlparser2-tree-adapter/-/parse5-htmlparser2-tree-adapter-7.1.0.tgz",
+      "integrity": "sha512-ruw5xyKs6lrpo9x9rCZqZZnIUntICjQAd0Wsmp396Ul9lN/h+ifgVV1x1gZHi8euej6wTfpqX8j+BFQxF0NS/g==",
+      "license": "MIT",
+      "dependencies": {
+        "domhandler": "^5.0.3",
+        "parse5": "^7.0.0"
+      },
+      "funding": {
+        "url": "https://github.com/inikulin/parse5?sponsor=1"
+      }
+    },
+    "node_modules/parse5-parser-stream": {
+      "version": "7.1.2",
+      "resolved": "https://registry.npmjs.org/parse5-parser-stream/-/parse5-parser-stream-7.1.2.tgz",
+      "integrity": "sha512-JyeQc9iwFLn5TbvvqACIF/VXG6abODeB3Fwmv/TGdLk2LfbWkaySGY72at4+Ty7EkPZj854u4CrICqNk2qIbow==",
+      "license": "MIT",
+      "dependencies": {
+        "parse5": "^7.0.0"
+      },
+      "funding": {
+        "url": "https://github.com/inikulin/parse5?sponsor=1"
+      }
+    },
     "node_modules/parseurl": {
       "version": "1.3.3",
       "resolved": "https://registry.npmjs.org/parseurl/-/parseurl-1.3.3.tgz",
@@ -907,6 +1469,12 @@
         "node": ">= 0.10"
       }
     },
+    "node_modules/proxy-from-env": {
+      "version": "1.1.0",
+      "resolved": "https://registry.npmjs.org/proxy-from-env/-/proxy-from-env-1.1.0.tgz",
+      "integrity": "sha512-D+zkORCbA9f1tdWRK0RaCR3GPv50cMxcrz4X8k5LTSUD1Dkw47mKJEZQNunItRTkWwgtaUSo1RVFRIG9ZXiFYg==",
+      "license": "MIT"
+    },
     "node_modules/pstree.remy": {
       "version": "1.1.8",
       "resolved": "https://registry.npmjs.org/pstree.remy/-/pstree.remy-1.1.8.tgz",
@@ -953,6 +1521,18 @@
         "node": ">= 0.8"
       }
     },
+    "node_modules/raw-body/node_modules/iconv-lite": {
+      "version": "0.4.24",
+      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz",
+      "integrity": "sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==",
+      "license": "MIT",
+      "dependencies": {
+        "safer-buffer": ">= 2.1.2 < 3"
+      },
+      "engines": {
+        "node": ">=0.10.0"
+      }
+    },
     "node_modules/readdirp": {
       "version": "3.6.0",
       "resolved": "https://registry.npmjs.org/readdirp/-/readdirp-3.6.0.tgz",
@@ -1180,9 +1760,9 @@
       }
     },
     "node_modules/swagger-ui-dist": {
-      "version": "5.20.1",
-      "resolved": "https://registry.npmjs.org/swagger-ui-dist/-/swagger-ui-dist-5.20.1.tgz",
-      "integrity": "sha512-qBPCis2w8nP4US7SvUxdJD3OwKcqiWeZmjN2VWhq2v+ESZEXOP/7n4DeiOiiZcGYTKMHAHUUrroHaTsjUWTEGw==",
+      "version": "5.20.2",
+      "resolved": "https://registry.npmjs.org/swagger-ui-dist/-/swagger-ui-dist-5.20.2.tgz",
+      "integrity": "sha512-zP2biZvCt6R1IAz/iGcjeEViHez7UPHUFfMFyF6jcTKS1ZIP2cgr+KSZEMhBnpIcFfDrZxkD8v56taL5A8phuA==",
       "license": "Apache-2.0",
       "dependencies": {
         "@scarf/scarf": "=1.4.0"
@@ -1261,6 +1841,21 @@
       "dev": true,
       "license": "MIT"
     },
+    "node_modules/undici": {
+      "version": "6.21.2",
+      "resolved": "https://registry.npmjs.org/undici/-/undici-6.21.2.tgz",
+      "integrity": "sha512-uROZWze0R0itiAKVPsYhFov9LxrPMHLMEQFszeI2gCN6bnIIZ8twzBCJcN2LJrBBLfrP0t1FW0g+JmKVl8Vk1g==",
+      "license": "MIT",
+      "engines": {
+        "node": ">=18.17"
+      }
+    },
+    "node_modules/undici-types": {
+      "version": "5.26.5",
+      "resolved": "https://registry.npmjs.org/undici-types/-/undici-types-5.26.5.tgz",
+      "integrity": "sha512-JlCMO+ehdEIKqlFxk6IfVoAUVmgz7cU7zD/h9XZ0qzeosSHmUJVOzSQvvYSYWXkFXC+IfLKSIffhv0sVZup6pA==",
+      "license": "MIT"
+    },
     "node_modules/unpipe": {
       "version": "1.0.0",
       "resolved": "https://registry.npmjs.org/unpipe/-/unpipe-1.0.0.tgz",
@@ -1288,12 +1883,42 @@
         "node": ">= 0.8"
       }
     },
+    "node_modules/web-streams-polyfill": {
+      "version": "4.0.0-beta.3",
+      "resolved": "https://registry.npmjs.org/web-streams-polyfill/-/web-streams-polyfill-4.0.0-beta.3.tgz",
+      "integrity": "sha512-QW95TCTaHmsYfHDybGMwO5IJIM93I/6vTRk+daHTWFPhwh+C8Cg7j7XyKrwrj8Ib6vYXe0ocYNrmzY4xAAN6ug==",
+      "license": "MIT",
+      "engines": {
+        "node": ">= 14"
+      }
+    },
     "node_modules/webidl-conversions": {
       "version": "3.0.1",
       "resolved": "https://registry.npmjs.org/webidl-conversions/-/webidl-conversions-3.0.1.tgz",
       "integrity": "sha512-2JAn3z8AR6rjK8Sm8orRC0h/bcl/DqL7tRPdGZ4I1CjdF+EaMLmYxBHyXuKL849eucPFhvBoxMsflfOb8kxaeQ==",
       "license": "BSD-2-Clause"
     },
+    "node_modules/whatwg-encoding": {
+      "version": "3.1.1",
+      "resolved": "https://registry.npmjs.org/whatwg-encoding/-/whatwg-encoding-3.1.1.tgz",
+      "integrity": "sha512-6qN4hJdMwfYBtE3YBTTHhoeuUrDBPZmbQaxWAqSALV/MeEnR5z1xd8UKud2RAkFoPkmB+hli1TZSnyi84xz1vQ==",
+      "license": "MIT",
+      "dependencies": {
+        "iconv-lite": "0.6.3"
+      },
+      "engines": {
+        "node": ">=18"
+      }
+    },
+    "node_modules/whatwg-mimetype": {
+      "version": "4.0.0",
+      "resolved": "https://registry.npmjs.org/whatwg-mimetype/-/whatwg-mimetype-4.0.0.tgz",
+      "integrity": "sha512-QaKxh0eNIi2mE9p2vEdzfagOKHCcj1pJ56EEHGQOVxp8r9/iszLUUV7v89x9O1p/T+NlTM5W7jW6+cz4Fq1YVg==",
+      "license": "MIT",
+      "engines": {
+        "node": ">=18"
+      }
+    },
     "node_modules/whatwg-url": {
       "version": "5.0.0",
       "resolved": "https://registry.npmjs.org/whatwg-url/-/whatwg-url-5.0.0.tgz",
diff --git a/package.json b/package.json
index 6483558..88ca229 100644
--- a/package.json
+++ b/package.json
@@ -1,23 +1,27 @@
 {
-  "name": "hackclub-api-proxy",
+  "name": "sylph-api",
   "version": "1.0.0",
-  "description": "OpenAI API compatible proxy for HackClub AI",
-  "main": "server.js",
+  "description": "A compatibility layer for multiple AI providers",
+  "main": "src/server.js",
+  "type": "module",
   "scripts": {
-    "start": "node server.js",
-    "dev": "nodemon server.js",
-    "vercel-build": "cp -r node_modules/swagger-ui-express/static public/swagger-ui"
+    "start": "node src/server.js",
+    "dev": "nodemon src/server.js"
   },
+  "author": "minoa.cat",
+  "license": "MIT",
   "dependencies": {
-    "express": "^4.18.2",
+    "cheerio": "^1.0.0-rc.12",
     "cors": "^2.8.5",
-    "node-fetch": "^2.6.9",
+    "dotenv": "^16.0.3",
+    "express": "^4.21.2",
+    "express-rate-limit": "^6.11.2",
+    "huggingface-chat": "latest",
+    "morgan": "^1.10.0",
+    "openai": "^4.0.0",
     "swagger-ui-express": "^4.6.2"
   },
   "devDependencies": {
     "nodemon": "^2.0.22"
-  },
-  "engines": {
-    "node": ">=14.0.0"
   }
 }
diff --git a/public/404.html b/public/404.html
new file mode 100644
index 0000000..e998f8d
--- /dev/null
+++ b/public/404.html
@@ -0,0 +1,65 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <title>404 - Page Not Found</title>
+    <link rel="stylesheet" href="/styles.css">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <style>
+        .error-container {
+            display: flex;
+            flex-direction: column;
+            align-items: center;
+            justify-content: center;
+            min-height: 80vh;
+            text-align: center;
+            padding: 2rem;
+        }
+
+        .error-code {
+            font-family: 'Rubik 80s Fade', cursive;
+            font-size: 8rem;
+            margin: 0;
+            color: var(--text-bright);
+            animation: glow 2s ease-in-out infinite alternate;
+            line-height: 1;
+        }
+
+        .error-title {
+            font-size: 1.5rem;
+            margin: 1rem 0 2rem;
+            color: var(--text-muted);
+        }
+
+        .back-home {
+            display: inline-block;
+            padding: 0.75rem 1.5rem;
+            background: var(--button-bg);
+            border: 1px solid var(--border);
+            border-radius: 8px;
+            color: var(--text);
+            text-decoration: none;
+            transition: all 0.3s ease;
+        }
+
+        .back-home:hover {
+            background: var(--button-hover);
+            border-color: var(--text);
+            transform: translateY(-2px);
+        }
+
+        @media (max-width: 768px) {
+            .error-code {
+                font-size: 6rem;
+            }
+        }
+    </style>
+</head>
+<body>
+    <div class="error-container">
+        <h1 class="error-code">404</h1>
+        <p class="error-title">oops! page not found</p>
+        <a href="/" class="back-home">Return Home</a>
+    </div>
+</body>
+</html>
\ No newline at end of file
diff --git a/public/docs.html b/public/docs.html
new file mode 100644
index 0000000..493219b
--- /dev/null
+++ b/public/docs.html
@@ -0,0 +1,658 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <title>API Documentation - Sylph</title>
+    <link rel="stylesheet" href="/styles.css">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <script defer src="https://analytics.minoa.cat/script.js" data-website-id="dba618bd-576d-4166-a280-e38df64bf53f"></script>
+    <meta name="description" content="API documentation for Sylph - Multi-API Compatibility Layer for Large Language Models">
+    <meta name="keywords" content="AI API, language models, LLM, API documentation, OpenAI-compatible">
+    <meta name="robots" content="index, follow">
+    <style>
+        :root {
+            --sidebar-width: 250px;
+            --section-gap: 4rem;
+            --nav-height: 60px;
+        }
+
+        .docs-container {
+            display: flex;
+            max-width: 1200px;
+            margin: 0 auto;
+            gap: 2rem;
+            padding: 1rem;
+        }
+
+        .docs-sidebar {
+            position: sticky;
+            top: var(--nav-height);
+            height: calc(100vh - var(--nav-height));
+            width: var(--sidebar-width);
+            padding: 1rem;
+            overflow-y: auto;
+            padding-top: calc(var(--nav-height) + 2rem);
+        }
+
+        .docs-content {
+            flex: 1;
+            padding: 1rem;
+            max-width: calc(100% - var(--sidebar-width));
+            padding-top: calc(var(--nav-height) + 2rem);
+            padding-bottom: 300px;
+        }
+
+        .base-url-banner {
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 8px;
+            padding: 1rem;
+            margin-bottom: 2rem;
+            font-family: 'Ubuntu Mono', monospace;
+            color: var(--text-bright);
+            display: flex;
+            align-items: center;
+            gap: 1rem;
+        }
+
+        .base-url-label {
+            color: var(--text-muted);
+            font-size: 0.9rem;
+        }
+
+        .base-url {
+            flex: 1;
+            font-weight: bold;
+        }
+
+        .base-url-copy {
+            background: var(--button-bg);
+            border: 1px solid var(--border);
+            color: var(--text);
+            padding: 0.4rem 0.8rem;
+            border-radius: 4px;
+            cursor: pointer;
+            font-size: 0.9rem;
+            transition: all 0.3s ease;
+        }
+
+        .base-url-copy:hover {
+            background: var(--button-hover);
+            border-color: var(--text);
+            transform: translateY(-1px);
+        }
+
+        /* Section styling */
+        .section {
+            scroll-margin-top: calc(var(--nav-height) + 2rem);
+        }
+        
+        .section-nav {
+            margin: 2rem 0;
+        }
+
+        .section-nav-item {
+            display: block;
+            padding: 0.5rem 1rem;
+            margin: 0.5rem 0;
+            color: var(--link-color);
+            text-decoration: none;
+            border-radius: 6px;
+            transition: all 0.3s ease;
+            position: relative;
+        }
+
+        .section-nav-item:hover {
+            background: var(--card-bg);
+            color: var(--link-hover);
+        }
+
+        .section-nav-item.active {
+            background: var(--button-bg);
+            color: var(--link-hover);
+            border-left: 3px solid var(--text);
+        }
+
+        .endpoint {
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 12px;
+            margin: 3rem 0;
+            overflow: hidden;
+            transition: all 0.3s ease;
+        }
+
+        .endpoint:hover {
+            border-color: var(--text);
+            transform: translateY(-2px);
+        }
+
+        .endpoint-header {
+            padding: 1rem;
+            display: flex;
+            align-items: center;
+            gap: 1rem;
+            border-bottom: 1px solid var(--border);
+            background: rgba(0, 0, 0, 0.3);
+        }
+
+        .http-method {
+            padding: 0.25rem 0.75rem;
+            border-radius: 6px;
+            font-size: 0.9rem;
+            font-weight: bold;
+            min-width: 60px;
+            text-align: center;
+        }
+
+        .method-get { 
+            background: rgba(97, 175, 254, 0.1); 
+            color: #61affe;
+        }
+        
+        .method-post { 
+            background: rgba(73, 204, 144, 0.1); 
+            color: #49cc90;
+        }
+
+        .endpoint-url {
+            font-family: 'Ubuntu Mono', monospace;
+            color: var(--text-bright);
+        }
+
+        .endpoint-body {
+            padding: 2rem;
+        }
+
+        .parameter-table {
+            width: 100%;
+            border-collapse: collapse;
+            margin: 2rem 0;
+        }
+
+        .parameter-table th,
+        .parameter-table td {
+            padding: 1rem;
+            text-align: left;
+            border-bottom: 1px solid var(--border);
+        }
+
+        .parameter-table th {
+            color: var(--text-bright);
+            font-weight: normal;
+            background: rgba(0, 0, 0, 0.2);
+        }
+
+        .parameter-required {
+            color: #ff5555;
+            margin-left: 0.25rem;
+        }
+
+        .code-block {
+            background: rgba(0, 0, 0, 0.4);
+            border: 1px solid var(--border);
+            border-radius: 12px;
+            padding: 1.5rem;
+            margin: 1.5rem 0;
+            font-family: 'Ubuntu Mono', monospace;
+            position: relative;
+        }
+
+        .code-header {
+            display: flex;
+            justify-content: space-between;
+            align-items: center;
+            margin: -1.5rem -1.5rem 1rem -1.5rem;
+            padding: 0.75rem 1.5rem;
+            background: rgba(0, 0, 0, 0.2);
+            border-bottom: 1px solid var(--border);
+            color: var(--text-muted);
+            font-size: 0.9rem;
+        }
+
+        .copy-btn {
+            position: absolute;
+            top: 0.75rem;
+            right: 0.75rem;
+            background: var(--button-bg);
+            border: 1px solid var(--border);
+            color: var(--text);
+            padding: 0.25rem 0.75rem;
+            border-radius: 4px;
+            cursor: pointer;
+            font-size: 0.8rem;
+            transition: all 0.3s ease;
+        }
+
+        .copy-btn:hover {
+            background: var(--button-hover);
+            border-color: var(--text);
+            transform: translateY(-1px);
+        }
+
+        .copy-btn.copied {
+            background: var(--text);
+            color: var(--background);
+        }
+
+        @media (max-width: 1024px) {
+            :root {
+                --nav-height: 0;
+            }
+
+            .docs-container {
+                flex-direction: column;
+                padding: 0;
+            }
+
+            .docs-sidebar {
+                position: relative;
+                width: 100%;
+                height: auto;
+                padding: 1rem;
+                top: 0;
+            }
+
+            .docs-content {
+                max-width: 100%;
+                padding: 1rem;
+            }
+
+            .section-nav {
+                display: flex;
+                flex-wrap: wrap;
+                gap: 0.5rem;
+                margin: 1rem 0;
+            }
+
+            .section-nav-item {
+                flex: 1 1 auto;
+                min-width: 150px;
+                text-align: center;
+                margin: 0;
+            }
+        }
+
+        @media (max-width: 768px) {
+            .docs-container {
+                padding: 0;
+            }
+
+            .endpoint {
+                margin: 1.5rem 0;
+                border-radius: 0;
+            }
+
+            .endpoint-header {
+                flex-direction: column;
+                align-items: flex-start;
+                gap: 0.5rem;
+            }
+
+            .endpoint-body {
+                padding: 1rem;
+            }
+
+            .parameter-table {
+                display: block;
+                overflow-x: auto;
+                font-size: 0.9rem;
+            }
+
+            .code-block {
+                padding: 1rem;
+                margin: 1rem -1rem;
+                border-radius: 0;
+                border-left: none;
+                border-right: none;
+            }
+
+            .code-header {
+                margin: -1rem -1rem 1rem -1rem;
+                padding: 0.5rem 1rem;
+            }
+
+            pre {
+                font-size: 0.85rem;
+                overflow-x: auto;
+            }
+
+            .base-url-banner {
+                margin: 0 -1rem 1rem -1rem;
+                border-radius: 0;
+                border-left: none;
+                border-right: none;
+            }
+        }
+    </style>
+</head>
+<body>
+
+    <div class="docs-container">
+        <aside class="docs-sidebar">
+            <nav class="section-nav">
+                <a href="#introduction" class="section-nav-item">Introduction</a>
+                <a href="#authentication" class="section-nav-item">Authentication</a>
+                <a href="#models" class="section-nav-item">Models</a>
+                <a href="#chat" class="section-nav-item">Chat</a>
+                <a href="#errors" class="section-nav-item">Errors</a>
+            </nav>
+        </aside>
+
+        <main class="docs-content">
+            <div class="base-url-banner">
+                <span class="base-url-label">Base URL for OpenAI Compatable Things:</span>
+                <span class="base-url" id="baseUrl"></span>
+                <button class="base-url-copy" onclick="copyBaseUrl()">Copy</button>
+            </div>
+
+            <div id="introduction" class="section">
+                <h2>Introduction</h2>
+                <p>Sylph provides an OpenAI-compatible API for interacting with various AI models across different providers. Our API follows RESTful practices and returns JSON responses.</p>
+                
+                <div class="endpoint">
+                    <div class="endpoint-header">
+                        <span class="http-method method-get">GET</span>
+                        <span class="endpoint-url">/health</span>
+                    </div>
+                    <div class="endpoint-body">
+                        <p>Check API health status and provider availability.</p>
+                        
+                        <div class="code-block">
+                            <div class="code-header">
+                                Request
+                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                            </div>
+                            <pre>curl <span id="healthUrl"></span></pre>
+                        </div>
+
+                        <div class="code-block">
+                            <div class="code-header">
+                                Response
+                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                            </div>
+                            <pre>{
+  "status": "ok",
+  "providers": {
+    "available": ["HackClubProvider", "GoogleProvider", "..."],
+    "disabled": [],
+    "errors": {}
+  }
+}</pre>
+                        </div>
+                    </div>
+                </div>
+            </div>
+
+            <div id="authentication" class="section">
+                <h2>Authentication</h2>
+                <p>Authentication is optional but recommended. You can provide any string as the API key:</p>
+                <div class="code-block">
+                    <div class="code-header">
+                        HTTP Header
+                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                    </div>
+                    <pre>Authorization: Bearer any-string-works</pre>
+                </div>
+            </div>
+
+            <div id="models" class="section">
+                <h2>Models</h2>
+                <p class="text-muted" style="margin-bottom: 1rem;">Browse all available models at <a href="/models">/models</a></p>
+                
+                <div class="endpoint">
+                    <div class="endpoint-header">
+                        <span class="http-method method-get">GET</span>
+                        <span class="endpoint-url">/v1/models</span>
+                    </div>
+                    <div class="endpoint-body">
+                        <p>List all available models across enabled providers.</p>
+                        
+                        <div class="code-block">
+                            <div class="code-header">
+                                Request
+                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                            </div>
+                            <pre>curl <span class="api-url"></span>/v1/models \
+     -H "Authorization: Bearer any-string"</pre>
+                        </div>
+
+                        <div class="code-block">
+                            <div class="code-header">
+                                Response
+                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                            </div>
+                            <pre>{
+  "object": "list",
+  "data": [
+    {
+      "id": "hackclub/llama-3.3-70b-versatile",
+      "object": "model",
+      "created": 1743648062945,
+      "owned_by": "https://hackclub.com",
+      "permission": [],
+      "root": "llama",
+      "context_length": 128000,
+      "capabilities": {
+        "text": true,
+        "images": false
+      },
+      "health": {
+        "status": "operational",
+        "latency": 312
+      }
+    }
+  ]
+}</pre>
+                        </div>
+                    </div>
+                </div>
+            </div>
+
+            <div id="chat" class="section">
+                <h2>Chat</h2>
+                
+                <div class="endpoint">
+                    <div class="endpoint-header">
+                        <span class="http-method method-post">POST</span>
+                        <span class="endpoint-url">/v1/chat/completions</span>
+                    </div>
+                    <div class="endpoint-body">
+                        <p>Create a chat completion with the specified model.</p>
+
+                        <h4>Request Parameters</h4>
+                        <table class="parameter-table">
+                            <tr>
+                                <th>Parameter</th>
+                                <th>Type</th>
+                                <th>Description</th>
+                            </tr>
+                            <tr>
+                                <td>model<span class="parameter-required">*</span></td>
+                                <td>string</td>
+                                <td>ID of the model to use</td>
+                            </tr>
+                            <tr>
+                                <td>messages<span class="parameter-required">*</span></td>
+                                <td>array</td>
+                                <td>Array of messages for the conversation</td>
+                            </tr>
+                            <tr>
+                                <td>stream</td>
+                                <td>boolean</td>
+                                <td>Whether to stream responses (default: false)</td>
+                            </tr>
+                            <tr>
+                                <td>temperature</td>
+                                <td>number</td>
+                                <td>Controls randomness (0-2, default: 0.7)</td>
+                            </tr>
+                        </table>
+
+                        <div class="code-block">
+                            <div class="code-header">
+                                Request
+                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                            </div>
+                            <pre>curl <span class="api-url"></span>/v1/chat/completions \
+     -H "Content-Type: application/json" \
+     -H "Authorization: Bearer any-string" \
+     -d '{
+  "model": "hackclub/llama-3.3-70b-versatile",
+  "messages": [
+    {"role": "user", "content": "Hello!"}
+  ],
+  "temperature": 0.7
+}'</pre>
+                        </div>
+
+                        <div class="code-block">
+                            <div class="code-header">
+                                Response
+                                <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                            </div>
+                            <pre>{
+  "id": "chat-12345",
+  "object": "chat.completion",
+  "created": 1743648062945,
+  "model": "hackclub/llama-3.3-70b-versatile",
+  "choices": [{
+    "index": 0,
+    "message": {
+      "role": "assistant",
+      "content": "Hello! How can I help you today?"
+    },
+    "finish_reason": "stop"
+  }],
+  "usage": {
+    "prompt_tokens": 10,
+    "completion_tokens": 8,
+    "total_tokens": 18
+  }
+}</pre>
+                        </div>
+                    </div>
+                </div>
+            </div>
+
+            <div id="errors" class="section">
+                <h2>Errors</h2>
+                <table class="parameter-table">
+                    <tr>
+                        <th>Code</th>
+                        <th>Description</th>
+                    </tr>
+                    <tr>
+                        <td>400</td>
+                        <td>Invalid request parameters</td>
+                    </tr>
+                    <tr>
+                        <td>401</td>
+                        <td>Invalid or missing API key</td>
+                    </tr>
+                    <tr>
+                        <td>404</td>
+                        <td>Model not found</td>
+                    </tr>
+                    <tr>
+                        <td>429</td>
+                        <td>Too many requests</td>
+                    </tr>
+                    <tr>
+                        <td>500</td>
+                        <td>Server or provider error</td>
+                    </tr>
+                </table>
+
+                <div class="code-block">
+                    <div class="code-header">
+                        Error Response Example
+                        <button class="copy-btn" onclick="copyCode(this)">Copy</button>
+                    </div>
+                    <pre>{
+  "error": {
+    "message": "Model not found: invalid-model",
+    "type": "invalid_request_error",
+    "code": "model_not_found",
+    "param": "model"
+  }
+}</pre>
+                </div>
+            </div>
+        </main>
+    </div>
+
+    <script>
+        // Update API URLs
+        const baseUrl = window.location.origin + '/v1';
+        document.getElementById('baseUrl').textContent = baseUrl;
+        document.querySelectorAll('.api-url').forEach(el => {
+            el.textContent = window.location.origin;
+        });
+        document.getElementById('healthUrl').textContent = window.location.origin + '/health';
+
+        // Copy functions
+        function copyBaseUrl() {
+            navigator.clipboard.writeText(baseUrl);
+            const btn = document.querySelector('.base-url-copy');
+            btn.textContent = 'Copied!';
+            setTimeout(() => btn.textContent = 'Copy', 2000);
+        }
+
+        function copyCode(button) {
+            const pre = button.parentElement.nextElementSibling;
+            const code = pre.textContent;
+            
+            // Replace placeholders with actual values
+            const processedCode = code
+                .replace(/<span class="api-url"><\/span>/g, window.location.origin)
+                .replace('<span id="healthUrl"></span>', window.location.origin + '/health');
+
+            navigator.clipboard.writeText(processedCode);
+            button.textContent = 'Copied!';
+            button.classList.add('copied');
+            setTimeout(() => {
+                button.textContent = 'Copy';
+                button.classList.remove('copied');
+            }, 2000);
+        }
+
+        // Active section tracking
+        function updateActiveSection() {
+            const sections = document.querySelectorAll('.section');
+            let activeSection = '';
+
+            sections.forEach(section => {
+                const rect = section.getBoundingClientRect();
+                if (rect.top <= 100 && rect.bottom > 100) {
+                    activeSection = section.id;
+                }
+            });
+
+            document.querySelectorAll('.section-nav-item').forEach(item => {
+                if (item.getAttribute('href').slice(1) === activeSection) {
+                    item.classList.add('active');
+                } else {
+                    item.classList.remove('active');
+                }
+            });
+        }
+
+        // Smooth scroll to sections
+        document.querySelectorAll('.section-nav-item').forEach(link => {
+            link.addEventListener('click', (e) => {
+                e.preventDefault();
+                const id = link.getAttribute('href').slice(1);
+                document.getElementById(id).scrollIntoView({
+                    behavior: 'smooth',
+                    block: 'start'
+                });
+            });
+        });
+
+        // Initialize
+        window.addEventListener('scroll', updateActiveSection);
+        updateActiveSection();
+    </script>
+</body>
+</html>
diff --git a/public/donate.html b/public/donate.html
new file mode 100644
index 0000000..5edd0a3
--- /dev/null
+++ b/public/donate.html
@@ -0,0 +1,369 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <title>Donate - Sylph</title>
+    <link rel="stylesheet" href="/styles.css">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <script defer src="https://analytics.minoa.cat/script.js" data-website-id="dba618bd-576d-4166-a280-e38df64bf53f"></script>
+    <style>
+        .donate-container {
+            max-width: 1200px;
+            margin: 0 auto;
+            padding: 0 1rem;
+        }
+
+        /* Center PayPal card */
+        .paypal-section {
+            margin: 0 auto 2rem;
+        }
+
+        /* Grid for crypto cards */
+        .crypto-grid {
+            display: grid;
+            grid-template-columns: repeat(2, 1fr);
+            gap: 2rem;
+            margin: 2rem 0;
+        }
+
+        @media (max-width: 768px) {
+            .crypto-grid {
+                grid-template-columns: 1fr;
+            }
+
+            .donate-card {
+                aspect-ratio: 4/3;
+                height: auto;
+            }
+        }
+
+        .donate-card {
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 12px;
+            padding: 2rem;
+            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
+            cursor: pointer;
+            position: relative;
+            overflow: hidden;
+            display: flex;
+            flex-direction: column;
+            gap: 1rem;
+        }
+
+        .donate-card:not(.disabled):hover {
+            transform: translateY(-3px);
+            border-color: var(--text);
+            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.15);
+        }
+
+        .donate-card::before {
+            content: '';
+            position: absolute;
+            top: 0;
+            left: 0;
+            right: 0;
+            height: 4px;
+            background: linear-gradient(90deg, var(--gradient-start), var(--gradient-end));
+            opacity: 0;
+            transition: opacity 0.3s ease;
+        }
+
+        .donate-card:not(.disabled):hover::before {
+            opacity: 1;
+        }
+
+        .donate-card.disabled {
+            opacity: 0.5;
+            cursor: not-allowed;
+        }
+
+        .donate-header {
+            display: flex;
+            align-items: center;
+            gap: 1rem;
+            margin-bottom: 0.5rem;
+        }
+
+        .donate-icon {
+            width: 32px;
+            height: 32px;
+            display: flex;
+            align-items: center;
+            justify-content: center;
+        }
+
+        .donate-title {
+            font-size: 1.8rem;
+            color: var(--text-bright);
+            margin: 0;
+        }
+
+        .donate-description {
+            color: var(--text-muted);
+            font-size: 1.1rem;
+            margin: 0;
+            flex: 1;
+        }
+
+        /* Modal styles */
+        .modal {
+            display: none;
+            position: fixed;
+            top: 0;
+            left: 0;
+            width: 100%;
+            height: 100%;
+            background: rgba(0, 0, 0, 0.8);
+            backdrop-filter: blur(5px);
+            z-index: 1500;
+        }
+
+        .modal.show {
+            display: block;
+        }
+
+        .modal-content {
+            position: relative;
+            background: var(--card-bg);
+            margin: 15% auto;
+            padding: 2rem;
+            border: 1px solid var(--border);
+            border-radius: 12px;
+            max-width: 500px;
+            animation: slideIn 0.3s ease;
+        }
+
+        .close-modal {
+            position: absolute;
+            right: 1rem;
+            top: 1rem;
+            font-size: 1.5rem;
+            cursor: pointer;
+            color: var(--text-muted);
+            transition: color 0.3s ease;
+            background: none;
+            border: none;
+            width: 32px;
+            height: 32px;
+            display: flex;
+            align-items: center;
+            justify-content: center;
+            border-radius: 50%;
+        }
+
+        .close-modal:hover {
+            color: var(--text-bright);
+            background: var(--button-bg);
+        }
+
+        .wallet-address {
+            background: var(--button-bg);
+            padding: 1rem;
+            border-radius: 8px;
+            font-family: monospace;
+            margin: 1rem 0;
+            word-break: break-all;
+            border: 1px solid var(--border);
+            color: var(--text-bright);
+            position: relative;
+            transition: all 0.3s ease;
+        }
+
+        .wallet-address:hover {
+            background: var(--button-hover);
+            border-color: var(--text);
+        }
+
+        .copy-button {
+            background: var(--button-bg);
+            border: 1px solid var(--border);
+            color: var(--text);
+            padding: 0.75rem 1.5rem;
+            border-radius: 8px;
+            cursor: pointer;
+            transition: all 0.3s ease;
+            display: flex;
+            align-items: center;
+            gap: 0.5rem;
+            margin: 0 auto;
+        }
+
+        .copy-button:hover {
+            background: var(--button-hover);
+            border-color: var(--text);
+            transform: translateY(-2px);
+        }
+
+        .notification {
+            position: fixed;
+            top: -100px;
+            left: 50%;
+            transform: translateX(-50%);
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 8px;
+            padding: 1rem 2rem;
+            color: var(--text-bright);
+            transition: top 0.3s cubic-bezier(0.4, 0, 0.2, 1);
+            z-index: 2000;
+            backdrop-filter: blur(5px);
+        }
+
+        .notification.show {
+            top: 20px;
+        }
+
+        @media (max-width: 768px) {
+            .modal-content {
+                margin: 5% auto;
+                width: calc(100% - 2rem);
+            }
+        }
+    </style>
+</head>
+<body>
+    <div class="donate-container">
+        <h1 class="site-header" onclick="location.href='/'">/donate</h1>
+        <p class="text-center text-muted" style="font-size: 1.2rem; margin-bottom: 2rem;">Support the development and hosting of Sylph</p>
+
+        <!-- PayPal Card (centered) -->
+        <div class="paypal-section">
+        <div class="donate-card" onclick="window.location.href='https://paypal.me/m1noa'">
+            <div class="donate-header">
+                <div class="donate-icon">
+                    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="-2 -2 24 24"><g fill="#003fb0"><path d="m8.252 13.405l-.006.036a.47.47 0 0 1-.462.388h-1.32a.465.465 0 0 1-.457-.542l1.329-7.9A.465.465 0 0 1 7.794 5H11.4c1.192 0 2.158.966 2.158 2.158q0 .213-.026.42c.483.342.798.906.798 1.543a2.98 2.98 0 0 1-2.982 2.982h-.303a.7.7 0 0 0-.688.582l-.164.974l.164-.974a.7.7 0 0 1 .688-.582h.303a2.98 2.98 0 0 0 2.982-2.982c0-.637-.315-1.2-.798-1.544a3.4 3.4 0 0 1-3.372 2.98H9.155a.505.505 0 0 0-.485.364l-.602 3.576a.41.41 0 0 0 .404.478h1.154c.2 0 .37-.145.404-.342l.164-.974l-.164.974a.41.41 0 0 1-.404.342H8.472a.408.408 0 0 1-.404-.478z"/><path d="M13.967 8c.19.295.301.647.301 1.025a2.98 2.98 0 0 1-2.981 2.982h-.304a.7.7 0 0 0-.688.582l-.327 1.949a.41.41 0 0 1-.404.341H8.507l.6-3.56a.505.505 0 0 1 .484-.366h1.007A3.4 3.4 0 0 0 13.967 8m-5.46 6.88H8.41a.41.41 0 0 1-.404-.478l.03-.176h.185a.47.47 0 0 0 .461-.388l.007-.036l-.182 1.077z"/><path d="M4 2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2zm0-2h12a4 4 0 0 1 4 4v12a4 4 0 0 1-4 4H4a4 4 0 0 1-4-4V4a4 4 0 0 1 4-4"/></g></svg>
+                </div>
+                <h2 class="donate-title">PayPal</h2>
+            </div>
+            <p class="donate-description">Support via PayPal (preferred)</p>
+        </div>
+
+        </div>
+
+        <!-- Crypto Cards (2 per row) -->
+        <div class="crypto-grid">
+            <div class="donate-card" onclick="showWallet('eth')">
+                <div class="donate-header">
+                    <div class="donate-icon">
+                        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><g fill="none" fill-rule="evenodd"><circle cx="16" cy="16" r="16" fill="#627eea"/><g fill="#fff" fill-rule="nonzero"><path fill-opacity="0.602" d="M16.498 4v8.87l7.497 3.35z"/><path d="M16.498 4L9 16.22l7.498-3.35z"/><path fill-opacity="0.602" d="M16.498 21.968v6.027L24 17.616z"/><path d="M16.498 27.995v-6.028L9 17.616z"/><path fill-opacity="0.2" d="m16.498 20.573l7.497-4.353l-7.497-3.348z"/><path fill-opacity="0.602" d="m9 16.22l7.498 4.353v-7.701z"/></g></g></svg>
+                    </div>
+                    <h2 class="donate-title">Ethereum</h2>
+                </div>
+                <p class="donate-description">Support via ETH on the Ethereum network</p>
+            </div>
+
+            <div class="donate-card" onclick="showWallet('sol')">
+                <div class="donate-header">
+                    <div class="donate-icon">
+                        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><g fill="none"><circle cx="16" cy="16" r="16" fill="#66f9a1"/><path fill="#fff" d="M9.925 19.687a.6.6 0 0 1 .415-.17h14.366a.29.29 0 0 1 .207.497l-2.838 2.815a.6.6 0 0 1-.415.171H7.294a.291.291 0 0 1-.207-.498zm0-10.517A.6.6 0 0 1 10.34 9h14.366c.261 0 .392.314.207.498l-2.838 2.815a.6.6 0 0 1-.415.17H7.294a.291.291 0 0 1-.207-.497zm12.15 5.225a.6.6 0 0 0-.415-.17H7.294a.291.291 0 0 0-.207.498l2.838 2.815c.11.109.26.17.415.17h14.366a.291.291 0 0 0 .207-.498z"/></g></svg>
+                    </div>
+                    <h2 class="donate-title">Solana</h2>
+                </div>
+                <p class="donate-description">Support via SOL on the Solana network</p>
+            </div>
+
+            <div class="donate-card" onclick="showWallet('btc')">
+                <div class="donate-header">
+                    <div class="donate-icon">
+                        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><g fill="none" fill-rule="evenodd"><circle cx="16" cy="16" r="16" fill="#f7931a"/><path fill="#fff" fill-rule="nonzero" d="M23.189 14.02c.314-2.096-1.283-3.223-3.465-3.975l.708-2.84l-1.728-.43l-.69 2.765c-.454-.114-.92-.22-1.385-.326l.695-2.783L15.596 6l-.708 2.839q-.565-.127-1.104-.26l.002-.009l-2.384-.595l-.46 1.846s1.283.294 1.256.312c.7.175.826.638.805 1.006l-.806 3.235q.073.017.18.057l-.183-.045l-1.13 4.532c-.086.212-.303.531-.793.41c.018.025-1.256-.313-1.256-.313l-.858 1.978l2.25.561c.418.105.828.215 1.231.318l-.715 2.872l1.727.43l.708-2.84q.707.19 1.378.357l-.706 2.828l1.728.43l.715-2.866c2.948.558 5.164.333 6.097-2.333c.752-2.146-.037-3.385-1.588-4.192c1.13-.26 1.98-1.003 2.207-2.538m-3.95 5.538c-.533 2.147-4.148.986-5.32.695l.95-3.805c1.172.293 4.929.872 4.37 3.11m.535-5.569c-.487 1.953-3.495.96-4.47.717l.86-3.45c.975.243 4.118.696 3.61 2.733"/></g></svg>
+                    </div>
+                    <h2 class="donate-title">Bitcoin</h2>
+                </div>
+                <p class="donate-description">Support via BTC on the Bitcoin network</p>
+            </div>
+
+            <div class="donate-card disabled">
+                <div class="donate-header">
+                    <div class="donate-icon">
+                        <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="#f60" d="M12 3a8.99 8.99 0 0 1 8.543 11.813h-2.918V7.288l-5.633 5.869l-5.617-5.861v7.517H3.457A8.9 8.9 0 0 1 3 12.008C3 7.03 7.026 3 12 3m.025 12.822l3.912-3.914V16.5h3.749C18.022 19.246 15.254 21.002 12 21c-3.244 0-6.098-1.91-7.686-4.5h3.748v-4.592z"/></svg>
+                    </div>
+                    <h2 class="donate-title">Monero</h2>
+                </div>
+                <p class="donate-description">Coming soon...</p>
+            </div>
+        </div>
+    </div>
+
+    <div id="notification" class="notification"></div>
+
+    <div id="walletModal" class="modal">
+        <div class="modal-content">
+            <button class="close-modal" onclick="closeModal()">&times;</button>
+            <h2 id="modalTitle"></h2>
+            <div id="walletAddress" class="wallet-address"></div>
+            <button class="copy-button" onclick="copyAddress()">
+                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24">
+                    <path fill="currentColor" d="M19 3h-4.18C14.4 1.84 13.3 1 12 1c-1.3 0-2.4.84-2.82 2H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-7 0c.55 0 1 .45 1 1s-.45 1-1 1s-1-.45-1-1s.45-1 1-1zm7 16H5V5h2v2h10V5h2v14z"/>
+                </svg>
+                Copy Address
+            </button>
+        </div>
+    </div>
+
+    <script>
+        const wallets = {
+            eth: '0xA3AB1B884518bAa642C8abE8160f013fbcd80aD2',
+            sol: 'GQpCYKyy7H1o3UUs6t25sj1xf5i7gbWUeBLBZSSc9WeH',
+            btc: 'bc1p38mvpl9jqxr4cqnsc2lhxf7spzj0tfzmv4dwvx4rrq9qwtfty9hqnjlyky',
+            xmr: 'Coming soon'
+        };
+
+        const titles = {
+            eth: 'Ethereum (ETH) Address',
+            sol: 'Solana (SOL) Address',
+            btc: 'Bitcoin (BTC) Address',
+            xmr: 'Monero (XMR) Address'
+        };
+
+        function showWallet(type) {
+            if (type === 'xmr') return;
+            
+            const modal = document.getElementById('walletModal');
+            const title = document.getElementById('modalTitle');
+            const address = document.getElementById('walletAddress');
+            
+            title.textContent = titles[type];
+            address.textContent = wallets[type];
+            modal.style.display = 'block';
+            modal.classList.add('show');
+        }
+
+        function closeModal() {
+            const modal = document.getElementById('walletModal');
+            modal.classList.remove('show');
+            setTimeout(() => {
+                modal.style.display = 'none';
+            }, 300);
+        }
+
+        function copyAddress() {
+            const address = document.getElementById('walletAddress').textContent;
+            navigator.clipboard.writeText(address)
+                .then(() => showNotification('Wallet address copied to clipboard'))
+                .catch(() => showNotification('Failed to copy address'));
+        }
+
+        function showNotification(message) {
+            const notification = document.getElementById('notification');
+            notification.textContent = message;
+            notification.classList.add('show');
+            
+            setTimeout(() => {
+                notification.classList.remove('show');
+            }, 3000);
+        }
+
+        // Close modal when clicking outside
+        document.getElementById('walletModal').addEventListener('click', function(event) {
+            if (event.target === this) {
+                closeModal();
+            }
+        });
+
+        // Prevent modal content clicks from closing
+        document.querySelector('.modal-content').addEventListener('click', function(event) {
+            event.stopPropagation();
+        });
+    </script>
+</body>
+</html>
\ No newline at end of file
diff --git a/public/index.html b/public/index.html
new file mode 100644
index 0000000..d11d6a4
--- /dev/null
+++ b/public/index.html
@@ -0,0 +1,225 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <title>Sylph - Free AI Access for Everyone</title>
+    <link rel="stylesheet" href="/styles.css">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <script defer src="https://analytics.minoa.cat/script.js" data-website-id="dba618bd-576d-4166-a280-e38df64bf53f"></script>
+    
+    <meta name="description" content="Free access to AI language models through a unified API interface. OpenAI-compatible endpoints for seamless integration with multiple LLMs.">
+    <meta name="keywords" content="Free AI API, free language models, LLM API, OpenAI alternative">
+    <meta name="author" content="M1noa">
+    <meta name="robots" content="index, follow">
+    
+    <meta property="og:title" content="Sylph - Free AI Access">
+    <meta property="og:description" content="Free access to AI language models through a unified API interface. OpenAI-compatible for seamless integration.">
+    <meta property="og:image" content="https://api.sylph.chat/logo.png">
+    <meta property="og:url" content="https://api.sylph.chat">
+    <meta property="og:type" content="website">
+    
+    <meta name="twitter:card" content="summary_large_image">
+    <meta name="twitter:title" content="Sylph - Free AI Access">
+    <meta name="twitter:description" content="Free access to AI language models through a unified API interface. OpenAI-compatible for seamless integration.">
+    <meta name="twitter:image" content="https://api.sylph.chat/logo.png">
+    
+    <meta name="theme-color" content="#ffc0cb">
+    <style>
+        .hero {
+            text-align: center;
+            margin: 2rem auto;
+            max-width: 800px;
+        }
+
+        .description {
+            font-size: 1.2rem;
+            margin: 1rem 0 3rem;
+        }
+
+        .action-buttons {
+            max-width: 900px;
+            margin: 0 auto;
+        }
+
+        .button-row {
+            display: grid;
+            grid-template-columns: repeat(2, 1fr);
+            gap: 1.5rem;
+            margin-bottom: 1.5rem;
+        }
+
+        .feature-card {
+            transition: transform 0.3s cubic-bezier(0.4, 0, 0.2, 1);
+        }
+
+        .feature-card:hover {
+            transform: translateY(-3px);
+        }
+
+        .footer {
+            padding: 1rem 2rem;
+            display: flex;
+            justify-content: space-between;
+            align-items: center;
+            gap: 1rem;
+            border-top: 1px solid var(--border);
+            background: var(--card-bg);
+        }
+
+        .footer .credits {
+            flex: 1;
+        }
+
+        .footer .support {
+            transform: translateY(2px);
+        }
+
+        .paypal-button {
+            display: inline-flex;
+            align-items: center;
+            gap: 0.5rem;
+            padding: 0.5rem 1.25rem;
+            background: #0070ba;
+            color: white;
+            border: none;
+            border-radius: 4px;
+            font-size: 1rem;
+            cursor: pointer;
+            text-decoration: none;
+            transition: all 0.3s ease;
+        }
+
+        .paypal-button:hover {
+            background: #005ea6;
+            transform: translateY(-2px);
+        }
+
+        .paypal-button svg {
+            width: 18px;
+            height: 18px;
+            fill: currentColor;
+        }
+
+        @media (max-width: 768px) {
+            .button-row {
+                grid-template-columns: 1fr;
+                margin-bottom: 1rem;
+            }
+
+            .footer {
+                flex-direction: column;
+                gap: 1rem;
+                text-align: center;
+                padding: 1rem;
+            }
+        }
+    </style>
+</head>
+<body>
+    <div class="container">
+        <div class="hero">
+            <h1 class="site-header" onclick="location.href='/'">/Sylph</h1>
+            <p class="description">Free access to AI language models through a unified, OpenAI-compatible API router.</p>
+            
+            <div class="action-buttons">
+                <div class="button-row">
+                    <a href="/docs" class="feature-card">
+                        <div class="card-icon">ðŸ“š</div>
+                        <div class="card-content">
+                            <h2>API Docs</h2>
+                            <p>Get started with our OpenAI-compatible API</p>
+                            <div class="card-badge">View Docs</div>
+                        </div>
+                    </a>
+                    <a href="/models" class="feature-card">
+                        <div class="card-icon">ðŸ¤–</div>
+                        <div class="card-content">
+                            <h2>Models</h2>
+                            <p>Browse available free AI models</p>
+                            <div class="card-badge">Browse All</div>
+                        </div>
+                    </a>
+                </div>
+                <div class="button-row">
+                    <a href="https://github.com/m1noa/Sylph" target="_blank" class="feature-card">
+                        <div class="card-icon">ðŸ› ï¸</div>
+                        <div class="card-content">
+                            <h2>GitHub</h2>
+                            <p>View the source code and contribute</p>
+                            <div class="card-badge">Open Source</div>
+                        </div>
+                    </a>
+                    <a href="/donate" class="feature-card">
+                        <div class="card-icon">â¤ï¸</div>
+                        <div class="card-content">
+                            <h2>Support Sylph</h2>
+                            <p>Support via PayPal or cryptocurrency</p>
+                            <div class="card-badge">Donate</div>
+                        </div>
+                    </a>
+                </div>
+            </div>
+        </div>
+    </div>
+
+    <footer class="footer">
+        <div class="credits">
+            <div class="loading-animation text-muted" style="font-style: italic;">
+                Loading providers<span class="dots">...</span>
+            </div>
+        </div>
+        <div class="support">
+            <a href="https://paypal.me/m1noa" target="_blank" class="paypal-button">
+                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
+                    <path fill="currentColor" d="M7.016 19.198h-4.2a.562.562 0 0 1-.555-.65L5.093.584A.69.69 0 0 1 5.776 0h7.222c3.417 0 5.904 2.488 5.846 5.5q-.008.376-.066.747A6.794 6.794 0 0 1 12.071 12H8.743a.69.69 0 0 0-.682.583l-.325 2.056l-.013.083l-.692 4.39zM19.79 6.142c-.01.087-.01.175-.023.261a7.76 7.76 0 0 1-7.695 6.598H9.007l-.283 1.795l-.013.083l-.692 4.39l-.134.843l-.014.088H6.86l-.497 3.15a.562.562 0 0 0 .555.65h3.612c.34 0 .63-.249.683-.585l.952-6.031a.69.69 0 0 1 .683-.584h2.126a6.79 6.79 0 0 0 6.707-5.752c.306-1.95-.466-3.744-1.89-4.906z"/>
+                </svg>
+                Support on PayPal
+            </a>
+        </div>
+        <div class="legal-links">
+            <a href="/privacy">Privacy Policy</a>
+            <a href="/tos">Terms of Service</a>
+        </div>
+    </footer>
+
+    <script>
+        async function loadProviders() {
+            try {
+                const response = await fetch('/v1/models');
+                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
+                
+                const data = await response.json();
+                const providers = new Set();
+
+                if (!data.data?.length) {
+                    throw new Error('No providers found');
+                }
+
+                data.data.forEach(model => {
+                    const url = model.owned_by;
+                    if (url && !url.includes('localhost')) {
+                        providers.add(model.owned_by);
+                    }
+                });
+
+                const creditsHtml = Array.from(providers)
+                    .map(url => `<a href="${url}" target="_blank">${new URL(url).hostname}</a>`)
+                    .join('');
+
+                document.querySelector('.credits').innerHTML = 
+                    `Thanks to our API providers: <div class="provider-links">${creditsHtml}</div>`;
+            } catch (error) {
+                console.error('Failed to load providers:', error);
+                document.querySelector('.credits').innerHTML = `
+                    <div class="text-muted" style="text-align: center;">
+                        Unable to load providers. Please try again later.
+                        ${process.env.DEBUG_MODE === 'true' ? `<br><small>${error.message}</small>` : ''}
+                    </div>`;
+            }
+        }
+
+        // Load providers on page load
+        loadProviders();
+    </script>
+</body>
+</html>
diff --git a/public/models.html b/public/models.html
new file mode 100644
index 0000000..45ebaa3
--- /dev/null
+++ b/public/models.html
@@ -0,0 +1,729 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>Models - Sylph</title>
+    <link rel="stylesheet" href="/styles.css">
+    <style>
+        .controls {
+            margin-bottom: 2rem;
+            display: flex;
+            gap: 1rem;
+            flex-wrap: wrap;
+            align-items: center;
+        }
+
+        /* Control group for filters and sorting */
+        .control-group {
+            display: flex;
+            gap: 0.5rem;
+            align-items: center;
+            flex-wrap: wrap;
+        }
+
+        .select-control {
+            padding: 0.75rem 1rem;
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 8px;
+            color: var(--text);
+            font-size: 1rem;
+            min-width: 150px;
+            cursor: pointer;
+        }
+
+        .select-control:focus {
+            border-color: var(--text);
+            outline: none;
+        }
+
+        @media (max-width: 768px) {
+            .controls {
+                flex-direction: column;
+                gap: 1rem;
+            }
+
+            .control-group {
+                flex: 1 1 100%;
+                flex-direction: column;
+                width: 100%;
+            }
+            
+            .select-control, .search-input {
+                width: 100%;
+            }
+
+            .model-card {
+                aspect-ratio: 4/3;
+                height: auto;
+                padding: 1rem;
+            }
+
+            .model-stats {
+                gap: 0.5rem;
+            }
+
+            .stat-item {
+                min-width: 0;
+                flex: 1 1 calc(50% - 0.25rem);
+            }
+        }
+
+        .search-box {
+            flex: 1;
+            min-width: 250px;
+            position: relative;
+        }
+
+        .search-input {
+            width: 100%;
+            padding: 0.75rem 1rem;
+            padding-left: 2.5rem;
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 8px;
+            color: var(--text);
+            font-size: 1rem;
+            transition: all 0.3s ease;
+        }
+
+        .search-input:focus {
+            border-color: var(--text);
+            outline: none;
+            box-shadow: 0 0 0 3px rgba(255, 255, 255, 0.1);
+        }
+
+        .search-icon {
+            position: absolute;
+            left: 0.75rem;
+            top: 50%;
+            transform: translateY(-50%);
+            color: var(--text-muted);
+            pointer-events: none;
+        }
+
+        .models-container {
+            max-width: 1400px;
+            margin: 0 auto;
+            padding: 0 1rem;
+        }
+
+        .provider-section {
+            margin: 3rem 0;
+            animation: slideUp 0.5s ease-out backwards;
+        }
+
+        @keyframes slideUp {
+            from {
+                opacity: 0;
+                transform: translateY(20px);
+            }
+            to {
+                opacity: 1;
+                transform: translateY(0);
+            }
+        }
+
+        .provider-header {
+            display: flex;
+            align-items: center;
+            gap: 1rem;
+            margin-bottom: 2rem;
+            padding-bottom: 0.5rem;
+            border-bottom: 2px solid var(--border);
+        }
+
+        .provider-name {
+            font-size: 1.8rem;
+            color: var(--text-bright);
+            margin: 0;
+            display: flex;
+            align-items: center;
+            gap: 1rem;
+            flex: 1;
+        }
+
+        .provider-stats {
+            background: var(--card-bg);
+            padding: 0.5rem 1rem;
+            border-radius: 20px;
+            font-size: 0.9rem;
+            color: var(--text-muted);
+            border: 1px solid var(--border);
+        }
+
+        .models-grid {
+            display: grid;
+            grid-template-columns: repeat(3, 1fr); /* 3 models per row by default */
+            gap: 1.5rem;
+        }
+
+        @media (max-width: 1400px) {
+            .models-grid {
+                grid-template-columns: repeat(2, 1fr); /* 2 models per row on smaller screens */
+            }
+        }
+
+        .model-card {
+            background: var(--card-bg);
+            border: 1px solid var(--border);
+            border-radius: 16px;
+            padding: 1.5rem;
+            position: relative;
+            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
+        }
+
+        .model-card:hover {
+            transform: translateY(-4px);
+            border-color: var(--text);
+            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.15);
+        }
+
+        .model-card::before {
+            content: '';
+            position: absolute;
+            top: 0;
+            left: 0;
+            right: 0;
+            height: 4px;
+            background: linear-gradient(90deg, var(--gradient-start), var(--gradient-end));
+            opacity: 0;
+            transition: opacity 0.3s ease;
+        }
+
+        .model-card:hover::before {
+            opacity: 1;
+        }
+
+        .model-header {
+            display: flex;
+            align-items: flex-start;
+            gap: 1rem;
+            margin-bottom: 1.5rem;
+        }
+
+        .model-name-section {
+            flex: 1;
+        }
+
+        .model-id {
+            font-size: 1.2rem;
+            font-weight: bold;
+            color: var(--text-bright);
+            margin: 0 0 0.25rem 0;
+        }
+
+        .model-status {
+            width: 16px;
+            height: 16px;
+            border-radius: 50%;
+            margin-top: 0.25rem;
+            position: relative;
+            flex-shrink: 0;
+            cursor: help;
+        }
+
+        .status-operational { 
+            background: linear-gradient(45deg, #22c55e, #16a34a);
+            box-shadow: 0 0 10px rgba(34, 197, 94, 0.3);
+        }
+
+        .status-unknown { 
+            background: linear-gradient(45deg, #eab308, #ca8a04);
+            box-shadow: 0 0 10px rgba(234, 179, 8, 0.3);
+        }
+
+        .status-error { 
+            background: linear-gradient(45deg, #ef4444, #dc2626);
+            box-shadow: 0 0 10px rgba(239, 68, 68, 0.3);
+        }
+
+        .model-stats {
+            display: flex;
+            gap: 1rem;
+            margin-bottom: 1.5rem;
+            flex-wrap: wrap;
+        }
+
+        .stat-item {
+            flex: 1;
+            min-width: 120px; /* ensure reasonable size on narrow screens */
+        }
+
+        .stat-item {
+            background: rgba(0, 0, 0, 0.2);
+            padding: 0.7rem;
+            border-radius: 12px;
+            border: 1px solid var(--border);
+        }
+
+        .stat-label {
+            font-size: 0.8rem;
+            color: var(--text-muted);
+            margin-bottom: 0.25rem;
+        }
+
+        .stat-value {
+            font-size: 1.1rem;
+            color: var(--text-bright);
+        }
+
+        .model-badges {
+            display: flex;
+            flex-wrap: wrap;
+            gap: 0.5rem;
+        }
+
+        .badge {
+            font-size: 0.8rem;
+            padding: 0.25rem 0.75rem;
+            border-radius: 20px;
+            background: var(--button-bg);
+            color: var(--text);
+            border: 1px solid var(--border);
+        }
+
+        .badge.capability {
+            background: rgba(99, 102, 241, 0.1);
+            color: #818cf8;
+            border-color: rgba(99, 102, 241, 0.2);
+        }
+
+        .status-tooltip {
+            position: absolute;
+            bottom: calc(100% + 10px);
+            left: 50%;
+            transform: translateX(-50%) translateY(10px);
+            background: rgba(255, 255, 255, 0.1); /* glass effect background */
+            padding: 0.75rem 1rem;
+            border-radius: 8px;
+            font-size: 0.9rem;
+            color: var(--text);
+            white-space: nowrap;
+            opacity: 0;
+            visibility: hidden;
+            transition: all 0.2s ease;
+            z-index: 10;
+            border: 1px solid rgba(255, 255, 255, 0.2);
+            pointer-events: none;
+            backdrop-filter: blur(8px);
+            -webkit-backdrop-filter: blur(8px);
+            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
+        }
+
+        .status-tooltip::after {
+            content: '';
+            position: absolute;
+            bottom: -6px;
+            left: 50%;
+            transform: translateX(-50%);
+            border-style: solid;
+            border-width: 6px 6px 0 6px;
+            border-color: var(--bg-dark) transparent transparent transparent;
+        }
+
+        .model-status:hover .status-tooltip {
+            opacity: 1;
+            visibility: visible;
+            transform: translateX(-50%) translateY(0);
+        }
+
+        @media (max-width: 1200px) {
+            .models-grid {
+                grid-template-columns: repeat(2, 1fr);
+                gap: 1rem;
+            }
+        }
+
+        @media (max-width: 900px) {
+            .models-grid {
+                grid-template-columns: 1fr;
+            }
+
+            .models-container {
+                padding: 0 0.5rem;
+            }
+
+            .provider-section {
+                margin: 2rem 0;
+            }
+
+            .provider-name {
+                font-size: 1.5rem;
+            }
+        }
+
+        @media (max-width: 768px) {
+            .provider-header {
+                flex-direction: column;
+                align-items: flex-start;
+                gap: 0.5rem;
+            }
+
+            .model-stats {
+                grid-template-columns: 1fr;
+            }
+        }
+
+        .loading {
+            display: flex;
+            align-items: center;
+            justify-content: center;
+            min-height: 200px;
+            color: var(--text-muted);
+            font-size: 1.2rem;
+        }
+
+        .loading::after {
+            content: '...';
+            animation: dots 1.5s steps(4, end) infinite;
+            width: 1.5em;
+            display: inline-block;
+            text-align: left;
+        }
+
+        @keyframes dots {
+            0%, 20% { content: '.'; }
+            40% { content: '..'; }
+            60% { content: '...'; }
+            80%, 100% { content: ''; }
+        }
+
+        .no-results {
+            text-align: center;
+            padding: 3rem;
+            color: var(--text-muted);
+            background: var(--card-bg);
+            border-radius: 12px;
+            border: 1px solid var(--border);
+        }
+    </style>
+</head>
+<body>
+
+    <main class="models-container">
+        <h1 class="site-header" onclick="location.href='/';">/models</h1>
+        <p class="text-muted text-center" style="opacity: 0.7; margin-bottom: 2rem;">Base URL for API requests: <code>{window.location.origin}/v1</code></p>
+        <div class="controls">
+            <!-- Search -->
+            <div class="search-box">
+                <input type="text" id="search" class="search-input" placeholder="Search models..." aria-label="Search models">
+                <svg class="search-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
+                    <path d="M21 21L16.65 16.65M19 11C19 15.4183 15.4183 19 11 19C6.58172 19 3 15.4183 3 11C3 6.58172 6.58172 3 11 3C15.4183 3 19 6.58172 19 11Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
+                </svg>
+            </div>
+
+            <!-- Sorting and Filtering -->
+            <div class="control-group">
+                <select id="sortBy" class="select-control" aria-label="Sort models by">
+                    <option value="provider-asc">Provider (A-Z)</option>
+                    <option value="provider-desc">Provider (Z-A)</option>
+                    <option value="name-asc">Name (A-Z)</option>
+                    <option value="name-desc">Name (Z-A)</option>
+                    <option value="context-asc">Context Length (Low-High)</option>
+                    <option value="context-desc">Context Length (High-Low)</option>
+                </select>
+
+                <select id="filterProvider" class="select-control" aria-label="Filter by provider">
+                    <option value="">All Providers</option>
+                </select>
+
+                <select id="filterCapability" class="select-control" aria-label="Filter by capability">
+                    <option value="">All Capabilities</option>
+                    <option value="text">Text</option>
+                    <option value="images">Images</option>
+                </select>
+            </div>
+        </div>
+        <div id="models-content"></div>
+    </main>
+
+    <script>
+        // Debounce function for search
+        function debounce(func, wait) {
+            let timeout;
+            return function executedFunction(...args) {
+                const later = () => {
+                    clearTimeout(timeout);
+                    func(...args);
+                };
+                clearTimeout(timeout);
+                timeout = setTimeout(later, wait);
+            };
+        }
+
+        function formatNumber(num) {
+            if (num >= 1000000) return (num/1000000).toFixed(1) + 'M';
+            if (num >= 1000) return (num/1000).toFixed(1) + 'K';
+            return num.toString();
+        }
+
+        function formatDate(timestamp) {
+            const date = new Date(timestamp);
+            const now = new Date();
+            const diff = Math.floor((now - date) / 1000);
+
+            if (diff < 60) return 'just now';
+            if (diff < 3600) return Math.floor(diff/60) + 'm ago';
+            if (diff < 86400) return Math.floor(diff/3600) + 'h ago';
+            return Math.floor(diff/86400) + 'd ago';
+        }
+
+        class ModelsView {
+            constructor() {
+                this.models = [];
+                this.searchInput = document.getElementById('search');
+                this.sortSelect = document.getElementById('sortBy');
+                this.providerFilter = document.getElementById('filterProvider');
+                this.capabilityFilter = document.getElementById('filterCapability');
+                this.content = document.getElementById('models-content');
+                
+                // Set up live search
+                this.searchInput.addEventListener('input', () => this.renderModels());
+                this.sortSelect.addEventListener('change', () => this.renderModels());
+                this.providerFilter.addEventListener('change', () => this.renderModels());
+                this.capabilityFilter.addEventListener('change', () => this.renderModels());
+            }
+
+            updateFilterOptions() {
+                // Update provider filter options
+                const providers = [...new Set(this.models.map(m => m.owned_by))].sort();
+                this.providerFilter.innerHTML = '<option value="">All Providers</option>' +
+                    providers.map(p => `<option value="${p}">${p.replace(/^https?:\/\//, '')}</option>`).join('');
+            }
+
+            async loadModels() {
+                this.content.innerHTML = '<div class="loading">Loading available models</div>';
+
+                try {
+                    // Load models
+                    const response = await fetch('/v1/models');
+                    const data = await response.json();
+                    this.models = data.data;
+
+                    // Update filter options after loading models
+                    this.updateFilterOptions();
+
+                    // Add status filter
+                    const statusFilter = document.createElement('select');
+                    statusFilter.id = 'filterStatus';
+                    statusFilter.className = 'select-control';
+                    statusFilter.innerHTML = `
+                        <option value="">All Status</option>
+                        <option value="operational">Operational</option>
+                        <option value="error">Error</option>
+                        <option value="unknown">Unknown</option>
+                    `;
+                    statusFilter.addEventListener('change', () => this.renderModels());
+
+                    // Add context length filter
+                    const contextFilter = document.createElement('select');
+                    contextFilter.id = 'filterContext';
+                    contextFilter.className = 'select-control';
+                    contextFilter.innerHTML = `
+                        <option value="">All Context Lengths</option>
+                        <option value="0-8192">Up to 8K</option>
+                        <option value="8192-32768">8K to 32K</option>
+                        <option value="32768-131072">32K to 128K</option>
+                        <option value="131072-">128K+</option>
+                    `;
+                    contextFilter.addEventListener('change', () => this.renderModels());
+
+                    // Add filters to control group
+                    const controlGroup = document.querySelector('.control-group');
+                    controlGroup.appendChild(statusFilter);
+                    controlGroup.appendChild(contextFilter);
+
+                    // Store filter references
+                    this.statusFilter = statusFilter;
+                    this.contextFilter = contextFilter;
+
+                    this.renderModels();
+                } catch (error) {
+                    console.error('Error loading models:', error);
+                    this.content.innerHTML = `
+                        <div class="error">
+                            Failed to load models. Please try again later.
+                            ${process.env.DEBUG_MODE === 'true' ? `<br><small>${error.message}</small>` : ''}
+                        </div>
+                    `;
+                }
+            }
+
+            renderModels() {
+                const searchTerm = this.searchInput.value.toLowerCase();
+                const providerFilter = this.providerFilter.value;
+                const capabilityFilter = this.capabilityFilter.value;
+                const statusFilter = this.statusFilter?.value;
+                const contextFilter = this.contextFilter?.value;
+                const [sortField, sortOrder] = this.sortSelect.value.split('-');
+                
+                // Filter models
+                let filteredModels = this.models.filter(model => {
+                    const matchesSearch = !searchTerm ||
+                        model.id.toLowerCase().includes(searchTerm) ||
+                        model.root?.toLowerCase().includes(searchTerm) ||
+                        Object.keys(model.capabilities || {})
+                            .some(cap => cap.toLowerCase().includes(searchTerm));
+
+                    const matchesProvider = !providerFilter ||
+                        model.owned_by === providerFilter;
+
+                    const matchesCapability = !capabilityFilter ||
+                        model.capabilities?.[capabilityFilter];
+
+                    const matchesStatus = !statusFilter ||
+                        model.health?.status === statusFilter;
+
+                    const matchesContext = !contextFilter || (() => {
+                        if (!contextFilter) return true;
+                        const [min, max] = contextFilter.split('-').map(Number);
+                        const contextLength = model.context_length || 0;
+                        return max ?
+                            contextLength >= min && contextLength < max :
+                            contextLength >= min;
+                    })();
+
+                    return matchesSearch && matchesProvider && matchesCapability &&
+                           matchesStatus && matchesContext;
+                });
+
+                // Sort models
+                filteredModels.sort((a, b) => {
+                    let comparison = 0;
+                    switch (sortField) {
+                        case 'provider':
+                            comparison = a.owned_by.localeCompare(b.owned_by);
+                            break;
+                        case 'name':
+                            comparison = a.id.localeCompare(b.id);
+                            break;
+                        case 'context':
+                            comparison = (a.context_length || 0) - (b.context_length || 0);
+                            break;
+                    }
+                    return sortOrder === 'desc' ? -comparison : comparison;
+                });
+
+                // Group by provider
+                const modelsByProvider = new Map();
+                filteredModels.forEach(model => {
+                    const provider = model.owned_by || 'Unknown Provider';
+                    if (!modelsByProvider.has(provider)) {
+                        modelsByProvider.set(provider, []);
+                    }
+                    modelsByProvider.get(provider).push(model);
+                });
+
+                // Sort providers by model count
+                // Sort providers by model count (ascending) and then by name
+                const sortedProviders = Array.from(modelsByProvider.entries())
+                    .sort((a, b) => {
+                        const countDiff = a[1].length - b[1].length; // Ascending order
+                        return countDiff !== 0 ? countDiff : a[0].localeCompare(b[0]); // Then by name
+                    });
+
+                if (sortedProviders.length === 0) {
+                    this.content.innerHTML = `
+                        <div class="no-results">
+                            No models found matching "${searchTerm}"
+                        </div>
+                    `;
+                    return;
+                }
+
+                this.content.innerHTML = '';
+
+                sortedProviders.forEach(([provider, models], index) => {
+                    const section = document.createElement('section');
+                    section.className = 'provider-section';
+                    section.style.animationDelay = `${index * 0.1}s`;
+
+                    const operational = models.filter(m => m.health?.status === 'operational').length;
+                    const providerName = provider.replace(/^https?:\/\//, '');
+
+                    section.innerHTML = `
+                        <div class="provider-header">
+                            <h2 class="provider-name">${providerName}</h2>
+                            <div class="provider-stats">
+                                ${operational}/${models.length} operational
+                            </div>
+                        </div>
+                        <div class="models-grid">
+                            ${models
+                                .sort((a, b) => {
+                                    // Sort by health status first (operational > unknown > error)
+                                    const statusOrder = { operational: 0, unknown: 1, error: 2 };
+                                    const statusDiff = (statusOrder[a.health?.status] || 1) - (statusOrder[b.health?.status] || 1);
+                                    if (statusDiff !== 0) return statusDiff;
+                                    
+                                    // Then by name
+                                    return a.id.localeCompare(b.id);
+                                })
+                                .map(model => {
+                                    const statusClass = model.health?.status === 'operational' ? 'operational' :
+                                                      model.health?.status === 'unknown' ? 'unknown' : 'error';
+                                    
+                                    const capabilities = model.capabilities ? 
+                                        Object.entries(model.capabilities)
+                                            .filter(([_, v]) => v)
+                                            .map(([k]) => `<span class="badge capability">${k}</span>`)
+                                            .join('') : '';
+
+                                    const context = model.context_length ? 
+                                        `<div class="stat-item">
+                                            <div class="stat-label">Context Window</div>
+                                            <div class="stat-value">${formatNumber(model.context_length)}</div>
+                                        </div>` : '';
+
+                                    const latency = model.health?.latency ?
+                                        `<div class="stat-item">
+                                            <div class="stat-label">Latency</div>
+                                            <div class="stat-value">~${model.health.latency}ms</div>
+                                        </div>` : '';
+
+                                    return `
+                                        <div class="model-card">
+                                            <div class="model-header">
+                                                <div class="model-name-section">
+                                                    <h3 class="model-id">${model.id}</h3>
+                                                </div>
+                                                <div class="model-status status-${statusClass}">
+                                                    <div class="status-tooltip">
+                                                        Status: ${model.health?.status || 'unknown'}<br>
+                                                        Latency: ${model.health?.latency ? `~${model.health.latency}ms` : 'N/A'}<br>
+                                                        Last Check: ${model.health?.lastCheck ? formatDate(model.health.lastCheck) : 'never'}
+                                                    </div>
+                                                </div>
+                                            </div>
+                                            
+                                            <div class="model-stats">
+                                                ${context}
+                                                ${latency}
+                                            </div>
+
+                                            ${capabilities ? `
+                                                <div class="model-badges">
+                                                    ${capabilities}
+                                                </div>
+                                            ` : ''}
+                                        </div>
+                                    `;
+                                }).join('')}
+                        </div>
+                    `;
+
+                    this.content.appendChild(section);
+                });
+            }
+        }
+
+        // Initialize and start
+        const view = new ModelsView();
+        view.loadModels();
+
+        // Manual refresh only
+    </script>
+</body>
+</html>
diff --git a/public/privacy.html b/public/privacy.html
new file mode 100644
index 0000000..00ea410
--- /dev/null
+++ b/public/privacy.html
@@ -0,0 +1,108 @@
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>Privacy Policy - Sylph</title>
+    <link rel="stylesheet" href="/styles.css">
+    <style>
+        .container {
+            max-width: 800px;
+            margin: 0 auto;
+            padding: 2rem;
+        }
+
+        .legal-content {
+            margin-top: 2rem;
+        }
+
+        .legal-content h1 {
+            color: var(--text-bright);
+            margin-bottom: 1.5rem;
+            font-size: 2rem;
+        }
+
+        .legal-content h2 {
+            color: var(--text-bright);
+            margin-top: 2rem;
+            margin-bottom: 1rem;
+            font-size: 1.5rem;
+        }
+
+        .legal-content p {
+            margin-bottom: 1rem;
+            line-height: 1.6;
+        }
+
+        .legal-content ul {
+            margin: 1rem 0 1.5rem;
+            padding-left: 1.5rem;
+        }
+
+        .legal-content li {
+            margin-bottom: 0.5rem;
+            line-height: 1.5;
+        }
+
+        .legal-content strong {
+            color: var(--text-bright);
+        }
+
+        @media (max-width: 768px) {
+            .container {
+                padding: 1rem;
+            }
+            
+            .legal-content h1 {
+                font-size: 1.75rem;
+            }
+
+            .legal-content h2 {
+                font-size: 1.25rem;
+            }
+        }
+    </style>
+</head>
+<body>
+    <div class="legal-content">
+        <h1>Privacy Policy</h1>
+        <p>Last updated: April 3, 2025</p>
+
+        <p>This Privacy Policy describes how Sylph handles information when you use our API service.</p>
+
+        <h2>1. Information We Collect</h2>
+        <p>Sylph acts as a proxy layer and does not store the content of your API requests or the responses generated by the underlying AI models. We may collect the following types of information:</p>
+        <ul>
+            <li><strong>Usage Data:</strong> We may collect metadata about your API requests, such as the timestamp, the model used, request duration, and status codes. This data is used for monitoring, improving the service, and enforcing rate limits.</li>
+            <li><strong>API Keys:</strong> If authentication is enabled, we store a secure representation (e.g., hash) of API keys for verification purposes. The original keys are not stored.</li>
+            <li><strong>Error Logs:</strong> We may log error details to diagnose and fix issues with the service. These logs do not contain the content of your prompts or responses.</li>
+        </ul>
+
+        <h2>2. How We Use Information</h2>
+        <p>We use the collected information solely for the purpose of operating, maintaining, and improving the Sylph service. This includes:</p>
+        <ul>
+            <li>Providing API functionality.</li>
+            <li>Monitoring service performance and uptime.</li>
+            <li>Troubleshooting errors and technical issues.</li>
+            <li>Enforcing rate limits and security policies.</li>
+            <li>Analyzing usage patterns to optimize resource allocation.</li>
+        </ul>
+
+        <h2>3. Information Sharing</h2>
+        <p>We do not share your usage data or API key information with third parties, except as required by law or to protect the security and integrity of our service.</p>
+        <p>Your prompts and the generated responses are passed directly to the respective third-party AI providers you select via the model ID. We do not control how these providers handle your data. Please review the privacy policies of the specific AI providers you use.</p>
+
+        <h2>4. Data Security</h2>
+        <p>We implement reasonable security measures to protect the metadata we collect. However, no method of transmission over the Internet or electronic storage is 100% secure.</p>
+
+        <h2>5. Data Retention</h2>
+        <p>We retain usage metadata and error logs only for as long as necessary to fulfill the purposes outlined in this policy, or as required by law. Typically, logs are rotated and deleted periodically.</p>
+
+        <h2>6. Changes to This Policy</h2>
+        <p>We may update this Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page.</p>
+
+        <h2>7. Contact Us</h2>
+        <p>If you have any questions about this Privacy Policy, please contact us via the GitHub repository.</p>
+    </div>
+</body>
+</html>
\ No newline at end of file
diff --git a/public/robots.txt b/public/robots.txt
new file mode 100644
index 0000000..55037f5
--- /dev/null
+++ b/public/robots.txt
@@ -0,0 +1,6 @@
+User-agent: *
+Allow: /
+Disallow: /health
+Disallow: /v1/*
+
+Sitemap: https://api.sylph.chat/sitemap.xml
diff --git a/public/sitemap.xml b/public/sitemap.xml
new file mode 100644
index 0000000..d8d5dc9
--- /dev/null
+++ b/public/sitemap.xml
@@ -0,0 +1,21 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
+  <url>
+    <loc>https://api.sylph.chat/</loc>
+    <lastmod>2025-04-03</lastmod>
+    <changefreq>weekly</changefreq>
+    <priority>1.0</priority>
+  </url>
+  <url>
+    <loc>https://api.sylph.chat/docs</loc>
+    <lastmod>2025-04-03</lastmod>
+    <changefreq>weekly</changefreq>
+    <priority>0.9</priority>
+  </url>
+  <url>
+    <loc>https://api.sylph.chat/models</loc>
+    <lastmod>2025-04-03</lastmod>
+    <changefreq>daily</changefreq>
+    <priority>0.8</priority>
+  </url>
+</urlset>
\ No newline at end of file
diff --git a/public/styles.css b/public/styles.css
new file mode 100644
index 0000000..a8d6ea5
--- /dev/null
+++ b/public/styles.css
@@ -0,0 +1,415 @@
+/* Import fonts */
+@import url('https://fonts.googleapis.com/css2?family=Ubuntu+Mono:wght@400;700&display=swap');
+@import url('https://fonts.googleapis.com/css2?family=Rubik+80s+Fade&display=swap');
+
+/* Base theme variables */
+:root {
+    --background: #000000;
+    --text: rgba(255, 192, 203, 0.85);
+    --text-muted: rgba(255, 192, 203, 0.5);
+    --text-bright: rgba(255, 192, 203, 0.95);
+    --gradient-start: rgba(255, 192, 203, 0.2);
+    --gradient-end: rgba(255, 182, 193, 0.2);
+    --card-bg: rgba(255, 192, 203, 0.1);
+    --card-hover: rgba(255, 192, 203, 0.15);
+    --border: rgba(255, 192, 203, 0.2);
+    --hover-border: rgba(255, 192, 203, 0.4);
+    --button-bg: rgba(255, 192, 203, 0.15);
+    --button-hover: rgba(255, 192, 203, 0.25);
+    --highlight-bg: rgba(255, 192, 203, 0.3);
+    --highlight-text: rgba(255, 255, 255, 0.95);
+    --scrollbar-bg: rgba(255, 192, 203, 0.05);
+    --scrollbar-thumb: rgba(255, 192, 203, 0.2);
+    --scrollbar-thumb-hover: rgba(255, 192, 203, 0.3);
+    --link-color: #ff9eb5;
+    --link-hover: #ffb8c9;
+    --status-green: rgba(80, 250, 123, 0.8);
+    --status-yellow: rgba(241, 250, 140, 0.8);
+    --status-red: rgba(255, 85, 85, 0.8);
+}
+
+/* Reset and base styles */
+* {
+    box-sizing: border-box;
+    margin: 0;
+    padding: 0;
+}
+
+/* Custom scrollbar */
+::-webkit-scrollbar {
+    width: 12px;
+    height: 12px;
+}
+
+::-webkit-scrollbar-track {
+    background: var(--scrollbar-bg);
+    border-radius: 6px;
+}
+
+::-webkit-scrollbar-thumb {
+    background: var(--scrollbar-thumb);
+    border-radius: 6px;
+    border: 3px solid var(--background);
+}
+
+::-webkit-scrollbar-thumb:hover {
+    background: var(--scrollbar-thumb-hover);
+}
+
+/* Selection highlight */
+::selection {
+    background: var(--highlight-bg);
+    color: var(--highlight-text);
+}
+
+body {
+    background-color: var(--background);
+    color: var(--text);
+    font-family: 'Ubuntu Mono', monospace;
+    line-height: 1.6;
+    min-height: 100vh;
+    display: flex;
+    flex-direction: column;
+    scrollbar-width: thin;
+    scrollbar-color: var(--scrollbar-thumb) var(--scrollbar-bg);
+}
+
+/* Typography */
+h1, h2, h3, h4, h5, h6 {
+    color: var(--text-bright);
+    font-weight: normal;
+    line-height: 1.2;
+}
+
+.site-header {
+    font-family: 'Rubik 80s Fade', cursive;
+    font-size: 3.5rem;
+    margin: 2rem 0;
+    text-align: center;
+    color: var(--text-bright);
+    animation: glow 2s ease-in-out infinite alternate;
+    cursor: pointer;
+}
+
+.site-header:hover {
+    text-shadow: 0 0 15px var(--text),
+                 0 0 30px var(--text);
+}
+
+@keyframes glow {
+    from {
+        text-shadow: 0 0 5px var(--text-muted),
+                     0 0 10px var(--text-muted);
+    }
+    to {
+        text-shadow: 0 0 10px var(--text),
+                     0 0 20px var(--text);
+    }
+}
+
+/* Links */
+a {
+    color: var(--link-color);
+    text-decoration: none;
+    transition: all 0.3s ease;
+    position: relative;
+}
+
+a:hover {
+    color: var(--link-hover);
+}
+
+a::after {
+    content: '';
+    position: absolute;
+    width: 100%;
+    height: 1px;
+    bottom: -2px;
+    left: 0;
+    background-color: var(--link-hover);
+    transform: scaleX(0);
+    transform-origin: bottom right;
+    transition: transform 0.3s ease;
+}
+
+.feature-card a::after {
+    display: none;
+}
+
+/* Feature Cards */
+.feature-card {
+    background: var(--card-bg);
+    border: 1px solid var(--border);
+    border-radius: 16px;
+    padding: 2rem;
+    transition: all 0.3s ease-out;
+    margin-bottom: 1.5rem;
+    position: relative;
+    display: flex;
+    align-items: flex-start;
+    gap: 1.5rem;
+    overflow: hidden;
+    text-decoration: none;
+}
+
+.feature-card:hover {
+    border-color: var(--hover-border);
+    transform: translateY(-3px);
+    background: var(--card-hover);
+    box-shadow: 0 8px 24px rgba(255, 192, 203, 0.15);
+}
+
+.feature-card:hover .card-badge {
+    transform: translateY(0);
+    opacity: 1;
+}
+
+.feature-card .card-icon {
+    font-size: 2rem;
+    min-width: 48px;
+    height: 48px;
+    display: flex;
+    align-items: center;
+    justify-content: center;
+    background: var(--button-bg);
+    border-radius: 12px;
+    border: 1px solid var(--border);
+    transition: all 0.3s ease;
+}
+
+.feature-card:hover .card-icon {
+    background: var(--button-hover);
+    border-color: var(--text);
+    transform: scale(1.1) rotate(-5deg);
+}
+
+.feature-card .card-content {
+    flex: 1;
+}
+
+.feature-card h2 {
+    font-size: 1.5rem;
+    margin-bottom: 0.5rem;
+    color: var(--text-bright);
+}
+
+.feature-card p {
+    color: var(--text-muted);
+    margin-bottom: 1.5rem;
+}
+
+.feature-card .card-badge {
+    position: absolute;
+    bottom: 1.5rem;
+    right: 1.5rem;
+    background: var(--button-bg);
+    padding: 0.5rem 1rem;
+    border-radius: 20px;
+    font-size: 0.9rem;
+    color: var(--text);
+    border: 1px solid var(--border);
+    transform: translateY(20px);
+    opacity: 0;
+    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
+}
+
+/* Main container */
+.container {
+    max-width: 1200px;
+    margin: 0 auto;
+    padding: 2rem;
+    flex: 1;
+}
+
+/* Footer */
+.footer {
+    background: var(--card-bg);
+    padding: 0.75rem 2rem;
+    margin-top: auto;
+    border-top: 1px solid var(--border);
+    display: flex;
+    justify-content: space-between;
+    align-items: center;
+    gap: 1rem;
+}
+
+.footer .credits {
+    flex: 1;
+    margin: 0;
+    text-align: left;
+}
+
+.footer .support {
+    display: flex;
+    align-items: center;
+}
+
+.footer .legal-links {
+    display: flex;
+    gap: 1rem;
+}
+
+.footer .legal-links a {
+    color: var(--text-muted);
+    font-size: 0.9rem;
+}
+
+/* Provider loading animation */
+.loading-animation {
+    opacity: 0;
+    transform: translateY(10px);
+    animation: fadeInUp 0.5s ease forwards;
+}
+
+.provider-links {
+    display: flex;
+    flex-wrap: wrap;
+    justify-content: flex-start;
+    gap: 0.5rem;
+    opacity: 0;
+    animation: fadeIn 0.5s ease forwards;
+}
+
+.provider-links a {
+    background: var(--button-bg);
+    border: 1px solid var(--border);
+    padding: 0.5rem 1rem;
+    border-radius: 6px;
+    transition: all 0.3s ease;
+    animation: slideIn 0.5s ease forwards;
+}
+
+.provider-links a:nth-child(1) { animation-delay: 0.1s; }
+.provider-links a:nth-child(2) { animation-delay: 0.2s; }
+.provider-links a:nth-child(3) { animation-delay: 0.3s; }
+.provider-links a:nth-child(4) { animation-delay: 0.4s; }
+.provider-links a:nth-child(5) { animation-delay: 0.5s; }
+
+.provider-links a:hover {
+    background: var(--button-hover);
+    border-color: var(--text);
+    transform: translateY(-2px);
+}
+
+/* Health status indicators */
+.health-status {
+    position: absolute;
+    top: 1rem;
+    right: 1rem;
+    width: 12px;
+    height: 12px;
+    border-radius: 50%;
+    cursor: help;
+    transition: transform 0.2s ease;
+}
+
+.health-status:hover {
+    transform: scale(1.2);
+}
+
+.health-status::after {
+    content: attr(data-tooltip);
+    position: absolute;
+    top: 150%;
+    right: -10px;
+    min-width: 150px;
+    background: var(--card-bg);
+    color: var(--text);
+    padding: 0.5rem;
+    border-radius: 6px;
+    font-size: 0.8rem;
+    border: 1px solid var(--border);
+    opacity: 0;
+    visibility: hidden;
+    transition: all 0.2s ease;
+    pointer-events: none;
+    z-index: 100;
+}
+
+.health-status:hover::after {
+    opacity: 1;
+    visibility: visible;
+}
+
+.status-green {
+    background: var(--status-green);
+    box-shadow: 0 0 10px var(--status-green);
+}
+
+.status-yellow {
+    background: var(--status-yellow);
+    box-shadow: 0 0 10px var(--status-yellow);
+}
+
+.status-red {
+    background: var(--status-red);
+    box-shadow: 0 0 10px var(--status-red);
+}
+
+/* Utils */
+.text-muted { color: var(--text-muted); }
+.text-bright { color: var(--text-bright); }
+.text-center { text-align: center; }
+
+.mt-1 { margin-top: 1rem; }
+.mt-2 { margin-top: 2rem; }
+.mb-1 { margin-bottom: 1rem; }
+.mb-2 { margin-bottom: 2rem; }
+
+/* Animations */
+@keyframes fadeIn {
+    from { opacity: 0; }
+    to { opacity: 1; }
+}
+
+@keyframes fadeInUp {
+    from {
+        opacity: 0;
+        transform: translateY(10px);
+    }
+    to {
+        opacity: 1;
+        transform: translateY(0);
+    }
+}
+
+@keyframes slideIn {
+    from {
+        opacity: 0;
+        transform: translateY(20px);
+    }
+    to {
+        opacity: 1;
+        transform: translateY(0);
+    }
+}
+
+/* Responsive */
+@media (max-width: 768px) {
+    .container {
+        padding: 1rem;
+    }
+
+    .site-header {
+        font-size: 2.5rem;
+    }
+
+    .footer {
+        padding: 1rem;
+        flex-direction: column;
+        text-align: center;
+        gap: 0.75rem;
+    }
+
+    .footer .credits {
+        text-align: center;
+    }
+
+    .footer .legal-links {
+        justify-content: center;
+    }
+
+    .provider-links {
+        justify-content: center;
+    }
+}
\ No newline at end of file
diff --git a/public/tos.html b/public/tos.html
new file mode 100644
index 0000000..9788109
--- /dev/null
+++ b/public/tos.html
@@ -0,0 +1,105 @@
+    <!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>Terms of Service - Sylph</title>
+    <link rel="stylesheet" href="/styles.css">
+    <style>
+        .container {
+            max-width: 800px;
+            margin: 0 auto;
+            padding: 2rem;
+        }
+
+        .legal-content {
+            margin-top: 2rem;
+        }
+
+        .legal-content h1 {
+            color: var(--text-bright);
+            margin-bottom: 1.5rem;
+            font-size: 2rem;
+        }
+
+        .legal-content h2 {
+            color: var(--text-bright);
+            margin-top: 2rem;
+            margin-bottom: 1rem;
+            font-size: 1.5rem;
+        }
+
+        .legal-content p {
+            margin-bottom: 1rem;
+            line-height: 1.6;
+        }
+
+        .legal-content ul {
+            margin: 1rem 0 1.5rem;
+            padding-left: 1.5rem;
+        }
+
+        .legal-content li {
+            margin-bottom: 0.5rem;
+            line-height: 1.5;
+        }
+
+        .legal-content strong {
+            color: var(--text-bright);
+        }
+
+        @media (max-width: 768px) {
+            .container {
+                padding: 1rem;
+            }
+            
+            .legal-content h1 {
+                font-size: 1.75rem;
+            }
+
+            .legal-content h2 {
+                font-size: 1.25rem;
+            }
+        }
+    </style>
+</head>
+<body>
+    <div class="legal-content">
+        <h1>Terms of Service</h1>
+        <p>Last updated: April 3, 2025</p>
+
+        <p>Welcome to Sylph! By using our API service, you agree to these terms. Please read them carefully.</p>
+
+        <h2>1. Use of Service</h2>
+        <p>You agree to use the Sylph API only for lawful purposes and in accordance with these terms. You are responsible for any content generated or actions taken through your use of the API.</p>
+
+        <h2>2. API Keys</h2>
+        <p>If authentication is enabled, you are responsible for maintaining the confidentiality of your API key. Do not share your key with unauthorized parties.</p>
+
+        <h2>3. Prohibited Uses</h2>
+        <p>You may not use the service:</p>
+        <ul>
+            <li>In any way that violates any applicable federal, state, local, or international law or regulation.</li>
+            <li>To exploit, harm, or attempt to exploit or harm minors in any way.</li>
+            <li>To generate content that is unlawful, harmful, threatening, abusive, harassing, defamatory, vulgar, obscene, or otherwise objectionable.</li>
+            <li>To interfere with or disrupt the integrity or performance of the service or third-party data contained therein.</li>
+            <li>To attempt to gain unauthorized access to the service or its related systems or networks.</li>
+            <li>Through automated coding assistants or similar AI tools (e.g., GitHub Copilot, Cline, Cursor, etc.).</li>
+            <li>In any way that could be considered "vibe coding" or automated code generation.</li>
+            <li>To perform high-frequency or automated requests that could degrade service quality for other users.</li>
+        </ul>
+
+        <h2>4. Disclaimer of Warranties</h2>
+        <p>The service is provided on an "AS IS" and "AS AVAILABLE" basis. We disclaim all warranties of any kind, whether express or implied, including, but not limited to, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement.</p>
+
+        <h2>5. Limitation of Liability</h2>
+        <p>In no event shall Sylph, nor its directors, employees, partners, agents, suppliers, or affiliates, be liable for any indirect, incidental, special, consequential or punitive damages, including without limitation, loss of profits, data, use, goodwill, or other intangible losses, resulting from your access to or use of or inability to access or use the service.</p>
+
+        <h2>6. Changes to Terms</h2>
+        <p>We reserve the right, at our sole discretion, to modify or replace these Terms at any time. We will provide notice of any changes by posting the new Terms on this page.</p>
+
+        <h2>7. Contact Us</h2>
+        <p>If you have any questions about these Terms, please contact us via the GitHub repository.</p>
+    </div>
+</body>
+</html>
\ No newline at end of file
diff --git a/server.js b/server.js
deleted file mode 100644
index ecccc80..0000000
--- a/server.js
+++ /dev/null
@@ -1,940 +0,0 @@
-const express = require('express');
-const cors = require('cors');
-const fetch = require('node-fetch');
-const swaggerUi = require('swagger-ui-express');
-const docs = require('./docs.json');
-const fs = require('fs');
-const path = require('path');
-
-const app = express();
-const port = process.env.PORT || 3000;
-
-// Debug mode with getter/setter for toggling
-let _debugMode = false; // Default to false for production
-const DEBUG_MODE = {
-    get enabled() {
-        return _debugMode;
-    },
-    set enabled(value) {
-        const oldValue = _debugMode;
-        _debugMode = !!value; // Convert to boolean
-        if (oldValue !== _debugMode) {
-            console.log(`[${new Date().toISOString()}] Debug mode ${_debugMode ? 'ENABLED' : 'DISABLED'}`);
-        }
-    },
-    toggle() {
-        this.enabled = !this.enabled;
-        return this.enabled;
-    }
-};
-
-// Hardcoded model name (no longer dynamically fetched)
-const currentModel = 'llama-3.3-70b-versatile';
-
-// Check if we're running on Vercel
-const isVercelEnv = process.env.VERCEL === '1';
-
-// Function to log messages to file and console
-const logMessage = (message, isDebug = false) => {
-    if (isDebug && !DEBUG_MODE.enabled) {
-        return; // Skip debug messages if debug mode is disabled
-    }
-    
-    const timestamp = new Date().toISOString();
-    const prefix = isDebug ? '[DEBUG] ' : '';
-    const logEntry = `[${timestamp}] ${prefix}${message}\n`;
-    
-    // Log to console
-    console.log(logEntry.trim());
-    
-    // Only write to file if not in Vercel environment
-    if (!isVercelEnv) {
-        try {
-            // Create logs directory if it doesn't exist
-            const logsDir = path.join(__dirname, 'logs');
-            if (!fs.existsSync(logsDir)) {
-                fs.mkdirSync(logsDir);
-            }
-            
-            // Log to file
-            const logFile = path.join(logsDir, `${new Date().toISOString().split('T')[0]}.log`);
-            fs.appendFileSync(logFile, logEntry);
-        } catch (err) {
-            console.error(`Failed to write to log file: ${err.message}`);
-        }
-    }
-};
-
-// Function to safely stringify objects for logging
-const safeStringify = (obj) => {
-    try {
-        return JSON.stringify(obj, null, 2);
-    } catch (error) {
-        return `[Cannot stringify: ${error.message}]`;
-    }
-};
-
-// Helper function to log response details for debugging
-const logResponseDetails = async (response, requestId) => {
-    const contentType = response.headers.get('content-type') || 'unknown';
-    try {
-        // Try to get some text for debugging
-        const text = await response.text();
-        const preview = text.substring(0, 200) + (text.length > 200 ? '...' : '');
-        logMessage(`[${requestId}] Response details: Status=${response.status}, ContentType=${contentType}, Body preview: ${preview}`);
-        return text;
-    } catch (error) {
-        logMessage(`[${requestId}] Failed to read response body: ${error.message}`);
-        return null;
-    }
-};
-
-// Generate an OpenAI-compatible ID
-const generateOpenAiId = () => {
-    // Format: "chatcmpl-" + random alphanumeric string
-    return 'chatcmpl-' + Math.random().toString(36).substring(2, 9);
-};
-
-// Helper function to sanitize message content (ensures content is always a string)
-const sanitizeMessages = (messages) => {
-    if (!Array.isArray(messages)) {
-        return [];
-    }
-    
-    return messages.map(msg => {
-        if (typeof msg !== 'object' || msg === null) {
-            return { role: 'user', content: String(msg || '') };
-        }
-        
-        // Ensure role is a string
-        const role = typeof msg.role === 'string' ? msg.role : 'user';
-        
-        // Handle content based on type
-        let content = '';
-        if (typeof msg.content === 'string') {
-            content = msg.content;
-        } else if (Array.isArray(msg.content)) {
-            // For array content (like with images), convert to string
-            content = msg.content.map(item => {
-                if (typeof item === 'string') {
-                    return item;
-                } else if (item && typeof item === 'object' && item.type === 'text' && typeof item.text === 'string') {
-                    return item.text;
-                }
-                return '';
-            }).join('\n').trim();
-        } else if (msg.content && typeof msg.content === 'object') {
-            // For object content, try to stringify or extract text
-            if (msg.content.type === 'text' && typeof msg.content.text === 'string') {
-                content = msg.content.text;
-            } else {
-                try {
-                    content = JSON.stringify(msg.content);
-                } catch (e) {
-                    content = '';
-                }
-            }
-        }
-        
-        return { role, content };
-    });
-};
-
-// Function to get valid content from a response or use a fallback
-const getValidMessageContent = (hackclubResponse, requestId) => {
-    try {
-        // Check if we have a valid response with a message
-        if (hackclubResponse && 
-            typeof hackclubResponse === 'object' &&
-            hackclubResponse.choices && 
-            Array.isArray(hackclubResponse.choices) &&
-            hackclubResponse.choices.length > 0 && 
-            hackclubResponse.choices[0].message &&
-            typeof hackclubResponse.choices[0].message === 'object' &&
-            typeof hackclubResponse.choices[0].message.content === 'string') {
-            
-            // Valid message found
-            const content = hackclubResponse.choices[0].message.content;
-            if (content.trim() !== '') {
-                logMessage(`[${requestId}] Found valid message content: "${content.substring(0, 50)}${content.length > 50 ? '...' : ''}"`, true);
-                return {
-                    role: hackclubResponse.choices[0].message.role || 'assistant',
-                    content: content
-                };
-            }
-        }
-        
-        // If we get here, the response was invalid or incomplete
-        logMessage(`[${requestId}] Invalid message structure or empty content, using fallback message`, true);
-        logMessage(`[${requestId}] Response was: ${safeStringify(hackclubResponse)}`, true);
-        
-        // Return a fallback message
-        return {
-            role: 'assistant',
-            content: "I apologize, but I couldn't generate a proper response at this time. Please try again."
-        };
-    } catch (error) {
-        logMessage(`[${requestId}] Error extracting message: ${error.message}`, true);
-        return {
-            role: 'assistant',
-            content: "An error occurred while processing your request. Please try again later."
-        };
-    }
-};
-
-// Function to transform HackClub response to OpenAI response format - SIMPLIFIED to exactly match OpenAI format
-const transformToOpenAiFormat = (hackclubResponse, model, requestId) => {
-    // Log the input for debugging
-    if (DEBUG_MODE.enabled) {
-        logMessage(`[${requestId}] HackClub Response to transform: ${safeStringify(hackclubResponse)}`, true);
-    }
-    
-    // Get current timestamp
-    const timestamp = Math.floor(Date.now() / 1000);
-    
-    // Get a valid message or fallback
-    const messageObj = getValidMessageContent(hackclubResponse, requestId);
-    
-    // Create a guaranteed-valid OpenAI format response
-    const openAiResponse = {
-        id: generateOpenAiId(),
-        object: "chat.completion",
-        created: timestamp,
-        model: model || "gpt-3.5-turbo", 
-        choices: [
-            {
-                index: 0,
-                message: messageObj,
-                finish_reason: "stop",
-                logprobs: null
-            }
-        ],
-        usage: {
-            prompt_tokens: 10,
-            completion_tokens: Math.ceil(messageObj.content.length / 4),
-            total_tokens: 10 + Math.ceil(messageObj.content.length / 4)
-        }
-    };
-
-    // Try to use the HackClub fields if they exist
-    if (hackclubResponse && typeof hackclubResponse === 'object') {
-        if (hackclubResponse.id) {
-            openAiResponse.id = hackclubResponse.id;
-        }
-        
-        if (hackclubResponse.created) {
-            openAiResponse.created = hackclubResponse.created;
-        }
-        
-        if (hackclubResponse.choices && 
-            hackclubResponse.choices.length > 0) {
-            
-            const choiceData = hackclubResponse.choices[0];
-            
-            if (choiceData.finish_reason) {
-                openAiResponse.choices[0].finish_reason = choiceData.finish_reason;
-            }
-            
-            if (choiceData.index !== undefined) {
-                openAiResponse.choices[0].index = choiceData.index;
-            }
-        }
-        
-        if (hackclubResponse.usage) {
-            if (hackclubResponse.usage.prompt_tokens !== undefined) {
-                openAiResponse.usage.prompt_tokens = hackclubResponse.usage.prompt_tokens;
-            }
-            
-            if (hackclubResponse.usage.completion_tokens !== undefined) {
-                openAiResponse.usage.completion_tokens = hackclubResponse.usage.completion_tokens;
-            }
-            
-            if (hackclubResponse.usage.total_tokens !== undefined) {
-                openAiResponse.usage.total_tokens = hackclubResponse.usage.total_tokens;
-            } else if (hackclubResponse.usage.prompt_tokens !== undefined && 
-                       hackclubResponse.usage.completion_tokens !== undefined) {
-                openAiResponse.usage.total_tokens = hackclubResponse.usage.prompt_tokens + hackclubResponse.usage.completion_tokens;
-            }
-        }
-    }
-    
-    // Log the final response for debugging
-    if (DEBUG_MODE.enabled) {
-        logMessage(`[${requestId}] Final OpenAI response (guaranteed valid): ${safeStringify(openAiResponse)}`, true);
-    }
-    
-    return openAiResponse;
-};
-
-// No need to fetch the model since we're using a hardcoded value
-
-app.use(cors());
-app.use(express.json());
-
-// Request logging middleware
-app.use((req, res, next) => {
-    // Generate a unique request ID
-    const requestId = Math.random().toString(36).substring(2, 15);
-    req.requestId = requestId;
-    
-    // Record request start time
-    req.requestStartTime = Date.now();
-    
-    // Get client IP
-    const ip = req.headers['x-forwarded-for'] || req.socket.remoteAddress;
-    
-    // Log the request
-    logMessage(`[${requestId}] Request: ${req.method} ${req.url} from IP ${ip}`);
-    if (req.method === 'POST' && (req.url === '/chat/completions' || req.url === '/v1/chat/completions')) {
-        // For chat completions, log message count but not content for privacy
-        const messageCount = req.body.messages ? req.body.messages.length : 0;
-        logMessage(`[${requestId}] Chat request with ${messageCount} messages`);
-        
-        // In debug mode, log the request structure
-        if (DEBUG_MODE.enabled) {
-            // Redact sensitive content but keep the structure
-            const debugBody = JSON.parse(JSON.stringify(req.body));
-            if (debugBody.messages) {
-                debugBody.messages = debugBody.messages.map(msg => {
-                    return {
-                        role: msg.role,
-                        content: msg.content ? "[REDACTED FOR PRIVACY]" : undefined,
-                        // Keep structure but redact content
-                        ...(typeof msg.content === 'object' ? { content_type: 'complex-structure' } : {})
-                    };
-                });
-            }
-            logMessage(`[${requestId}] Request body structure: ${safeStringify(debugBody)}`, true);
-        }
-    }
-    
-    // Capture the original send function
-    const originalSend = res.send;
-    
-    // Override the send function to log the response
-    res.send = function(body) {
-        // Calculate response time
-        const responseTime = Date.now() - req.requestStartTime;
-        
-        // Log response info
-        logMessage(`[${requestId}] Response: ${res.statusCode} (${responseTime}ms)`);
-        
-        // In debug mode, log response body structure
-        if (DEBUG_MODE.enabled && body) {
-            try {
-                const responseObj = typeof body === 'string' ? JSON.parse(body) : body;
-                // Redact actual content but keep structure in the logs
-                if (responseObj.choices && responseObj.choices.length > 0 && responseObj.choices[0].message) {
-                    const contentLength = responseObj.choices[0].message.content ? responseObj.choices[0].message.content.length : 0;
-                    responseObj.choices[0].message.content = `[CONTENT REDACTED - ${contentLength} chars]`;
-                }
-                logMessage(`[${requestId}] Response body structure: ${safeStringify(responseObj)}`, true);
-            } catch (error) {
-                logMessage(`[${requestId}] Could not parse response body: ${error.message}`, true);
-            }
-        }
-        
-        // Call the original send function
-        return originalSend.call(this, body);
-    };
-    
-    next();
-});
-
-// Serve documentation
-// First try public directory (for Vercel), fall back to node_modules (for local dev)
-app.use('/swagger-ui', express.static(path.join(__dirname, 'public/swagger-ui')));
-app.use('/swagger-ui', express.static(path.join(__dirname, 'node_modules/swagger-ui-express/static')));
-app.use('/', swaggerUi.serve);
-app.get('/', swaggerUi.setup(docs, {
-  customCssUrl: '/swagger-ui/swagger-ui.css',
-  customJs: '/swagger-ui/swagger-ui-bundle.js',
-  customfavIcon: '/swagger-ui/favicon-32x32.png'
-}));
-
-
-// OpenAI compatible models endpoint
-app.get('/models', (req, res) => {
-    // Return a response in OpenAI format
-    res.json({
-        object: "list",
-        data: [
-            {
-                id: currentModel,
-                object: "model",
-                created: Math.floor(Date.now() / 1000),
-                owned_by: "hackclub",
-                permission: [],
-                root: currentModel,
-                parent: null
-            }
-        ]
-    });
-});
-
-// Add the v1/models endpoint for OpenAI compatibility 
-app.get('/v1/models', (req, res) => {
-    // Return a response in OpenAI format
-    res.json({
-        object: "list",
-        data: [
-            {
-                id: currentModel,
-                object: "model",
-                created: Math.floor(Date.now() / 1000),
-                owned_by: "hackclub",
-                permission: [],
-                root: currentModel,
-                parent: null
-            }
-        ]
-    });
-});
-
-// Function to stream response in OpenAI-compatible SSE format
-const streamResponse = (content, req, res) => {
-    // Set headers for SSE (Server-Sent Events)
-    res.writeHead(200, {
-        'Content-Type': 'text/event-stream',
-        'Cache-Control': 'no-cache',
-        'Connection': 'keep-alive',
-        'Access-Control-Allow-Origin': '*'
-    });
-
-    // Create a unique ID for this streaming session
-    const responseId = generateOpenAiId();
-    const timestamp = Math.floor(Date.now() / 1000);
-    
-    // Break the content into small chunks (3-8 words per chunk)
-    const words = content.split(' ');
-    const chunks = [];
-    let currentChunk = [];
-    
-    for (const word of words) {
-        currentChunk.push(word);
-        // Randomly decide if we should end this chunk (between 3-8 words)
-        if (currentChunk.length >= 3 && (currentChunk.length >= 8 || Math.random() < 0.3)) {
-            chunks.push(currentChunk.join(' '));
-            currentChunk = [];
-        }
-    }
-    
-    // Add any remaining words as the final chunk
-    if (currentChunk.length > 0) {
-        chunks.push(currentChunk.join(' '));
-    }
-    
-    // Stream each chunk with a small delay
-    let index = 0;
-    const streamChunk = () => {
-        if (index < chunks.length) {
-            // Format the chunk as an OpenAI-compatible SSE event
-            const chunk = chunks[index];
-            const chunkObj = {
-                id: `${responseId}-${index}`,
-                object: "chat.completion.chunk",
-                created: timestamp,
-                model: currentModel,
-                choices: [{
-                    index: 0,
-                    delta: {
-                        content: index === 0 ? chunk : ' ' + chunk
-                    },
-                    finish_reason: null
-                }]
-            };
-            
-            // Send the chunk as an SSE event
-            res.write(`data: ${JSON.stringify(chunkObj)}\n\n`);
-            
-            // Schedule the next chunk
-            index++;
-            setTimeout(streamChunk, 10); // Super fast streaming with 10ms delay
-        } else {
-            // Send the final chunk with finish_reason
-            const finalChunkObj = {
-                id: `${responseId}-${index}`,
-                object: "chat.completion.chunk",
-                created: timestamp,
-                model: currentModel,
-                choices: [{
-                    index: 0,
-                    delta: {},
-                    finish_reason: "stop"
-                }]
-            };
-            
-            res.write(`data: ${JSON.stringify(finalChunkObj)}\n\n`);
-            
-            // End the stream with [DONE]
-            res.write('data: [DONE]\n\n');
-            res.end();
-        }
-    };
-    
-    // Start streaming
-    streamChunk();
-};
-
-// Main proxy endpoint
-app.post('/chat/completions', async (req, res) => {
-    try {
-        // Check if streaming is requested
-        const isStreaming = req.body.stream === true;
-        
-        logMessage(`[${req.requestId}] Request ${isStreaming ? 'with streaming' : 'without streaming'}`);
-        
-        // Handle the special case where the client is expecting a response but no actual messages are passed
-        if (!req.body.messages || req.body.messages.length === 0) {
-            if (isStreaming) {
-                // Stream a stub message
-                return streamResponse("Hello! How can I assist you today?", req, res);
-            } else {
-                const stubResponse = {
-                    id: generateOpenAiId(),
-                    object: "chat.completion",
-                    created: Math.floor(Date.now() / 1000),
-                    model: currentModel,
-                    choices: [
-                        {
-                            index: 0,
-                            message: {
-                                role: "assistant",
-                                content: "Hello! How can I assist you today?"
-                            },
-                            finish_reason: "stop",
-                            logprobs: null
-                        }
-                    ],
-                    usage: {
-                        prompt_tokens: 0,
-                        completion_tokens: 8,
-                        total_tokens: 8
-                    }
-                };
-                logMessage(`[${req.requestId}] No messages found in request, responding with stub message`, true);
-                return res.json(stubResponse);
-            }
-        }
-        
-        // Sanitize messages to ensure content is always a string
-        const sanitizedMessages = sanitizeMessages(req.body.messages || []);
-        
-        if (DEBUG_MODE.enabled) {
-            logMessage(`[${req.requestId}] Sanitized messages structure: ${safeStringify(
-                sanitizedMessages.map(m => ({ role: m.role, content_length: m.content ? m.content.length : 0 }))
-            )}`, true);
-        }
-        
-        // Forward the request to HackClub API with sanitized messages
-        const requestBody = {
-            messages: sanitizedMessages
-        };
-
-        // handle additional parameters that HackClub API might support
-        if (req.body.temperature !== undefined) {
-            requestBody.temperature = req.body.temperature;
-        }
-        
-        if (req.body.max_tokens !== undefined) {
-            requestBody.max_tokens = req.body.max_tokens;
-        }
-
-        // Log the outgoing request body structure (not content for privacy)
-        logMessage(`[${req.requestId}] Forwarding request to HackClub API with ${sanitizedMessages.length} sanitized messages`);
-
-        // forward the request to HackClub API
-        const response = await fetch('https://ai.hackclub.com/chat/completions', {
-            method: 'POST',
-            headers: {
-                'Content-Type': 'application/json',
-            },
-            body: JSON.stringify(requestBody),
-        });
-        
-        // Check if the response is OK
-        if (!response.ok) {
-            const errorText = await logResponseDetails(response, req.requestId);
-            logMessage(`[${req.requestId}] Error from HackClub API: ${response.status} ${errorText || 'No response body'}`, true);
-            
-            // Still return a valid response to the client, but with an error message
-            const fallbackResponse = {
-                id: generateOpenAiId(),
-                object: "chat.completion",
-                created: Math.floor(Date.now() / 1000),
-                model: currentModel,
-                choices: [
-                    {
-                        index: 0,
-                        message: {
-                            role: "assistant",
-                            content: `I'm sorry, there was an error processing your request. The HackClub API returned status code ${response.status}.`
-                        },
-                        finish_reason: "stop",
-                        logprobs: null
-                    }
-                ],
-                usage: {
-                    prompt_tokens: 0,
-                    completion_tokens: 20,
-                    total_tokens: 20
-                }
-            };
-            return res.json(fallbackResponse);
-        }
-
-        // Clone the response for text extraction and JSON parsing
-        const responseClone = response.clone();
-        
-        // Try to parse as JSON
-        let data;
-        try {
-            const responseText = await response.text();
-            if (DEBUG_MODE.enabled) {
-                logMessage(`[${req.requestId}] Raw HackClub API response: ${responseText}`, true);
-            }
-            data = JSON.parse(responseText);
-        } catch (err) {
-            // If JSON parsing fails, log the raw response for debugging
-            const responseText = await logResponseDetails(responseClone, req.requestId);
-            logMessage(`[${req.requestId}] JSON parsing error: ${err.message}. Response was: ${responseText || 'Unable to read response'}`, true);
-            
-            // Still return a valid response to the client, but with an error message
-            const fallbackResponse = {
-                id: generateOpenAiId(),
-                object: "chat.completion",
-                created: Math.floor(Date.now() / 1000),
-                model: currentModel,
-                choices: [
-                    {
-                        index: 0,
-                        message: {
-                            role: "assistant",
-                            content: "I'm sorry, there was an error processing your request. The response from HackClub could not be parsed as valid JSON."
-                        },
-                        finish_reason: "stop",
-                        logprobs: null
-                    }
-                ],
-                usage: {
-                    prompt_tokens: 0,
-                    completion_tokens: 25,
-                    total_tokens: 25
-                }
-            };
-            return res.json(fallbackResponse);
-        }
-
-        // Check if streaming was requested
-        if (isStreaming) {
-            // For streaming, extract the content and stream it
-            const messageContent = getValidMessageContent(data, req.requestId).content;
-            return streamResponse(messageContent, req, res);
-        } else {
-            // Transform to OpenAI format (guaranteed valid)
-            const openAiResponse = transformToOpenAiFormat(data, currentModel, req.requestId);
-            res.json(openAiResponse);
-        }
-    } catch (error) {
-        // Get client IP
-        const ip = req.headers['x-forwarded-for'] || req.socket.remoteAddress;
-        
-        // Log the error with request details
-        logMessage(`[${req.requestId}] ERROR: ${error.message} from IP ${ip}`);
-        logMessage(`[${req.requestId}] Error stack: ${error.stack}`);
-        
-        // Always return a valid response structure even if there's an error
-        res.json({
-            id: generateOpenAiId(),
-            object: "chat.completion",
-            created: Math.floor(Date.now() / 1000),
-            model: currentModel,
-            choices: [
-                {
-                    index: 0,
-                    message: {
-                        role: "assistant",
-                        content: "I apologize, but an error occurred while processing your request. Please try again later."
-                    },
-                    finish_reason: "stop",
-                    logprobs: null
-                }
-            ],
-            usage: {
-                prompt_tokens: 0,
-                completion_tokens: 17,
-                total_tokens: 17
-            }
-        });
-    }
-});
-
-// Compatibility with v1/chat/completions endpoint (for other clients)
-app.post('/v1/chat/completions', async (req, res) => {
-    try {
-        // Check if streaming is requested
-        const isStreaming = req.body.stream === true;
-        
-        logMessage(`[${req.requestId}] V1 Request ${isStreaming ? 'with streaming' : 'without streaming'}`);
-        
-        // Handle the special case where the client is expecting a response but no actual messages are passed
-        if (!req.body.messages || req.body.messages.length === 0) {
-            if (isStreaming) {
-                // Stream a stub message
-                return streamResponse("Hello! How can I assist you today?", req, res);
-            } else {
-                const stubResponse = {
-                    id: generateOpenAiId(),
-                    object: "chat.completion",
-                    created: Math.floor(Date.now() / 1000),
-                    model: currentModel,
-                    choices: [
-                        {
-                            index: 0,
-                            message: {
-                                role: "assistant",
-                                content: "Hello! How can I assist you today?"
-                            },
-                            finish_reason: "stop",
-                            logprobs: null
-                        }
-                    ],
-                    usage: {
-                        prompt_tokens: 0,
-                        completion_tokens: 8,
-                        total_tokens: 8
-                    }
-                };
-                logMessage(`[${req.requestId}] No messages found in request, responding with stub message`, true);
-                return res.json(stubResponse);
-            }
-        }
-        
-        // Sanitize messages to ensure content is always a string
-        const sanitizedMessages = sanitizeMessages(req.body.messages || []);
-        
-        if (DEBUG_MODE.enabled) {
-            logMessage(`[${req.requestId}] Sanitized messages structure: ${safeStringify(
-                sanitizedMessages.map(m => ({ role: m.role, content_length: m.content ? m.content.length : 0 }))
-            )}`, true);
-        }
-        
-        // Forward the request to HackClub API with sanitized messages
-        const requestBody = {
-            messages: sanitizedMessages
-        };
-
-        // handle additional parameters that HackClub API might support
-        if (req.body.temperature !== undefined) {
-            requestBody.temperature = req.body.temperature;
-        }
-        
-        if (req.body.max_tokens !== undefined) {
-            requestBody.max_tokens = req.body.max_tokens;
-        }
-
-        // Log the outgoing request body structure (not content for privacy)
-        logMessage(`[${req.requestId}] Forwarding request to HackClub API via v1 endpoint with ${sanitizedMessages.length} sanitized messages`);
-
-        // forward the request to HackClub API
-        const response = await fetch('https://ai.hackclub.com/chat/completions', {
-            method: 'POST',
-            headers: {
-                'Content-Type': 'application/json',
-            },
-            body: JSON.stringify(requestBody),
-        });
-
-        // Check if the response is OK
-        if (!response.ok) {
-            const errorText = await logResponseDetails(response, req.requestId);
-            logMessage(`[${req.requestId}] Error from HackClub API: ${response.status} ${errorText || 'No response body'}`, true);
-            
-            // Still return a valid response to the client, but with an error message
-            const fallbackResponse = {
-                id: generateOpenAiId(),
-                object: "chat.completion",
-                created: Math.floor(Date.now() / 1000),
-                model: currentModel,
-                choices: [
-                    {
-                        index: 0,
-                        message: {
-                            role: "assistant",
-                            content: `I'm sorry, there was an error processing your request. The HackClub API returned status code ${response.status}.`
-                        },
-                        finish_reason: "stop",
-                        logprobs: null
-                    }
-                ],
-                usage: {
-                    prompt_tokens: 0,
-                    completion_tokens: 20,
-                    total_tokens: 20
-                }
-            };
-            return res.json(fallbackResponse);
-        }
-
-        // Clone the response for text extraction and JSON parsing
-        const responseClone = response.clone();
-        
-        // Try to parse as JSON
-        let data;
-        try {
-            const responseText = await response.text();
-            if (DEBUG_MODE.enabled) {
-                logMessage(`[${req.requestId}] Raw HackClub API response: ${responseText}`, true);
-            }
-            data = JSON.parse(responseText);
-        } catch (err) {
-            // If JSON parsing fails, log the raw response for debugging
-            const responseText = await logResponseDetails(responseClone, req.requestId);
-            logMessage(`[${req.requestId}] JSON parsing error: ${err.message}. Response was: ${responseText || 'Unable to read response'}`, true);
-            
-            // Still return a valid response to the client, but with an error message
-            const fallbackResponse = {
-                id: generateOpenAiId(),
-                object: "chat.completion",
-                created: Math.floor(Date.now() / 1000),
-                model: currentModel,
-                choices: [
-                    {
-                        index: 0,
-                        message: {
-                            role: "assistant",
-                            content: "I'm sorry, there was an error processing your request. The response from HackClub could not be parsed as valid JSON."
-                        },
-                        finish_reason: "stop",
-                        logprobs: null
-                    }
-                ],
-                usage: {
-                    prompt_tokens: 0,
-                    completion_tokens: 25,
-                    total_tokens: 25
-                }
-            };
-            return res.json(fallbackResponse);
-        }
-
-        // Check if streaming was requested
-        if (isStreaming) {
-            // For streaming, extract the content and stream it
-            const messageContent = getValidMessageContent(data, req.requestId).content;
-            return streamResponse(messageContent, req, res);
-        } else {
-            // Transform to OpenAI format (guaranteed valid)
-            const openAiResponse = transformToOpenAiFormat(data, currentModel, req.requestId);
-            res.json(openAiResponse);
-        }
-    } catch (error) {
-        // Get client IP
-        const ip = req.headers['x-forwarded-for'] || req.socket.remoteAddress;
-        
-        // Log the error with request details
-        logMessage(`[${req.requestId}] ERROR in v1 endpoint: ${error.message} from IP ${ip}`);
-        logMessage(`[${req.requestId}] Error stack: ${error.stack}`);
-        
-        // Always return a valid response structure even if there's an error
-        res.json({
-            id: generateOpenAiId(),
-            object: "chat.completion",
-            created: Math.floor(Date.now() / 1000),
-            model: currentModel,
-            choices: [
-                {
-                    index: 0,
-                    message: {
-                        role: "assistant",
-                        content: "I apologize, but an error occurred while processing your request. Please try again later."
-                    },
-                    finish_reason: "stop",
-                    logprobs: null
-                }
-            ],
-            usage: {
-                prompt_tokens: 0,
-                completion_tokens: 17,
-                total_tokens: 17
-            }
-        });
-    }
-});
-
-// Add a test endpoint that returns a guaranteed valid OpenAI format response
-app.get('/test-response', (req, res) => {
-    const testResponse = {
-        id: generateOpenAiId(),
-        object: "chat.completion",
-        created: Math.floor(Date.now() / 1000),
-        model: currentModel,
-        choices: [
-            {
-                index: 0,
-                message: {
-                    role: "assistant",
-                    content: "This is a test response in standard OpenAI format. Your client should be able to parse and display this message properly."
-                },
-                finish_reason: "stop",
-                logprobs: null
-            }
-        ],
-        usage: {
-            prompt_tokens: 0,
-            completion_tokens: 25,
-            total_tokens: 25
-        }
-    };
-    
-    res.json(testResponse);
-});
-
-// Error handler middleware
-app.use((err, req, res, next) => {
-    // Get client IP
-    const ip = req.headers['x-forwarded-for'] || req.socket.remoteAddress;
-    
-    // Log the error
-    logMessage(`[${req.requestId || 'UNKNOWN'}] Unhandled error: ${err.message} from IP ${ip}`);
-    logMessage(`[${req.requestId || 'UNKNOWN'}] Error stack: ${err.stack}`);
-    
-    // Send error response
-    res.status(500).json({
-        error: {
-            message: 'Server error: ' + err.message,
-            type: 'internal_server_error'
-        }
-    });
-});
-
-// Handle invalid routes
-app.use((req, res) => {
-    const ip = req.headers['x-forwarded-for'] || req.socket.remoteAddress;
-    logMessage(`[${req.requestId}] 404 Not Found: ${req.method} ${req.url} from IP ${ip}`);
-    
-    res.status(404).json({
-        error: {
-            message: 'Not found',
-            type: 'invalid_request_error'
-        }
-    });
-});
-
-// Start the server
-app.listen(port, () => {
-    logMessage(`Server running on port ${port}`);
-    logMessage(`Current model: ${currentModel}`);
-    logMessage(`Debug mode: ${DEBUG_MODE.enabled ? 'ENABLED' : 'DISABLED'}`);
-});
-
-// Handle uncaught exceptions
-process.on('uncaughtException', (err) => {
-    logMessage(`UNCAUGHT EXCEPTION: ${err.message}`);
-    logMessage(`Stack: ${err.stack}`);
-});
-
-// Handle unhandled promise rejections
-process.on('unhandledRejection', (reason, promise) => {
-    logMessage(`UNHANDLED REJECTION: ${reason}`);
-});
diff --git a/src/middleware/maintenance.js b/src/middleware/maintenance.js
new file mode 100644
index 0000000..22370ef
--- /dev/null
+++ b/src/middleware/maintenance.js
@@ -0,0 +1,16 @@
+// Maintenance mode middleware
+const maintenanceMiddleware = (req, res, next) => {
+    if (process.env.MAINTENANCE_MODE === 'true') {
+        const message = process.env.MAINTENANCE_MESSAGE || 'Service is temporarily under maintenance.';
+        return res.status(503).json({
+            error: {
+                message,
+                type: 'maintenance_mode',
+                code: 'service_unavailable'
+            }
+        });
+    }
+    next();
+};
+
+export default maintenanceMiddleware;
\ No newline at end of file
diff --git a/src/middleware/protect.js b/src/middleware/protect.js
new file mode 100644
index 0000000..87bbc42
--- /dev/null
+++ b/src/middleware/protect.js
@@ -0,0 +1,63 @@
+// middleware/protect.js - Authentication middleware
+import fs from 'fs';
+import path from 'path';
+import { fileURLToPath } from 'url';
+const __dirname = path.dirname(fileURLToPath(import.meta.url));
+
+function loadApiKeys() {
+    if (!process.env.REQUIRE_API_KEY || process.env.REQUIRE_API_KEY === 'false') {
+        return new Set();
+    }
+
+    try {
+        const keysPath = process.env.API_KEYS_FILE || path.join(__dirname, '../../keys.txt');
+        const content = fs.readFileSync(keysPath, 'utf8');
+        return new Set(content.split('\n').filter(key => key.trim()));
+    } catch (error) {
+        console.error('Failed to load API keys:', error);
+        return new Set();
+    }
+}
+
+const apiKeys = loadApiKeys();
+
+export default function protectRoute(req, res, next) {
+    if (!process.env.REQUIRE_API_KEY || process.env.REQUIRE_API_KEY === 'false') {
+        return next();
+    }
+
+    const authHeader = req.headers.authorization;
+    if (!authHeader) {
+        return res.status(401).json({
+            error: {
+                message: 'Missing API key',
+                type: 'auth_error',
+                code: 'missing_api_key'
+            }
+        });
+    }
+
+    const parts = authHeader.split(' ');
+    if (parts.length !== 2 || parts[0].toLowerCase() !== 'bearer') {
+        return res.status(401).json({
+            error: {
+                message: 'Invalid authentication format. Use: Bearer YOUR_API_KEY',
+                type: 'auth_error',
+                code: 'invalid_auth_format'
+            }
+        });
+    }
+
+    const apiKey = parts[1];
+    if (!apiKeys.has(apiKey)) {
+        return res.status(401).json({
+            error: {
+                message: 'Invalid API key',
+                type: 'auth_error',
+                code: 'invalid_api_key'
+            }
+        });
+    }
+
+    next();
+}
\ No newline at end of file
diff --git a/src/providers/base.js b/src/providers/base.js
new file mode 100644
index 0000000..ec5d43a
--- /dev/null
+++ b/src/providers/base.js
@@ -0,0 +1,110 @@
+// base.js - provider interface and types
+
+/**
+ * @typedef {Object} Message
+ * @property {string} role - The role of the message sender (e.g., 'user', 'assistant', 'system')
+ * @property {string} content - The content of the message
+ */
+
+/**
+ * @typedef {Object} ChatOptions
+ * @property {boolean} [stream] - Whether to stream the response
+ * @property {number} [temperature] - Temperature for response generation
+ * @property {number} [max_tokens] - Maximum tokens in response
+ * @property {string} model - Model identifier
+ */
+
+/**
+ * @typedef {Object} ModelCapabilities
+ * @property {boolean} text - Whether the model supports text
+ * @property {boolean} images - Whether the model supports images
+ * @property {boolean} [audio] - Whether the model supports audio
+ * @property {boolean} [video] - Whether the model supports video
+ * @property {string} [tokenizer] - The tokenizer used by the model
+ */
+
+/**
+ * @typedef {Object} ModelInfo
+ * @property {string} id - The model identifier
+ * @property {string} object - Type of object (usually 'model')
+ * @property {number} created - Creation timestamp
+ * @property {string} owned_by - Model owner/provider
+ * @property {Array} permission - Permission settings
+ * @property {string} root - Base model identifier
+ * @property {string|null} parent - Parent model identifier
+ * @property {number} context_length - Maximum context length
+ * @property {ModelCapabilities} capabilities - Model capabilities
+ */
+
+/**
+ * @typedef {Object} ChatResponseChoice
+ * @property {number} index - Choice index
+ * @property {Message} message - Response message
+ * @property {string|null} finish_reason - Why generation stopped
+ */
+
+/**
+ * @typedef {Object} ChatResponse
+ * @property {string} id - Response identifier
+ * @property {string} object - Type of object (usually 'chat.completion')
+ * @property {number} created - Creation timestamp
+ * @property {string} model - Model used
+ * @property {ChatResponseChoice[]} choices - Response choices
+ * @property {Object} usage - Token usage statistics
+ */
+
+/**
+ * Base Provider Interface
+ * @interface
+ */
+class Provider {
+  /** @type {boolean} */
+  supportsStreaming = false;
+
+  /**
+   * Format a model name with provider prefix
+   * @param {string} modelId - Raw model identifier
+   * @returns {string} - Formatted model identifier
+   */
+  formatModelName(modelId) {
+    throw new Error('Not implemented');
+  }
+
+  /**
+   * Get base model name without provider prefix
+   * @param {string} model - Formatted model identifier
+   * @returns {string} - Base model name
+   */
+  getBaseModelName(model) {
+    throw new Error('Not implemented');
+  }
+
+  /**
+   * Check if provider can handle this model
+   * @param {string} model - Model identifier to check
+   * @returns {Promise<boolean>} - Whether provider can handle model
+   */
+  async canHandle(model) {
+    throw new Error('Not implemented');
+  }
+
+  /**
+   * Send chat request to provider
+   * @param {Message[]} messages - Chat messages
+   * @param {ChatOptions} options - Chat options
+   * @returns {Promise<ChatResponse|AsyncGenerator>} - Chat response
+   */
+  async chat(messages, options) {
+    throw new Error('Not implemented');
+  }
+
+  /**
+   * Get available models from provider
+   * @returns {Promise<ModelInfo[]>} - Available models
+   */
+  async getModels() {
+    throw new Error('Not implemented');
+  }
+}
+
+export default Provider;
\ No newline at end of file
diff --git a/src/providers/cerebras.js b/src/providers/cerebras.js
new file mode 100644
index 0000000..666e9d6
--- /dev/null
+++ b/src/providers/cerebras.js
@@ -0,0 +1,246 @@
+// Cerebras provider - implements Cerebras' API via OpenAI compatibility
+import OpenAI from 'openai';
+
+class CerebrasProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_CEREBRAS !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.CEREBRAS_API_KEY) {
+      this.apiKeys.push({ key: process.env.CEREBRAS_API_KEY, lastError: null });
+    }
+    if (process.env.CEREBRAS_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.CEREBRAS_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.CEREBRAS_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.CEREBRAS_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('Cerebras initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.cerebras.ai/v1'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('cerebras/') ? modelId : `cerebras/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^cerebras\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled || !this.client) return [];
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      const response = await this.client.models.list();
+      this.models.clear();
+
+      for (const model of response.data) {
+        const modelId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(modelId)) {
+          this.models.set(modelId, {
+            id: modelId,
+            object: 'model',
+            created: model.created || Date.now(),
+            owned_by: 'https://cerebras.ai',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: model.context_length || 8192,
+            capabilities: {
+              text: true,
+              images: false,
+              audio: false,
+              video: false
+            }
+          });
+        }
+      }
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      console.error('Failed to fetch Cerebras models:', error);
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('cerebras provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No Cerebras API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('Cerebras API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All Cerebras API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new CerebrasProvider();
\ No newline at end of file
diff --git a/src/providers/claude.js b/src/providers/claude.js
new file mode 100644
index 0000000..bfb9f96
--- /dev/null
+++ b/src/providers/claude.js
@@ -0,0 +1,313 @@
+// claude.js provider - anthropics fancy ai friend
+import OpenAI from 'openai';
+
+class ClaudeProvider {
+  constructor() {
+    this.enabled = Boolean(process.env.ANTHROPIC_API_KEY);
+    
+    // setup claude client using openai sdk since they have a compatible api
+    this.client = this.enabled ? new OpenAI({
+      apiKey: process.env.ANTHROPIC_API_KEY,
+      baseURL: 'https://api.anthropic.com/v1',
+      defaultHeaders: {
+        'anthropic-version': '2024-02-01'  // updated to latest version
+      }
+    }) : null;
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = [];
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+    this.retryAttempts = 3;
+    this.retryDelay = 1000; // 1 second
+    
+    // known claude models and their capabilities
+    this.knownModels = new Map([
+      ['claude-3-opus-20240229', { context: 200000, vision: true }],
+      ['claude-3-sonnet-20240229', { context: 200000, vision: true }],
+      ['claude-3-haiku-20240307', { context: 200000, vision: true }],
+      ['claude-2.1', { context: 200000, vision: false }],
+      ['claude-2.0', { context: 100000, vision: false }],
+      ['claude-instant-1.2', { context: 100000, vision: false }]
+    ]);
+  }
+
+  async sleep(ms) {
+    return new Promise(resolve => setTimeout(resolve, ms));
+  }
+
+  validateMessage(msg) {
+    if (!msg || typeof msg !== 'object') {
+      throw new Error('invalid message format');
+    }
+
+    if (!['user', 'assistant', 'system'].includes(msg.role)) {
+      throw new Error(`invalid message role: ${msg.role}`);
+    }
+
+    // handle different content types
+    if (Array.isArray(msg.content)) {
+      // validate each content part
+      msg.content.forEach(part => {
+        if (part.type === 'text' && typeof part.text !== 'string') {
+          throw new Error('invalid text content');
+        }
+        if (part.type === 'image' && (!part.image_url || typeof part.image_url.url !== 'string')) {
+          throw new Error('invalid image content');
+        }
+      });
+    } else if (typeof msg.content !== 'string') {
+      throw new Error('invalid message content');
+    }
+
+    return true;
+  }
+
+  formatModelName(modelId) {
+    // if it already has our prefix, use it as-is
+    if (modelId.startsWith('claude/')) {
+      return modelId;
+    }
+    return `claude/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    // if it starts with our prefix, remove it
+    if (model.startsWith('claude/')) {
+      return model.replace('claude/', '');
+    }
+    // if it's not our model, return as-is
+    return model;
+  }
+
+  disableModel(model) {
+    // store the full prefixed name
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async updateAvailableModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) {
+      return [];
+    }
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.length > 0) {
+      return this.models.map(m => m.id);
+    }
+
+    try {
+      const response = await this.client.models.list();
+      this.models = Array.from(response)
+        .filter(model => this.knownModels.has(model.id))
+        .map(model => {
+          const capabilities = this.knownModels.get(model.id);
+          return {
+            id: this.formatModelName(model.id),
+            object: 'model',
+            created: Date.now(),
+            owned_by: 'https://anthropic.com',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: capabilities.context,
+            capabilities: {
+              text: true,
+              images: capabilities.vision,
+              audio: false,
+              video: false
+            }
+          };
+        });
+
+      this.lastUpdate = now;
+      return this.models.map(m => m.id);
+    } catch (error) {
+      console.error('Failed to fetch Claude models:', error);
+      // Don't update lastUpdate on error so we'll try again sooner
+      return this.models.map(m => m.id);
+    }
+  }
+
+  async canHandle(model) {
+    // if provider is not enabled, can't handle any models
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  transformResponse(response, model) {
+    if (!response) {
+      throw new Error('empty response from api');
+    }
+
+    // convert from anthropic format to openai format
+    return {
+      id: response.id || ('claude-' + Math.random().toString(36).substring(2)),
+      object: 'chat.completion',
+      created: Math.floor(Date.now() / 1000),
+      model: this.formatModelName(model),
+      choices: [{
+        index: 0,
+        message: {
+          role: 'assistant',
+          content: response.choices?.[0]?.message?.content || response.content || ''
+        },
+        finish_reason: response.choices?.[0]?.finish_reason || 'stop'
+      }],
+      usage: response.usage || {
+        prompt_tokens: 0,
+        completion_tokens: 0,
+        total_tokens: 0
+      }
+    };
+  }
+
+  async chat(messages, options = {}) {
+    // if provider is not enabled, fail fast
+    if (!this.enabled) {
+      throw new Error('claude provider not enabled');
+    }
+
+    // validate input
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    messages.forEach(msg => this.validateMessage(msg));
+
+    const { model, stream, temperature, max_tokens } = options;
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+    if (!this.knownModels.has(baseModel)) {
+      throw new Error(`unsupported model: ${baseModel}`);
+    }
+
+    // validate temperature
+    if (temperature !== undefined && (typeof temperature !== 'number' || temperature < 0 || temperature > 1)) {
+      throw new Error('temperature must be between 0 and 1');
+    }
+
+    // validate max_tokens
+    if (max_tokens !== undefined && (typeof max_tokens !== 'number' || max_tokens < 1)) {
+      throw new Error('max_tokens must be a positive number');
+    }
+
+    let attempt = 0;
+    while (attempt < this.retryAttempts) {
+      try {
+        // Create completion with the OpenAI SDK
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages: messages.map(msg => ({
+            role: msg.role,
+            content: msg.content
+          })),
+          stream,
+          temperature,
+          max_tokens,
+          ...options
+        });
+
+        // Handle streaming responses
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Return regular response
+        return this.transformResponse(completion, model);
+      } catch (error) {
+        attempt++;
+
+        // Handle different error types
+        if (error.status === 401 || error.status === 403) {
+          throw new Error('unauthorized: check your api key');
+        }
+
+        if (error.status === 402) {
+          this.disableModel(model);
+          throw new Error('model quota exceeded');
+        }
+
+        if (error.status === 429) {
+          if (attempt < this.retryAttempts) {
+            const delay = this.retryDelay * Math.pow(2, attempt - 1); // exponential backoff
+            await this.sleep(delay);
+            continue;
+          }
+          throw new Error('rate limit exceeded');
+        }
+
+        if (error.status >= 500) {
+          if (attempt < this.retryAttempts) {
+            await this.sleep(this.retryDelay);
+            continue;
+          }
+        }
+
+        // Unknown error
+        throw error;
+      }
+    }
+
+    throw new Error('max retry attempts exceeded');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) {
+          continue; // skip invalid chunks
+        }
+
+        // Format chunks to match OpenAI format
+        yield {
+          id: chunk.id || ('claude-' + Math.random().toString(36).substring(2)),
+          object: 'chat.completion.chunk',
+          created: Math.floor(Date.now() / 1000),
+          model: this.formatModelName(model),
+          choices: [{
+            index: 0,
+            delta: {
+              content: chunk.choices[0].delta.content || ''
+            },
+            finish_reason: chunk.choices[0].finish_reason
+          }]
+        };
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 402) {
+        this.disableModel(model);
+        throw new Error('model quota exceeded');
+      }
+      if (error.status === 429) {
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+
+  async getModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return this.models.filter(model => !this.disabledModels.has(model.id));
+  }
+}
+
+export default ClaudeProvider;
\ No newline at end of file
diff --git a/src/providers/copilot-more.js b/src/providers/copilot-more.js
new file mode 100644
index 0000000..b28c4a7
--- /dev/null
+++ b/src/providers/copilot-more.js
@@ -0,0 +1,275 @@
+// copilot-more provider using the openai sdk - no auth needed which is pretty nice
+import OpenAI from 'openai';
+
+class CopilotMoreProvider {
+  constructor() {
+    // only need api url, no token needed
+    this.enabled = Boolean(process.env.COPILOT_MORE_API_URL);
+    
+    // setup client with openai sdk
+    this.client = this.enabled ? new OpenAI({
+      apiKey: 'anything', // accepts any value
+      baseURL: process.env.COPILOT_MORE_API_URL
+    }) : null;
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+    this.retryAttempts = 3;
+    this.retryDelay = 1000; // 1 second
+  }
+
+  async sleep(ms) {
+    return new Promise(resolve => setTimeout(resolve, ms));
+  }
+
+  validateMessage(msg) {
+    if (!msg || typeof msg !== 'object') {
+      throw new Error('invalid message format');
+    }
+
+    if (!['user', 'assistant', 'system'].includes(msg.role)) {
+      throw new Error(`invalid message role: ${msg.role}`);
+    }
+
+    if (typeof msg.content !== 'string') {
+      throw new Error('invalid message content');
+    }
+
+    return true;
+  }
+
+  formatModelName(modelId) {
+    // if it already has our prefix, use it as-is
+    if (modelId.startsWith('copilot-more/')) {
+      return modelId;
+    }
+    return `copilot-more/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    // if it starts with our prefix, remove it
+    if (model.startsWith('copilot-more/')) {
+      return model.replace('copilot-more/', '');
+    }
+    // if it's not our model, return as-is
+    return model;
+  }
+
+  disableModel(model) {
+    // store the full prefixed name
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async updateAvailableModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) {
+      return [];
+    }
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      // fetch available models from the api
+      const response = await this.client.models.list();
+      const models = response.data || [];
+
+      this.models.clear();
+      for (const model of models) {
+        const formattedId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(formattedId)) {
+          this.models.set(formattedId, {
+            id: formattedId,
+            object: 'model',
+            created: Date.now(),
+            owned_by: 'https://github.com/jjleng/copilot-more',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: model.context_window || 8192, // default if not specified
+            capabilities: {
+              text: true,
+              images: false,
+              audio: false,
+              video: false
+            }
+          });
+        }
+      }
+    } catch (error) {
+      console.error('failed to fetch copilot-more models:', error);
+      return Array.from(this.models.keys());
+    }
+
+    this.lastUpdate = now;
+    return Array.from(this.models.keys());
+  }
+
+  async canHandle(model) {
+    // if provider is not enabled, can't handle any models
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  transformResponse(response, model) {
+    if (!response) {
+      throw new Error('empty response from api');
+    }
+
+    // Responses should be in OpenAI format already
+    if (response && typeof response === 'object') {
+      if (response.model) {
+        response.model = this.formatModelName(model);
+      }
+      if (response.choices && Array.isArray(response.choices)) {
+        response.choices.forEach(choice => {
+          if (choice.model) {
+            choice.model = this.formatModelName(model);
+          }
+        });
+      }
+    }
+    return response;
+  }
+
+  async chat(messages, options = {}) {
+    // if provider is not enabled, fail fast
+    if (!this.enabled) {
+      throw new Error('copilot-more provider not enabled');
+    }
+
+    // validate input
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    messages.forEach(msg => this.validateMessage(msg));
+
+    const { model, stream, temperature, max_tokens } = options;
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+
+    // validate temperature
+    if (temperature !== undefined && (typeof temperature !== 'number' || temperature < 0 || temperature > 2)) {
+      throw new Error('temperature must be between 0 and 2');
+    }
+
+    // validate max_tokens 
+    if (max_tokens !== undefined && (typeof max_tokens !== 'number' || max_tokens < 1)) {
+      throw new Error('max_tokens must be a positive number');
+    }
+
+    let attempt = 0;
+    while (attempt < this.retryAttempts) {
+      try {
+        // Create completion with the OpenAI SDK
+        const completion = await this.client.chat.completions.create({
+          model: baseModel, // use base model name without prefix
+          messages: messages.map(msg => ({
+            role: msg.role,
+            content: msg.content
+          })),
+          stream,
+          temperature,
+          max_tokens,
+          ...options
+        });
+
+        // Handle streaming responses
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Return regular response
+        return this.transformResponse(completion, model);
+      } catch (error) {
+        attempt++;
+
+        // Handle different error types
+        if (error.status === 401 || error.status === 403) {
+          throw new Error('api authorization failed');
+        }
+
+        if (error.status === 402) {
+          this.disableModel(model);
+          throw new Error('model quota exceeded');
+        }
+
+        if (error.status === 429) {
+          if (attempt < this.retryAttempts) {
+            const delay = this.retryDelay * Math.pow(2, attempt - 1); // exponential backoff
+            await this.sleep(delay);
+            continue;
+          }
+          throw new Error('rate limit exceeded');
+        }
+
+        if (error.status >= 500) {
+          if (attempt < this.retryAttempts) {
+            await this.sleep(this.retryDelay);
+            continue;
+          }
+        }
+
+        // Unknown error
+        throw error;
+      }
+    }
+
+    throw new Error('max retry attempts exceeded');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) {
+          continue; // skip invalid chunks
+        }
+
+        // Transform chunks to include our model name
+        if (chunk.model) {
+          chunk.model = this.formatModelName(model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        throw new Error('api authorization failed');  
+      }
+      if (error.status === 402) {
+        this.disableModel(model);
+        throw new Error('model quota exceeded');
+      }
+      if (error.status === 429) {
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+
+  async getModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+}
+
+export default CopilotMoreProvider;
\ No newline at end of file
diff --git a/src/providers/deepinfra.js b/src/providers/deepinfra.js
new file mode 100644
index 0000000..c9e4006
--- /dev/null
+++ b/src/providers/deepinfra.js
@@ -0,0 +1,244 @@
+// DeepInfra provider - implements DeepInfra's API via OpenAI compatibility
+import OpenAI from 'openai';
+
+class DeepInfraProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_DEEPINFRA !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.DEEPINFRA_API_KEY) {
+      this.apiKeys.push({ key: process.env.DEEPINFRA_API_KEY, lastError: null });
+    }
+    if (process.env.DEEPINFRA_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.DEEPINFRA_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.DEEPINFRA_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.DEEPINFRA_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('DeepInfra initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.deepinfra.com/v1/openai'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('deepinfra/') ? modelId : `deepinfra/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^deepinfra\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled || !this.client) return [];
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      const response = await this.client.models.list();
+      this.models.clear();
+
+      for (const model of response.data) {
+        const modelId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(modelId)) {
+          this.models.set(modelId, {
+            id: modelId,
+            object: 'model',
+            created: model.created || Date.now(),
+            owned_by: 'https://deepinfra.com',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: model.context_length || 4096,
+            capabilities: {
+              text: true,
+              images: model.id.toLowerCase().includes('vision')
+            }
+          });
+        }
+      }
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      console.error('Failed to fetch DeepInfra models:', error);
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('deepinfra provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No DeepInfra API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('DeepInfra API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All DeepInfra API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new DeepInfraProvider();
\ No newline at end of file
diff --git a/src/providers/deepseek-free.js b/src/providers/deepseek-free.js
new file mode 100644
index 0000000..e9ce18b
--- /dev/null
+++ b/src/providers/deepseek-free.js
@@ -0,0 +1,275 @@
+// deepseek-free provider using deepseek-free-api, pretty cool but kinda janky lol
+import OpenAI from 'openai';
+
+class DeepseekFreeProvider {
+  constructor() {
+    this.enabled = Boolean(process.env.DEEPSEEK_FREE_API_TOKEN && process.env.DEEPSEEK_FREE_API_URL);
+    
+    // setup deepseek free client with openai sdk
+    this.client = this.enabled ? new OpenAI({
+      apiKey: process.env.DEEPSEEK_FREE_API_TOKEN,
+      baseURL: process.env.DEEPSEEK_FREE_API_URL
+    }) : null;
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+    this.retryAttempts = 3;
+    this.retryDelay = 1000; // 1 second
+
+    // just the one model for free api
+    this.knownModels = new Map([
+      ['deepseek-chat', { context: 64000, vision: false }]
+    ]);
+  }
+
+  async sleep(ms) {
+    return new Promise(resolve => setTimeout(resolve, ms));
+  }
+
+  validateMessage(msg) {
+    if (!msg || typeof msg !== 'object') {
+      throw new Error('invalid message format');
+    }
+
+    if (!['user', 'assistant', 'system'].includes(msg.role)) {
+      throw new Error(`invalid message role: ${msg.role}`);
+    }
+
+    if (typeof msg.content !== 'string') {
+      throw new Error('invalid message content');
+    }
+
+    return true;
+  }
+
+  formatModelName(modelId) {
+    // if it already has our prefix, use it as-is
+    if (modelId.startsWith('deepseek-free/')) {
+      return modelId;
+    }
+    return `deepseek-free/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    // if it starts with our prefix, remove it
+    if (model.startsWith('deepseek-free/')) {
+      return model.replace('deepseek-free/', '');
+    }
+    // if it's not our model, return as-is
+    return model;
+  }
+
+  disableModel(model) {
+    // store the full prefixed name
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async updateAvailableModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) {
+      return [];
+    }
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    // we know we only have one model for the free api
+    this.models.clear();
+    for (const [modelId, capabilities] of this.knownModels) {
+      const formattedId = this.formatModelName(modelId);
+      if (!this.disabledModels.has(formattedId)) {
+        this.models.set(formattedId, {
+          id: formattedId,
+          object: 'model',
+          created: Date.now(),
+          owned_by: 'https://chat.deepseek.com',
+          permission: [],
+          root: modelId,
+          parent: null,
+          context_length: capabilities.context,
+          capabilities: {
+            text: true,
+            images: capabilities.vision,
+            audio: false,
+            video: false
+          }
+        });
+      }
+    }
+
+    this.lastUpdate = now;
+    return Array.from(this.models.keys());
+  }
+
+  async canHandle(model) {
+    // if provider is not enabled, can't handle any models
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  transformResponse(response, model) {
+    if (!response) {
+      throw new Error('empty response from api');
+    }
+
+    // Responses should be in OpenAI format already
+    if (response && typeof response === 'object') {
+      if (response.model) {
+        response.model = this.formatModelName(model);
+      }
+      if (response.choices && Array.isArray(response.choices)) {
+        response.choices.forEach(choice => {
+          if (choice.model) {
+            choice.model = this.formatModelName(model);
+          }
+        });
+      }
+    }
+    return response;
+  }
+
+  async chat(messages, options = {}) {
+    // if provider is not enabled, fail fast
+    if (!this.enabled) {
+      throw new Error('deepseek-free provider not enabled');
+    }
+
+    // validate input
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    messages.forEach(msg => this.validateMessage(msg));
+
+    const { model, stream, temperature, max_tokens } = options;
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+    const modelInfo = this.knownModels.get(baseModel);
+    if (!modelInfo) {
+      throw new Error(`unsupported model: ${baseModel}`);
+    }
+
+    // validate temperature
+    if (temperature !== undefined && (typeof temperature !== 'number' || temperature < 0 || temperature > 2)) {
+      throw new Error('temperature must be between 0 and 2');
+    }
+
+    // validate max_tokens 
+    if (max_tokens !== undefined && (typeof max_tokens !== 'number' || max_tokens < 1)) {
+      throw new Error('max_tokens must be a positive number');
+    }
+
+    let attempt = 0;
+    while (attempt < this.retryAttempts) {
+      try {
+        // Create completion with the OpenAI SDK, always use 'deepseek' as model name
+        const completion = await this.client.chat.completions.create({
+          model: 'deepseek',
+          messages: messages.map(msg => ({
+            role: msg.role,
+            content: msg.content
+          })),
+          stream,
+          temperature,
+          max_tokens,
+          ...options
+        });
+
+        // Handle streaming responses
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Return regular response
+        return this.transformResponse(completion, model);
+      } catch (error) {
+        attempt++;
+
+        // Handle different error types
+        if (error.status === 401 || error.status === 403) {
+          throw new Error('unauthorized: check your api token');
+        }
+
+        if (error.status === 402) {
+          this.disableModel(model);
+          throw new Error('model quota exceeded');
+        }
+
+        if (error.status === 429) {
+          if (attempt < this.retryAttempts) {
+            const delay = this.retryDelay * Math.pow(2, attempt - 1); // exponential backoff
+            await this.sleep(delay);
+            continue;
+          }
+          throw new Error('rate limit exceeded');
+        }
+
+        if (error.status >= 500) {
+          if (attempt < this.retryAttempts) {
+            await this.sleep(this.retryDelay);
+            continue;
+          }
+        }
+
+        // Unknown error
+        throw error;
+      }
+    }
+
+    throw new Error('max retry attempts exceeded');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) {
+          continue; // skip invalid chunks
+        }
+
+        // Transform chunks to include our model name
+        if (chunk.model) {
+          chunk.model = this.formatModelName(model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        throw new Error('unauthorized: check your api token');
+      }
+      if (error.status === 402) {
+        this.disableModel(model);
+        throw new Error('model quota exceeded');
+      }
+      if (error.status === 429) {
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+
+  async getModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+}
+
+export default DeepseekFreeProvider;
\ No newline at end of file
diff --git a/src/providers/deepseek.js b/src/providers/deepseek.js
new file mode 100644
index 0000000..c47e363
--- /dev/null
+++ b/src/providers/deepseek.js
@@ -0,0 +1,313 @@
+// deepseek.js provider - fast and affordable llms
+import OpenAI from 'openai';
+
+class DeepseekProvider {
+  constructor() {
+    this.enabled = Boolean(process.env.DEEPSEEK_API_KEY);
+    
+    // setup deepseek client with openai sdk
+    this.client = this.enabled ? new OpenAI({
+      apiKey: process.env.DEEPSEEK_API_KEY,
+      baseURL: 'https://api.deepseek.com/v1'
+    }) : null;
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+    this.retryAttempts = 3;
+    this.retryDelay = 1000; // 1 second
+
+    // known deepseek models and their capabilities
+    this.knownModels = new Map([
+      ['deepseek-chat', { context: 32000, vision: false }],
+      ['deepseek-chat-v1', { context: 32000, vision: false }],
+      ['deepseek-coder', { context: 32000, vision: false }],
+      ['deepseek-coder-v1', { context: 32000, vision: false }],
+      ['deepseek-math', { context: 16000, vision: false }],
+      ['deepseek-vision', { context: 32000, vision: true }]
+    ]);
+  }
+
+  async sleep(ms) {
+    return new Promise(resolve => setTimeout(resolve, ms));
+  }
+
+  validateMessage(msg) {
+    if (!msg || typeof msg !== 'object') {
+      throw new Error('invalid message format');
+    }
+
+    if (!['user', 'assistant', 'system'].includes(msg.role)) {
+      throw new Error(`invalid message role: ${msg.role}`);
+    }
+
+    // handle different content types
+    if (Array.isArray(msg.content)) {
+      // validate each content part
+      msg.content.forEach(part => {
+        if (part.type === 'text' && typeof part.text !== 'string') {
+          throw new Error('invalid text content');
+        }
+        if (part.type === 'image_url' && (!part.image_url || typeof part.image_url.url !== 'string')) {
+          throw new Error('invalid image content');
+        }
+      });
+    } else if (typeof msg.content !== 'string') {
+      throw new Error('invalid message content');
+    }
+
+    return true;
+  }
+
+  formatModelName(modelId) {
+    // if it already has our prefix, use it as-is
+    if (modelId.startsWith('deepseek/')) {
+      return modelId;
+    }
+    return `deepseek/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    // if it starts with our prefix, remove it
+    if (model.startsWith('deepseek/')) {
+      return model.replace('deepseek/', '');
+    }
+    // if it's not our model, return as-is
+    return model;
+  }
+
+  disableModel(model) {
+    // store the full prefixed name
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async updateAvailableModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) {
+      return [];
+    }
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      // Get models directly from DeepSeek API
+      const models = await this.client.models.list();
+      this.models.clear();
+
+      // Process each model
+      for (const model of models.data) {
+        const formattedId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(formattedId)) {
+          const capabilities = this.knownModels.get(model.id) || {
+            context: model.context_length || 8192,
+            vision: model.id.includes('vision')
+          };
+
+          this.models.set(formattedId, {
+            id: formattedId,
+            object: 'model',
+            created: model.created || Date.now(),
+            owned_by: 'https://deepseek.ai',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: capabilities.context,
+            capabilities: {
+              text: true,
+              images: capabilities.vision,
+              audio: false,
+              video: false
+            }
+          });
+        }
+      }
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      console.error('Failed to fetch DeepSeek models:', error);
+      // Don't update lastUpdate on error so we'll try again sooner
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async canHandle(model) {
+    // if provider is not enabled, can't handle any models
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  transformResponse(response, model) {
+    if (!response) {
+      throw new Error('empty response from api');
+    }
+
+    // DeepSeek responses should already be in OpenAI format
+    if (response && typeof response === 'object') {
+      if (response.model) {
+        response.model = this.formatModelName(model);
+      }
+      if (response.choices && Array.isArray(response.choices)) {
+        response.choices.forEach(choice => {
+          if (choice.model) {
+            choice.model = this.formatModelName(model);
+          }
+        });
+      }
+    }
+    return response;
+  }
+
+  async chat(messages, options = {}) {
+    // if provider is not enabled, fail fast
+    if (!this.enabled) {
+      throw new Error('deepseek provider not enabled');
+    }
+
+    // validate input
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    messages.forEach(msg => this.validateMessage(msg));
+
+    const { model, stream, temperature, max_tokens } = options;
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+    const modelInfo = this.knownModels.get(baseModel);
+    if (!modelInfo) {
+      throw new Error(`unsupported model: ${baseModel}`);
+    }
+
+    // validate temperature
+    if (temperature !== undefined && (typeof temperature !== 'number' || temperature < 0 || temperature > 2)) {
+      throw new Error('temperature must be between 0 and 2');
+    }
+
+    // validate max_tokens
+    if (max_tokens !== undefined && (typeof max_tokens !== 'number' || max_tokens < 1)) {
+      throw new Error('max_tokens must be a positive number');
+    }
+
+    // don't allow image input for non-vision models
+    if (!modelInfo.vision && messages.some(msg => 
+      Array.isArray(msg.content) && 
+      msg.content.some(part => part.type === 'image_url')
+    )) {
+      throw new Error(`model ${baseModel} does not support image input`);
+    }
+
+    let attempt = 0;
+    while (attempt < this.retryAttempts) {
+      try {
+        // Create completion with the OpenAI SDK
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages: messages.map(msg => ({
+            role: msg.role,
+            content: msg.content
+          })),
+          stream,
+          temperature,
+          max_tokens,
+          ...options
+        });
+
+        // Handle streaming responses
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Return regular response
+        return this.transformResponse(completion, model);
+      } catch (error) {
+        attempt++;
+
+        // Handle different error types
+        if (error.status === 401 || error.status === 403) {
+          throw new Error('unauthorized: check your api key');
+        }
+
+        if (error.status === 402) {
+          this.disableModel(model);
+          throw new Error('model quota exceeded');
+        }
+
+        if (error.status === 429) {
+          if (attempt < this.retryAttempts) {
+            const delay = this.retryDelay * Math.pow(2, attempt - 1); // exponential backoff
+            await this.sleep(delay);
+            continue;
+          }
+          throw new Error('rate limit exceeded');
+        }
+
+        if (error.status >= 500) {
+          if (attempt < this.retryAttempts) {
+            await this.sleep(this.retryDelay);
+            continue;
+          }
+        }
+
+        // Unknown error
+        throw error;
+      }
+    }
+
+    throw new Error('max retry attempts exceeded');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) {
+          continue; // skip invalid chunks
+        }
+
+        // DeepSeek chunks should be in OpenAI format
+        if (chunk.model) {
+          chunk.model = this.formatModelName(model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 402) {
+        this.disableModel(model);
+        throw new Error('model quota exceeded');
+      }
+      if (error.status === 429) {
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+
+  async getModels() {
+    // if provider is not enabled, return empty list
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+}
+
+export default DeepseekProvider;
\ No newline at end of file
diff --git a/src/providers/fireworks.js b/src/providers/fireworks.js
new file mode 100644
index 0000000..4e93108
--- /dev/null
+++ b/src/providers/fireworks.js
@@ -0,0 +1,246 @@
+// Fireworks provider - implements Fireworks' API via OpenAI compatibility
+import OpenAI from 'openai';
+
+class FireworksProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_FIREWORKS !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.FIREWORKS_API_KEY) {
+      this.apiKeys.push({ key: process.env.FIREWORKS_API_KEY, lastError: null });
+    }
+    if (process.env.FIREWORKS_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.FIREWORKS_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.FIREWORKS_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.FIREWORKS_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('Fireworks initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.fireworks.ai/inference/v1'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('fireworks/') ? modelId : `fireworks/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^fireworks\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled || !this.client) return [];
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      const response = await this.client.models.list();
+      this.models.clear();
+
+      for (const model of response.data) {
+        const modelId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(modelId)) {
+          this.models.set(modelId, {
+            id: modelId,
+            object: 'model',
+            created: model.created || Date.now(),
+            owned_by: 'https://fireworks.ai',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: model.context_length || 8192,
+            capabilities: {
+              text: true,
+              images: model.id.toLowerCase().includes('vision'),
+              audio: false,
+              video: false
+            }
+          });
+        }
+      }
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      console.error('Failed to fetch Fireworks models:', error);
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('fireworks provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No Fireworks API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('Fireworks API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All Fireworks API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new FireworksProvider();
\ No newline at end of file
diff --git a/src/providers/glama.js b/src/providers/glama.js
new file mode 100644
index 0000000..7e695d4
--- /dev/null
+++ b/src/providers/glama.js
@@ -0,0 +1,173 @@
+// GLAMA API Provider
+import OpenAI from 'openai';
+
+class GlamaProvider {
+    constructor() {
+        this.enabled = process.env.ENABLE_GLAMA !== 'false';
+        this.baseURL = 'https://glama.ai/api/gateway/openai/v1';
+        this.modelsEndpoint = 'https://glama.ai/api/gateway/v1/models';
+        this.apiKeys = [];
+        this.models = new Map();
+        this.lastUpdate = 0;
+        this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+        // Setup API keys
+        if (process.env.GLAMA_API_KEY) {
+            this.apiKeys.push({ key: process.env.GLAMA_API_KEY, isPrimary: true, lastError: null });
+        }
+        if (process.env.GLAMA_BACKUP_KEY_1) {
+            this.apiKeys.push({ key: process.env.GLAMA_BACKUP_KEY_1, isPrimary: false, lastError: null });
+        }
+        if (process.env.GLAMA_BACKUP_KEY_2) {
+            this.apiKeys.push({ key: process.env.GLAMA_BACKUP_KEY_2, isPrimary: false, lastError: null });
+        }
+
+        this.enabled = this.enabled && this.apiKeys.length > 0;
+        this.activeKeyIndex = 0;
+
+        this.supportsStreaming = true;
+    }
+
+    createClient(keyIndex = this.activeKeyIndex) {
+        const key = this.apiKeys[keyIndex]?.key;
+        if (!key) return null;
+
+        return new OpenAI({
+            apiKey: key,
+            baseURL: this.baseURL,
+            timeout: 30000
+        });
+    }
+
+    formatModelName(modelId) {
+        if (modelId.startsWith('glama/')) {
+            return modelId;
+        }
+        return `glama/${modelId}`;
+    }
+
+    getBaseModelName(model) {
+        return model.replace(/^glama\//, '');
+    }
+
+    async updateAvailableModels() {
+        if (!this.enabled) return [];
+
+        const now = Date.now();
+        if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+            return Array.from(this.models.keys());
+        }
+
+        try {
+            const response = await fetch(this.modelsEndpoint);
+            if (!response.ok) {
+                throw new Error(`HTTP error! status: ${response.status}`);
+            }
+
+            const models = await response.json();
+            this.models.clear();
+
+            for (const model of models) {
+                // Only include free models
+                if (parseFloat(model.pricePerToken?.input || '1') > 0 || parseFloat(model.pricePerToken?.output || '1') > 0) {
+                    continue;
+                }
+
+                // Check for text input/output capability
+                const capabilities = model.capabilities || [];
+                if (!capabilities.includes('input:text') || !capabilities.includes('output:text')) {
+                    continue;
+                }
+
+                const formattedId = this.formatModelName(model.id);
+                this.models.set(formattedId, {
+                    id: formattedId,
+                    object: 'model',
+                    created: new Date(model.added).getTime(),
+                    owned_by: 'https://glama.ai',
+                    permission: [],
+                    root: model.id,
+                    parent: null,
+                    context_length: model.maxTokensInput || 32768,
+                    capabilities: {
+                        text: true,
+                        images: capabilities.includes('input:image') && capabilities.includes('output:image')
+                    }
+                });
+            }
+
+            this.lastUpdate = now;
+            return Array.from(this.models.keys());
+        } catch (error) {
+            console.error('Failed to fetch GLAMA models:', error);
+            return [];
+        }
+    }
+
+    async canHandle(model) {
+        if (!this.enabled) return false;
+
+        try {
+            const models = await this.updateAvailableModels();
+            return models.includes(this.formatModelName(model));
+        } catch {
+            return false;
+        }
+    }
+
+    async chat(messages, options = {}) {
+        if (!this.enabled) {
+            throw new Error('GLAMA provider is not enabled');
+        }
+
+        const client = this.createClient();
+        if (!client) throw new Error('No API key available');
+
+        const { model, stream } = options;
+        const baseModel = this.getBaseModelName(model);
+
+        try {
+            const completion = await client.chat.completions.create({
+                model: baseModel,
+                messages,
+                stream: Boolean(stream),
+                temperature: options.temperature,
+                max_tokens: options.max_tokens,
+                top_p: options.top_p
+            });
+
+            if (stream) {
+                return completion;
+            }
+
+            return {
+                id: completion.id,
+                object: 'chat.completion',
+                created: completion.created,
+                model: this.formatModelName(model),
+                choices: completion.choices.map(choice => ({
+                    index: choice.index,
+                    message: {
+                        role: 'assistant',
+                        content: choice.message.content
+                    },
+                    finish_reason: choice.finish_reason
+                })),
+                usage: completion.usage
+            };
+
+        } catch (error) {
+            console.error('GLAMA API error:', error);
+            throw error;
+        }
+    }
+
+    async getModels() {
+        if (!this.enabled) return [];
+        
+        await this.updateAvailableModels();
+        return Array.from(this.models.values());
+    }
+}
+
+export default new GlamaProvider();
\ No newline at end of file
diff --git a/src/providers/google.js b/src/providers/google.js
new file mode 100644
index 0000000..1617367
--- /dev/null
+++ b/src/providers/google.js
@@ -0,0 +1,224 @@
+// Google Provider
+class GoogleProvider {
+    constructor() {
+        // Load configuration
+        this.enabled = process.env.ENABLE_GOOGLE !== 'false';
+        this.primaryKey = process.env.GOOGLE_API_KEY;
+        this.backupKey1 = process.env.GOOGLE_API_KEY_BACKUP1;
+        this.backupKey2 = process.env.GOOGLE_API_KEY_BACKUP2;
+        this.models = new Map();
+        this.lastUpdate = 0;
+        this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+        // Enable if any key is available
+        this.enabled = this.enabled && (this.primaryKey || this.backupKey1 || this.backupKey2);
+
+        if (this.enabled) {
+            const keys = {
+                primaryKey: this.primaryKey ? '****' + this.primaryKey.slice(-4) : 'none',
+                backupKey1: this.backupKey1 ? '****' + this.backupKey1.slice(-4) : 'none',
+                backupKey2: this.backupKey2 ? '****' + this.backupKey2.slice(-4) : 'none'
+            };
+            console.log('Google initialized with keys:', keys);
+        }
+    }
+
+    formatModelName(modelId) {
+        if (modelId.startsWith('google/')) {
+            return modelId;
+        }
+        return `google/${modelId}`;
+    }
+
+    getBaseModelName(model) {
+        return model.replace(/^google\//, '');
+    }
+
+    async updateAvailableModels() {
+        const now = Date.now();
+        if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+            return Array.from(this.models.keys());
+        }
+
+        this.models.clear();
+
+        try {
+            const defaultModels = [
+                // Gemini Pro models
+                'gemini-1.5-pro-latest',
+                'gemini-1.5-pro-001',
+                'gemini-1.5-pro-002',
+                'gemini-1.5-pro',
+                
+                // Gemini Ultra models
+                'gemini-2.0-pro-exp',
+                'gemini-2.0-pro-exp-02-05',
+                'gemini-2.5-pro-exp-03-25',
+                
+                // Gemini Flash models (excluding vision variants)
+                'gemini-2.0-flash-exp',
+                'gemini-2.0-flash',
+                'gemini-2.0-flash-001',
+                'gemini-2.0-flash-lite-001',
+                'gemini-2.0-flash-lite',
+                'gemini-2.0-flash-lite-preview-02-05',
+                'gemini-2.0-flash-lite-preview',
+                
+                // Gemma models
+                'gemma-3-1b-it',
+                'gemma-3-4b-it',
+                'gemma-3-12b-it',
+                'gemma-3-27b-it'
+            ];
+
+            for (const modelId of defaultModels) {
+                const formattedId = this.formatModelName(modelId);
+                this.models.set(formattedId, {
+                    id: formattedId,
+                    object: 'model',
+                    created: Date.now(),
+                    owned_by: 'https://deepmind.google',
+                    permission: [],
+                    root: modelId,
+                    parent: null,
+                    context_length: 32768
+                });
+            }
+
+            this.lastUpdate = now;
+            return Array.from(this.models.keys());
+        } catch (error) {
+            console.error('Error fetching Google models:', error);
+            return [];
+        }
+    }
+
+    async getModels() {
+        if (!this.enabled) return [];
+        
+        await this.updateAvailableModels();
+        return Array.from(this.models.values());
+    }
+
+    async canHandle(model) {
+        if (!this.enabled) return false;
+
+        try {
+            const models = await this.updateAvailableModels();
+            return models.includes(this.formatModelName(model));
+        } catch {
+            return false;
+        }
+    }
+
+    // track which keys have hit rate limits
+    #rateLimitedKeys = new Set();
+
+    getApiKey() {
+        // Get all available keys that aren't rate limited
+        const availableKeys = [this.primaryKey, this.backupKey1, this.backupKey2]
+            .filter(key => key && !this.#rateLimitedKeys.has(key));
+        
+        if (availableKeys.length === 0) {
+            // if all keys are rate limited, clear the set and try again
+            this.#rateLimitedKeys.clear();
+            return this.primaryKey || this.backupKey1 || this.backupKey2;
+        }
+
+        return availableKeys[Math.floor(Math.random() * availableKeys.length)];
+    }
+
+    markKeyRateLimited(key) {
+        if (key) this.#rateLimitedKeys.add(key);
+    }
+
+    async chat(messages, options = {}) {
+        if (!this.enabled) {
+            throw new Error('Google provider is not enabled');
+        }
+
+        const apiKey = this.getApiKey();
+        if (!apiKey) {
+            throw new Error('No API key available');
+        }
+
+        const model = this.getBaseModelName(options.model || 'gemini-pro');
+        const temperature = options.temperature ?? 0.7;
+        const maxTokens = options.max_tokens;
+
+        try {
+            const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`, {
+                method: 'POST',
+                headers: {
+                    'Content-Type': 'application/json'
+                },
+                body: JSON.stringify({
+                    contents: messages.map(msg => ({
+                        role: msg.role === 'assistant' ? 'model' : 'user',
+                        parts: [{ text: msg.content }]
+                    })),
+                    generationConfig: {
+                        temperature,
+                        maxOutputTokens: maxTokens,
+                        stopSequences: options.stop || []
+                    },
+                    safetySettings: [
+                        { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
+                        { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
+                        { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE" },
+                        { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE" }
+                    ]
+                })
+            });
+if (!response.ok) {
+    const error = await response.json();
+    
+    // silently handle rate limits by trying backup keys
+    if (error.error?.message?.includes('quota') || error.error?.message?.includes('rate limit')) {
+        this.markKeyRateLimited(apiKey);
+                    
+                    // Try again with a different key if available
+                    const newKey = this.getApiKey();
+                    if (newKey && newKey !== apiKey) {
+                        // try again quietly with the new key
+                        return this.chat(messages, options);
+                    }
+                }
+                
+                throw new Error(error.error?.message || `HTTP error! status: ${response.status}`);
+            }
+
+            const data = await response.json();
+            
+            if (!data.candidates?.[0]?.content?.parts?.[0]?.text) {
+                throw new Error('Invalid response format from Google API');
+            }
+
+            return {
+                id: `gmn-${Date.now()}`,
+                object: 'chat.completion',
+                created: Date.now(),
+                model: this.formatModelName(model),
+                choices: [{
+                    index: 0,
+                    message: {
+                        role: 'assistant',
+                        content: data.candidates[0].content.parts[0].text
+                    },
+                    finish_reason: 'stop'
+                }],
+                usage: {
+                    prompt_tokens: 0,
+                    completion_tokens: 0,
+                    total_tokens: 0
+                }
+            };
+
+        } catch (error) {
+            console.error('Google API error:', error);
+            throw error;
+        }
+    }
+}
+
+export default new GoogleProvider();
\ No newline at end of file
diff --git a/src/providers/groq.js b/src/providers/groq.js
new file mode 100644
index 0000000..8e16aa0
--- /dev/null
+++ b/src/providers/groq.js
@@ -0,0 +1,243 @@
+// groq.js provider - fancy new hotness
+import OpenAI from 'openai';
+
+class GroqProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_GROQ !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.GROQ_API_KEY) {
+      this.apiKeys.push({ key: process.env.GROQ_API_KEY, lastError: null });
+    }
+    if (process.env.GROQ_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.GROQ_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.GROQ_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.GROQ_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+
+    // Initialize models Map
+    const modelsList = [
+      { name: 'allam-2-7b', context_length: 1024, image_support: false },
+      { name: 'deepseek-r1-distill-llama-70b', context_length: 131072, image_support: false },
+      { name: 'deepseek-r1-distill-qwen-32b', context_length: 131072, image_support: false },
+      { name: 'gemma2-9b-it', context_length: 8192, image_support: false },
+      { name: 'llama-3.1-8b-instant', context_length: 131072, image_support: false },
+      { name: 'llama-3.2-11b-vision-preview', context_length: 131072, image_support: true },
+      { name: 'llama-3.2-1b-preview', context_length: 131072, image_support: false },
+      { name: 'llama-3.2-3b-preview', context_length: 131072, image_support: false },
+      { name: 'llama-3.2-90b-vision-preview', context_length: 131072, image_support: true },
+      { name: 'llama-3.3-70b-specdec', context_length: 131072, image_support: false },
+      { name: 'llama-3.3-70b-versatile', context_length: 131072, image_support: false },
+      { name: 'llama-guard-3-8b', context_length: 8192, image_support: false },
+      { name: 'llama3-70b-8192', context_length: 131072, image_support: false },
+      { name: 'llama3-8b-8192', context_length: 131072, image_support: false },
+      { name: 'qwen-2.5-32b', context_length: 131072, image_support: false },
+      { name: 'qwen-2.5-coder-32b', context_length: 131072, image_support: false },
+      { name: 'qwen-qwq-32b', context_length: 131072, image_support: false }
+    ];
+
+    // Remove models that require special terms
+    const requiresTerms = ['mistral-saba-24b'];
+    modelsList
+      .filter(model => !requiresTerms.includes(model.name))
+      .forEach(model => {
+        const modelId = this.formatModelName(model.name);
+        this.models.set(modelId, {
+          id: modelId,
+          object: 'model',
+          created: Date.now(),
+          owned_by: 'https://groq.com',
+          permission: [],
+          root: model.name,
+          parent: null,
+          context_length: model.context_length,
+          capabilities: {
+            text: true,
+            images: model.image_support
+          }
+        });
+      });
+
+    // Create OpenAI client for the active key
+    this.updateClient();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('Groq initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.groq.com/openai/v1'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('groq/') ? modelId : `groq/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^groq\//, '');
+  }
+
+  async getModels() {
+    return this.enabled ? Array.from(this.models.values()) : [];
+  }
+
+  async canHandle(model) {
+    return this.enabled && this.models.has(this.formatModelName(model));
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('groq provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, timeout, ...otherOptions } = options;
+    
+    // groq doesn't support timeout option
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+    if (!this.models.has(this.formatModelName(baseModel))) {
+      throw new Error(`unsupported model: ${baseModel}`);
+    }
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No Groq API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('Groq API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Check for model terms error
+        if (error?.error?.code === 'model_terms_required') {
+          this.disableModel(model);
+          throw new Error(`Model ${model} requires terms acceptance and has been disabled`);
+        }
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All Groq API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new GroqProvider();
\ No newline at end of file
diff --git a/src/providers/hackclub.js b/src/providers/hackclub.js
new file mode 100644
index 0000000..0a844c3
--- /dev/null
+++ b/src/providers/hackclub.js
@@ -0,0 +1,249 @@
+// hackclub.js provider - our local ai friend
+import * as cheerio from 'cheerio';
+
+class HackClubProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_HACKCLUB !== 'false'; // Enable by default unless explicitly disabled
+    this.supportsStreaming = true;
+    this.lastRateLimit = 0;
+    this.rateLimitDelay = 1000; // 1 second between requests
+    this.currentModel = 'unknown';
+    this.lastModelCheck = 0;
+    this.modelCheckInterval = 5 * 60 * 1000; // check model every 5 minutes
+    this.models = new Map();
+  }
+
+  formatModelName(modelId) {
+    // if it already has our prefix, use it as-is
+    if (modelId.startsWith('hackclub/')) {
+      return modelId;
+    }
+    return `hackclub/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    // if it starts with our prefix, remove it
+    if (model.startsWith('hackclub/')) {
+      return model.replace('hackclub/', '');
+    }
+    // if it's not our model, return as-is
+    return model;
+  }
+
+  async getCurrentModel() {
+    const now = Date.now();
+    if (now - this.lastModelCheck < this.modelCheckInterval) {
+      return this.currentModel;
+    }
+
+    try {
+      const controller = new AbortController();
+      const timeout = setTimeout(() => controller.abort(), 5000);
+
+      const response = await fetch('https://ai.hackclub.com', {
+        signal: controller.signal
+      });
+      clearTimeout(timeout);
+      
+      const html = await response.text();
+      
+      // Use cheerio to parse the HTML and find the model name
+      const $ = cheerio.load(html);
+      const modelName = $('b code').text().trim();
+      
+      if (modelName) {
+        this.currentModel = modelName;
+      } else {
+        console.warn('Could not find model name in HackClub homepage');
+        this.currentModel = 'llama-3.3-70b-versatile';
+      }
+    } catch (error) {
+      console.error('Failed to get current HackClub model:', error);
+      this.currentModel = 'llama-3.3-70b-versatile';
+      if (error.name === 'AbortError') {
+        console.warn('HackClub model check timed out, using default model');
+      }
+    }
+
+    this.lastModelCheck = now;
+    return this.currentModel;
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+    return model.startsWith('hackclub/');
+  }
+
+  transformResponse(response, model) {
+    const modelName = response.model || model;
+
+    return {
+      id: response.id || ('hackclub-' + Math.random().toString(36).substring(2)),
+      object: response.object || 'chat.completion',
+      created: response.created || Math.floor(Date.now() / 1000),
+      model: this.formatModelName(modelName),
+      choices: [{
+        index: 0,
+        message: {
+          role: 'assistant',
+          content: response.choices?.[0]?.message?.content || response
+        },
+        finish_reason: response.choices?.[0]?.finish_reason || 'stop'
+      }],
+      usage: response.usage || {
+        prompt_tokens: 0,
+        completion_tokens: 0,
+        total_tokens: 0
+      }
+    };
+  }
+
+  async sleep(ms) {
+    return new Promise(resolve => setTimeout(resolve, ms));
+  }
+
+  async handleRateLimit() {
+    const now = Date.now();
+    const timeSinceLastRequest = now - this.lastRateLimit;
+    
+    if (timeSinceLastRequest < this.rateLimitDelay) {
+      await this.sleep(this.rateLimitDelay - timeSinceLastRequest);
+    }
+    
+    this.lastRateLimit = Date.now();
+  }
+
+  async makeRequest(url, options) {
+    if (!this.enabled) {
+      throw new Error('HackClub provider is not enabled');
+    }
+
+    await this.handleRateLimit();
+    const response = await fetch(url, options);
+
+    if (!response.ok) {
+      if (response.status === 429) {
+        this.rateLimitDelay = Math.min(this.rateLimitDelay * 2, 10000);
+        throw new Error('Rate limited by HackClub API');
+      }
+      const text = await response.text();
+      throw new Error(`HackClub API returned ${response.status}: ${text}`);
+    }
+
+    return response;
+  }
+
+  async *handleStream(response, model) {
+    if (!response.body) {
+      throw new Error('Response has no body');
+    }
+
+    const reader = response.body.getReader();
+    const decoder = new TextDecoder();
+    let buffer = '';
+
+    try {
+      while (true) {
+        const { value, done } = await reader.read();
+        if (done) break;
+
+        buffer += decoder.decode(value, { stream: true });
+        const lines = buffer.split('\n');
+        buffer = lines.pop() || '';
+
+        for (const line of lines) {
+          if (!line.trim() || !line.startsWith('data: ')) continue;
+          if (line.includes('[DONE]')) return;
+
+          const data = JSON.parse(line.slice(6));
+          if (!data.choices?.[0]?.delta) continue;
+
+          yield {
+            id: data.id || ('hackclub-' + Math.random().toString(36).substring(2)),
+            object: 'chat.completion.chunk',
+            created: data.created || Math.floor(Date.now() / 1000),
+            model: this.formatModelName(model),
+            choices: [{
+              index: 0,
+              delta: {
+                role: data.choices[0].delta.role,
+                content: data.choices[0].delta.content || ''
+              },
+              finish_reason: data.choices[0].finish_reason
+            }]
+          };
+        }
+      }
+    } finally {
+      reader.releaseLock();
+    }
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('HackClub provider is not enabled');
+    }
+
+    const currentModel = await this.getCurrentModel();
+    
+    try {
+      const response = await this.makeRequest('https://ai.hackclub.com/chat/completions', {
+        method: 'POST',
+        headers: {
+          'Content-Type': 'application/json',
+          'Accept': options.stream ? 'text/event-stream' : 'application/json'
+        },
+        body: JSON.stringify({
+          messages,
+          stream: options.stream,
+          model: currentModel
+        })
+      });
+
+      if (options.stream) {
+        return this.handleStream(response, currentModel);
+      }
+
+      const data = await response.json();
+      return this.transformResponse(data, currentModel);
+    } catch (error) {
+      // Only disable provider on permanent errors
+      if (error.message.includes('Failed to fetch') || error.status === 404) {
+        this.enabled = false;
+      }
+
+      const enhancedError = new Error(`HackClub API error: ${error.message}`);
+      enhancedError.cause = error;
+      enhancedError.requestBody = messages;
+      throw enhancedError;
+    }
+  }
+
+  async getModelInfo() {
+    if (!this.enabled) return null;
+
+    const modelName = await this.getCurrentModel();
+    return {
+      id: this.formatModelName(modelName),
+      object: 'model',
+      created: Date.now(),
+      owned_by: 'https://hackclub.com',
+      permission: [],
+      root: 'hackclub',
+      parent: null,
+      context_length: 128000,
+      capabilities: {
+        text: true,
+        images: false
+      }
+    };
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    const modelInfo = await this.getModelInfo();
+    return modelInfo ? [modelInfo] : [];
+  }
+}
+
+export default new HackClubProvider();
\ No newline at end of file
diff --git a/src/providers/huggingchat.js b/src/providers/huggingchat.js
new file mode 100644
index 0000000..9493f04
--- /dev/null
+++ b/src/providers/huggingchat.js
@@ -0,0 +1,65 @@
+// HuggingChat Provider
+class HuggingChatProvider {
+    constructor() {
+        this.enabled = process.env.ENABLE_HUGGINGCHAT !== 'false';
+        this.models = new Map();
+        this.lastUpdate = 0;
+        this.updateInterval = 5 * 60 * 1000; // 5 minutes
+        this.supportsStreaming = false;
+
+        // Default models list
+        this.defaultModels = [
+            'hugging/mistralai/Mixtral-8x7B-Instruct-v0.1',
+            'hugging/mistralai/Mistral-7B-Instruct-v0.2',
+            'hugging/meta-llama/Llama-2-70b-chat-hf',
+            'hugging/openchat/openchat-3.5',
+            'hugging/google/gemma-7b-it',
+            'hugging/microsoft/phi-2'
+        ];
+
+        // Initialize models map with defaults
+        this.defaultModels.forEach(modelId => {
+            this.models.set(modelId, {
+                id: modelId,
+                object: 'model',
+                created: Date.now(),
+                owned_by: 'https://huggingface.co',
+                permission: [],
+                root: modelId.replace('hugging/', ''),
+                parent: null,
+                context_length: 4096,
+                capabilities: {
+                    text: true,
+                    images: false
+                }
+            });
+        });
+    }
+
+    formatModelName(modelId) {
+        if (modelId.startsWith('hugging/')) {
+            return modelId;
+        }
+        return `hugging/${modelId}`;
+    }
+
+    getBaseModelName(model) {
+        return model.replace(/^hugging\//, '');
+    }
+
+    async canHandle(model) {
+        if (!this.enabled) return false;
+        return this.defaultModels.includes(this.formatModelName(model));
+    }
+
+    async chat(messages, options = {}) {
+        throw new Error('HuggingChat direct API access is currently unavailable. Please use a different provider.');
+    }
+
+    async getModels() {
+        if (!this.enabled) return [];
+        return Array.from(this.models.values());
+    }
+}
+
+export default new HuggingChatProvider();
\ No newline at end of file
diff --git a/src/providers/index.js b/src/providers/index.js
new file mode 100644
index 0000000..15141ca
--- /dev/null
+++ b/src/providers/index.js
@@ -0,0 +1,353 @@
+// providers/index.js - manage all our lovely ai friends
+import logger from '../utils/logger.js';
+
+// Import all providers
+import VoidsProvider from './voids.js';
+import OpenRouterProvider from './openrouter.js';
+import GoogleProvider from './google.js';
+import GlamaProvider from './glama.js';
+import GroqProvider from './groq.js';
+import HuggingChatProvider from './huggingchat.js';
+import HackClubProvider from './hackclub.js';
+import OpenAIProvider from './openai.js';
+import ClaudeProvider from './claude.js';
+import DeepseekProvider from './deepseek.js';
+import DeepseekFreeProvider from './deepseek-free.js';
+import TogetherProvider from './together.js';
+import PerplexityProvider from './perplexity.js';
+import CerebrasProvider from './cerebras.js';
+import FireworksProvider from './fireworks.js';
+import MistralProvider from './mistral.js';
+import DeepInfraProvider from './deepinfra.js';
+import CopilotMoreProvider from './copilot-more.js';
+
+class ProviderManager {
+  constructor() {
+    this.providers = [];
+    this.disabledProviders = new Set();
+    this.providerErrors = new Map();
+    this.rateLimits = new Map();
+    this.retryAttempts = 3;
+    this.retryDelay = 1000;
+    
+    this.providerConfigs = [
+      { instance: VoidsProvider, envKey: 'VOIDS_API_KEY', name: 'VoidsProvider' },
+      { instance: OpenRouterProvider, envKey: 'OPENROUTER_API_KEY', name: 'OpenRouterProvider' },
+      { instance: GoogleProvider, envKey: 'GOOGLE_API_KEY', name: 'GoogleProvider' },
+      { instance: GlamaProvider, envKey: 'GLAMA_API_KEY', name: 'GlamaProvider' },
+      { instance: GroqProvider, envKey: 'GROQ_API_KEY', name: 'GroqProvider' },
+      { instance: HuggingChatProvider, envKey: 'HUGGINGFACE_EMAIL', name: 'HuggingChatProvider' },
+      { instance: HackClubProvider, name: 'HackClubProvider' },
+      { instance: OpenAIProvider, envKey: 'OPENAI_API_KEY', name: 'OpenAIProvider' },
+      { instance: ClaudeProvider, envKey: 'ANTHROPIC_API_KEY', name: 'ClaudeProvider' },
+      { instance: DeepseekProvider, envKey: 'DEEPSEEK_API_KEY', name: 'DeepseekProvider' },
+      { instance: DeepseekFreeProvider, envKey: 'DEEPSEEK_FREE_API_KEY', name: 'DeepseekFreeProvider' },
+      { instance: TogetherProvider, envKey: 'TOGETHER_API_KEY', name: 'TogetherProvider' },
+      { instance: PerplexityProvider, envKey: 'PERPLEXITY_API_KEY', name: 'PerplexityProvider' },
+      { instance: CerebrasProvider, envKey: 'CEREBRAS_API_KEY', name: 'CerebrasProvider' },
+      { instance: FireworksProvider, envKey: 'FIREWORKS_API_KEY', name: 'FireworksProvider' },
+      { instance: MistralProvider, envKey: 'MISTRAL_API_KEY', name: 'MistralProvider' },
+      { instance: DeepInfraProvider, envKey: 'DEEPINFRA_API_KEY', name: 'DeepInfraProvider' },
+      { instance: CopilotMoreProvider, name: 'CopilotMoreProvider' }
+    ];
+
+    this.initializeProviders();
+  }
+
+  async sleep(ms) {
+    return new Promise(resolve => setTimeout(resolve, ms));
+  }
+
+  initializeProviders() {
+    for (const config of this.providerConfigs) {
+      try {
+        // Check if provider is explicitly disabled by ENABLE flag
+        const enableFlag = `ENABLE_${config.name.replace('Provider', '').toUpperCase()}`;
+        const isExplicitlyDisabled = process.env[enableFlag] === 'false';
+        
+        if (isExplicitlyDisabled) {
+          logger.debug(`Provider disabled by config:`, {
+            provider: config.name
+          });
+          this.disabledProviders.add(config.name);
+          continue;
+        }
+
+        // Check if provider has required configuration
+        const hasConfig = !config.envKey || process.env[config.envKey];
+        if (hasConfig) {
+          let instance = config.instance;
+          
+          // If provider is a class (not an instance), instantiate it
+          if (typeof instance === 'function' || !instance.enabled) {
+            instance = new config.instance();
+          }
+
+          if (instance.enabled) {
+            this.providers.push(instance);
+            logger.debug(`Provider initialized:`, {
+              provider: config.name,
+              status: 'enabled'
+            });
+          } else {
+            this.disabledProviders.add(config.name);
+            logger.debug(`Provider initialized:`, {
+              provider: config.name,
+              status: 'disabled'
+            });
+          }
+        } else {
+          logger.debug(`Provider missing configuration:`, {
+            provider: config.name
+          });
+          this.disabledProviders.add(config.name);
+          // Only add error if provider isn't explicitly disabled
+          if (!isExplicitlyDisabled) {
+            this.providerErrors.set(config.name, {
+              error: 'Missing required configuration',
+              timestamp: Date.now()
+            });
+          }
+        }
+      } catch (error) {
+        logger.error(`Failed to initialize provider:`, {
+          provider: config.name,
+          error: error.message,
+          stack: error.stack
+        });
+        this.disabledProviders.add(config.name);
+        // Only add error if provider isn't explicitly disabled
+        const enableFlag = `ENABLE_${config.name.replace('Provider', '').toUpperCase()}`;
+        if (process.env[enableFlag] !== 'false') {
+          this.providerErrors.set(config.name, {
+            error: error.message,
+            timestamp: Date.now()
+          });
+        }
+      }
+    }
+
+    // Log summary if debug mode
+    logger.debug('Provider initialization complete:', {
+      enabled: this.providers.map(p => p.constructor.name),
+      disabled: Array.from(this.disabledProviders),
+      errors: Object.fromEntries(this.providerErrors),
+      total_enabled: this.providers.length,
+      total_disabled: this.disabledProviders.size
+    });
+  }
+
+  validateMessage(msg) {
+    if (!msg || typeof msg !== 'object') {
+      throw new Error('invalid message format');
+    }
+    if (!['user', 'assistant', 'system'].includes(msg.role)) {
+      throw new Error(`invalid message role: ${msg.role}`);
+    }
+    if (Array.isArray(msg.content)) {
+      msg.content.forEach(part => {
+        if (part.type === 'text' && typeof part.text !== 'string') {
+          throw new Error('invalid text content');
+        }
+        if (part.type === 'image_url' && (!part.image_url || typeof part.image_url.url !== 'string')) {
+          throw new Error('invalid image content');
+        }
+      });
+    } else if (typeof msg.content !== 'string') {
+      throw new Error('invalid message content');
+    }
+    return true;
+  }
+
+  validateOptions(options) {
+    const { temperature, max_tokens } = options;
+    if (temperature !== undefined && (typeof temperature !== 'number' || temperature < 0 || temperature > 2)) {
+      throw new Error('temperature must be between 0 and 2');
+    }
+    if (max_tokens !== undefined && (typeof max_tokens !== 'number' || max_tokens < 1)) {
+      throw new Error('max_tokens must be a positive number');
+    }
+    return true;
+  }
+
+  disableProvider(provider) {
+    const name = provider.constructor.name;
+    this.disabledProviders.add(name);
+    logger.debug(`Provider disabled:`, {
+      provider: name,
+      reason: 'error'
+    });
+    // Only add error if provider isn't explicitly disabled
+    const enableFlag = `ENABLE_${name.replace('Provider', '').toUpperCase()}`;
+    if (process.env[enableFlag] !== 'false') {
+      this.providerErrors.set(name, { 
+        error: 'provider disabled due to error', 
+        timestamp: Date.now() 
+      });
+    }
+    this.providers = this.providers.filter(p => p.constructor.name !== name);
+  }
+
+  isProviderEnabled(provider) {
+    return !this.disabledProviders.has(provider.constructor.name);
+  }
+
+  checkRateLimit(provider) {
+    if (process.env.DISABLE_RATE_LIMIT === 'true') return;
+    const now = Date.now();
+    const limit = this.rateLimits.get(provider.constructor.name);
+    let delay = limit?.delay || 1000;
+    if (process.env.ENABLE_HEALTH_CHECKS !== 'false') {
+      delay = parseInt(process.env.HEALTH_CHECK_DELAY) + 400;
+    }
+    if (limit && now - limit.timestamp < delay) {
+      throw new Error('rate limit exceeded');
+    }
+  }
+
+  updateRateLimit(provider, delay = 1000) {
+    this.rateLimits.set(provider.constructor.name, { timestamp: Date.now(), delay });
+  }
+
+  async getAvailableModels() {
+    const modelsByProvider = new Map();
+    const errors = [];
+
+    // Collect models from each provider and group by provider
+    for (const provider of this.providers) {
+      if (!this.isProviderEnabled(provider) || !provider.enabled) continue;
+      try {
+        this.checkRateLimit(provider);
+        const providerModels = await provider.getModels();
+        modelsByProvider.set(provider.constructor.name, providerModels);
+        this.updateRateLimit(provider);
+      } catch (error) {
+        logger.error(`Failed to get models from provider:`, {
+          provider: provider.constructor.name,
+          error: error.message,
+          status: error.status || null,
+          details: error.details || null
+        });
+        errors.push({ provider: provider.constructor.name, error: error.message });
+        
+        // Only disable provider on non-transient errors
+        if (error.status !== 429 && error.status !== 503) {
+          this.disableProvider(provider);
+        } else {
+          this.updateRateLimit(provider, 1000 * Math.pow(2, errors.length));
+        }
+      }
+    }
+
+    // Sort providers by model count (ascending) and provider name
+    const sortedProviders = Array.from(modelsByProvider.entries())
+      .sort((a, b) => {
+        const countDiff = a[1].length - b[1].length;
+        return countDiff !== 0 ? countDiff : a[0].localeCompare(b[0]);
+      });
+
+    // Flatten sorted models into single array
+    const models = sortedProviders.reduce((acc, [_, providerModels]) => {
+      acc.push(...providerModels);
+      return acc;
+    }, []);
+    if (models.length === 0) {
+      const message = errors.length > 0 ?
+        'Failed to get models from any provider: ' + errors.map(e => `${e.provider} (${e.error})`).join(', ') :
+        'No enabled providers available';
+      throw new Error(message);
+    }
+    return models;
+  }
+
+  async getProviderForModel(model) {
+    if (!model) throw new Error('model is required');
+    const errors = [];
+    for (const provider of this.providers) {
+      if (!this.isProviderEnabled(provider) || !provider.enabled) continue;
+      try {
+        this.checkRateLimit(provider);
+        if (await provider.canHandle(model)) return provider;
+        this.updateRateLimit(provider);
+      } catch (error) {
+        logger.error(`Provider error:`, {
+          provider: provider.constructor.name,
+          error: error.message,
+          status: error.status || null,
+          details: error.details || null
+        });
+        errors.push({ provider: provider.constructor.name, error: error.message });
+        if (error.status === 429 || error.status === 503) {
+          this.updateRateLimit(provider, 1000 * Math.pow(2, errors.length));
+        } else {
+          this.disableProvider(provider);
+        }
+      }
+    }
+    throw {
+      message: `No provider available for model: ${model}. Errors: ${errors.map(e => `${e.provider} (${e.error})`).join(', ')}`,
+      type: 'invalid_request_error', 
+      param: 'model', 
+      code: 'model_not_found'
+    };
+  }
+
+  async chat(model, messages, options = {}) {
+    if (!Array.isArray(messages) || messages.length === 0) throw new Error('messages must be a non-empty array');
+    messages.forEach(msg => this.validateMessage(msg));
+    this.validateOptions(options);
+    let attempt = 0;
+    let lastError = null;
+    while (attempt < this.retryAttempts) {
+      try {
+        const provider = await this.getProviderForModel(model);
+        this.checkRateLimit(provider);
+        if (options.stream && !provider.supportsStreaming) {
+          throw { message: `Streaming not supported by provider for model: ${model}`, type: 'invalid_request_error', param: 'stream', code: 'streaming_not_supported' };
+        }
+        const response = await provider.chat(messages, { ...options, model });
+        this.updateRateLimit(provider);
+        return response;
+      } catch (error) {
+        attempt++;
+        lastError = error;
+        
+        // Handle transient errors differently
+        if ((error.status === 429 || error.status === 503) && attempt < this.retryAttempts) {
+          const delay = this.retryDelay * Math.pow(2, attempt - 1);
+          await this.sleep(delay);
+          continue;
+        } else if (error.status >= 500 && attempt < this.retryAttempts) {
+          await this.sleep(this.retryDelay);
+          continue;
+        }
+        if (error.provider) {
+          // Only add error if provider isn't explicitly disabled
+          const enableFlag = `ENABLE_${error.provider.replace('Provider', '').toUpperCase()}`;
+          if (process.env[enableFlag] !== 'false') {
+            this.providerErrors.set(error.provider, { 
+              error: error.message, 
+              timestamp: Date.now() 
+            });
+          }
+          if (error.status === 401 || error.status === 403) {
+            this.disableProvider({ constructor: { name: error.provider }});
+          }
+        }
+        throw error;
+      }
+    }
+    throw new Error(`Max retry attempts exceeded. Last error: ${lastError.message}`);
+  }
+
+  getProviderStatus() {
+    return {
+      available: this.providers.filter(p => p.enabled && this.isProviderEnabled(p)).map(p => p.constructor.name),
+      disabled: Array.from(this.disabledProviders),
+      errors: Object.fromEntries(this.providerErrors),
+      rateLimits: Object.fromEntries(this.rateLimits)
+    };
+  }
+}
+
+export default new ProviderManager();
\ No newline at end of file
diff --git a/src/providers/mistral.js b/src/providers/mistral.js
new file mode 100644
index 0000000..dabf030
--- /dev/null
+++ b/src/providers/mistral.js
@@ -0,0 +1,239 @@
+// Mistral provider - implements Mistral's API via OpenAI compatibility
+import OpenAI from 'openai';
+
+class MistralProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_MISTRAL !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.MISTRAL_API_KEY) {
+      this.apiKeys.push({ key: process.env.MISTRAL_API_KEY, lastError: null });
+    }
+    if (process.env.MISTRAL_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.MISTRAL_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.MISTRAL_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.MISTRAL_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+
+    // Hardcode models as Mistral doesn't have a /models endpoint
+    this.mistralModels = [
+      { id: 'mistral-small-latest', context_length: 32000 },
+      { id: 'pixtral-12b-2409', context_length: 16000 },
+      { id: 'open-mistral-nemo', context_length: 128000 },
+      { id: 'open-codestral-mamba', context_length: 256000 }
+    ];
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    // Initialize models from hardcoded list
+    this.updateAvailableModels();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('Mistral initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.mistral.ai/v1'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('mistral/') ? modelId : `mistral/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^mistral\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled) return [];
+
+    this.models.clear();
+    this.mistralModels.forEach(model => {
+      const modelId = this.formatModelName(model.id);
+      if (!this.disabledModels.has(modelId)) {
+        this.models.set(modelId, {
+          id: modelId,
+          object: 'model',
+          created: Date.now(),
+          owned_by: 'https://mistral.ai',
+          permission: [],
+          root: model.id,
+          parent: null,
+          context_length: model.context_length,
+          capabilities: {
+            text: true,
+            images: false,
+            audio: false,
+            video: false
+          }
+        });
+      }
+    });
+
+    this.lastUpdate = Date.now();
+    return Array.from(this.models.keys());
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+    return this.models.has(this.formatModelName(model));
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('mistral provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+    if (!this.models.has(this.formatModelName(baseModel))) {
+      throw new Error(`unsupported model: ${baseModel}`);
+    }
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No Mistral API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('Mistral API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All Mistral API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new MistralProvider();
\ No newline at end of file
diff --git a/src/providers/openai.js b/src/providers/openai.js
new file mode 100644
index 0000000..82b7049
--- /dev/null
+++ b/src/providers/openai.js
@@ -0,0 +1,246 @@
+// openai.js provider - the OG that started it all
+import OpenAI from 'openai';
+
+class OpenAIProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_OPENAI !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.OPENAI_API_KEY) {
+      this.apiKeys.push({ key: process.env.OPENAI_API_KEY, lastError: null });
+    }
+    if (process.env.OPENAI_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.OPENAI_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.OPENAI_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.OPENAI_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+    
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('OpenAI initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.openai.com/v1'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('openai/') ? modelId : `openai/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^openai\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled) return [];
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      const response = await this.client.models.list();
+      this.models.clear();
+
+      for (const model of response.data) {
+        const formattedId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(formattedId)) {
+          this.models.set(formattedId, {
+            id: formattedId,
+            object: model.object,
+            created: model.created,
+            owned_by: model.owned_by,
+            permission: model.permission || [],
+            root: model.id,
+            parent: null,
+            context_length: model.context_length || 4096,
+            capabilities: {
+              text: true,
+              images: model.id.includes('vision'),
+              audio: false,
+              video: false
+            }
+          });
+        }
+      }
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      console.error('Failed to fetch OpenAI models:', error);
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('openai provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No OpenAI API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('OpenAI API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All OpenAI API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new OpenAIProvider();
\ No newline at end of file
diff --git a/src/providers/openrouter.js b/src/providers/openrouter.js
new file mode 100644
index 0000000..78b6c82
--- /dev/null
+++ b/src/providers/openrouter.js
@@ -0,0 +1,468 @@
+// openrouter.js provider - implements OpenRouter's streaming API
+import logger from '../utils/logger.js';
+
+// permanent blacklist for invalid/broken models
+const IGNORED_MODELS = new Set([
+    'quasar-alpha',
+    'google/gemini-2.5-pro-preview-03-25', // preview models that dont exist anymore
+    'all-hands/openhands-lm-32b-v0.1',     // broken model
+    'deepseek/deepseek-v3-base:free',      // consistently returns empty responses
+    'deepseek-ai/deepseek-coder-33b-instruct:free'  // unreliable responses
+]);
+
+class OpenRouterProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_OPENROUTER !== 'false';
+    this.baseUrl = 'https://openrouter.ai/api/v1';
+    this.supportsStreaming = true;
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 60 * 1000; // 5 hours in ms
+    this.disabledModels = new Set();
+    this.lastProviderError = null;
+    this.providerDisableTime = 15 * 60 * 1000; // 15 minutes
+
+    // Setup API keys
+    this.apiKeys = [];
+    if (process.env.OPENROUTER_API_KEY) {
+      this.apiKeys.push({ key: process.env.OPENROUTER_API_KEY, lastError: null });
+    }
+    if (process.env.OPENROUTER_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.OPENROUTER_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.OPENROUTER_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.OPENROUTER_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.enabled = this.enabled && this.apiKeys.length > 0;
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      logger.debug('OpenRouter initialized:', {
+        keys: this.apiKeys.map(k => '****' + k.key.slice(-4)).join(', ')
+      });
+    }
+  }
+
+  isProviderDisabled() {
+    return this.lastProviderError &&
+           (Date.now() - this.lastProviderError.timestamp < this.providerDisableTime);
+  }
+
+  getCurrentKey() {
+    // If provider is in temporary disable state, return null
+    if (this.isProviderDisabled()) {
+      throw this.lastProviderError.error;
+    }
+
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (!keyInfo.lastError || now - keyInfo.lastError >= this.keyRotationDelay) {
+        this.activeKeyIndex = keyIndex;
+        return keyInfo.key;
+      }
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const now = Date.now();
+    const currentKey = this.apiKeys[this.activeKeyIndex];
+    if (currentKey) {
+      currentKey.lastError = now;
+    }
+
+    // Count how many keys are currently in error state
+    const failedKeys = this.apiKeys.filter(k => k.lastError && (now - k.lastError < this.keyRotationDelay));
+    
+    // If all keys have failed, disable the provider temporarily
+    if (failedKeys.length === this.apiKeys.length) {
+      this.lastProviderError = {
+        timestamp: now,
+        error: error
+      };
+      throw error;
+    }
+    
+    // Try next key
+    this.activeKeyIndex = (this.activeKeyIndex + 1) % this.apiKeys.length;
+  }
+
+  formatModelName(modelId) {
+    // For display/storage, always add openrouter/ prefix
+    if (!modelId.startsWith('openrouter/')) {
+      return `openrouter/${modelId}`;
+    }
+    return modelId;
+  }
+
+  getBaseModelName(model) {
+    // For API requests, remove openrouter/ prefix
+    if (model.startsWith('openrouter/')) {
+      return model.substring('openrouter/'.length);
+    }
+    return model;
+  }
+
+  disableModel(model) {
+    // store the full prefixed name
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    logger.debug(`Model disabled due to quota:`, {
+      model: fullName,
+      reason: 'Quota exceeded'
+    });
+    this.models.delete(fullName);
+  }
+
+  getModelOwner(modelId) {  
+    return 'https://openrouter.ai';
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled) return [];
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      const apiKey = this.getCurrentKey();
+      if (!apiKey) {
+        throw new Error('No valid API keys available');
+      }
+
+      const response = await fetch(`${this.baseUrl}/models`, {
+        headers: {
+          'Authorization': `Bearer ${apiKey}`,
+          'Content-Type': 'application/json'
+        }
+      });
+
+      if (!response.ok) {
+        if (response.status === 401 || response.status === 403) {
+          this.enabled = false;
+          throw new Error('Invalid API key');
+        }
+        throw new Error(`Failed to fetch models: ${response.status} ${response.statusText}`);
+      }
+
+      const data = await response.json();
+      this.models.clear();
+
+      data.data.forEach(model => {
+        // Skip ignored models and paid models
+        if (IGNORED_MODELS.has(model.id)) return;
+        
+        // Only include models that are free ($0) or have ":free" in their name
+        const isFreeModel = (
+          (model.pricing?.prompt === 0 && model.pricing?.completion === 0) ||
+          model.id.includes(':free')
+        );
+        
+        if (!isFreeModel) {
+          return;
+        }
+
+        // Add openrouter/ prefix for storage/display
+        const modelId = this.formatModelName(model.id);
+        
+        if (!this.disabledModels.has(modelId)) {
+          this.models.set(modelId, {
+            id: modelId, // Store with prefix
+            object: 'model',
+            created: Date.now(),
+            owned_by: this.getModelOwner(model.id),
+            permission: [],
+            root: model.id, // Store original ID without prefix
+            parent: null,
+            context_length: model.context_length || 4096,
+            pricing: model.pricing || { prompt: 0, completion: 0 },
+            capabilities: {
+              text: true,
+              images: model.id.includes('vision')
+            }
+          });
+        }
+      });
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      logger.error('Failed to fetch OpenRouter models:', {
+        error: error.message,
+        status: error.status,
+        details: error.details || null
+      });
+      if (error.message.includes('Invalid API key')) {
+        this.enabled = false;
+      }
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+
+    // dont even try handling ignored models lmao
+    const baseModel = this.getBaseModelName(model);
+    if (IGNORED_MODELS.has(baseModel)) return false;
+
+    try {
+      // Check if model exists in our list (with prefix)
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(baseModel));
+    } catch (error) {
+      logger.error('Failed to check model availability:', {
+        error: error.message,
+        model: baseModel,
+        status: error.status || null
+      });
+      return false;
+    }
+  }
+
+  transformResponse(response, model) {
+    // Ensure we have a valid response with content
+    if (!response?.choices?.[0]?.message?.content &&
+        !response?.choices?.[0]?.delta?.content) {
+      throw new Error('Empty or invalid response from model');
+    }
+
+    // Add openrouter/ prefix to model name in response
+    if (response.model) {
+      response.model = this.formatModelName(response.model);
+    }
+
+    // Extract content based on response format
+    const content = response.choices[0].message?.content ||
+                   response.choices[0].delta?.content;
+
+    // Log full response details in debug mode
+    if (process.env.DEBUG_MODE === 'true') {
+      logger.debug(`Response from ${model}:`, {
+        content: content?.slice(0, 100) + (content?.length > 100 ? '...' : ''),
+        finish_reason: response.choices?.[0]?.finish_reason || null,
+        tokens: response.usage || null,
+        latency: response.latency || null
+      });
+    }
+
+    return {
+      ...response,
+      choices: [{
+        ...response.choices[0],
+        message: {
+          role: 'assistant',
+          content: content || ''
+        }
+      }]
+    };
+  }
+
+  async *handleStream(reader, model) {
+    const decoder = new TextDecoder();
+    let buffer = '';
+
+    try {
+      while (true) {
+        const { done, value } = await reader.read();
+        if (done) break;
+
+        // Append new chunk to buffer
+        buffer += decoder.decode(value, { stream: true });
+
+        // Process complete lines from buffer
+        while (true) {
+          const lineEnd = buffer.indexOf('\n');
+          if (lineEnd === -1) break;
+
+          const line = buffer.slice(0, lineEnd).trim();
+          buffer = buffer.slice(lineEnd + 1);
+
+          if (line.startsWith('data: ')) {
+            const data = line.slice(6);
+            if (data === '[DONE]') break;
+
+            try {
+              const parsed = JSON.parse(data);
+              const content = parsed.choices?.[0]?.delta?.content;
+              if (content) {
+                const chunk = {
+                  id: parsed.id || ('or-' + Math.random().toString(36).substring(2)),
+                  object: 'chat.completion.chunk',
+                  created: parsed.created || Math.floor(Date.now() / 1000),
+                  model: this.formatModelName(model), // Add prefix to model name
+                  choices: [{
+                    index: 0,
+                    delta: { content },
+                    finish_reason: parsed.choices?.[0]?.finish_reason
+                  }]
+                };
+                yield chunk;
+              }
+            } catch (e) {
+              logger.error('Failed to parse stream chunk:', {
+                error: e.message,
+                chunk: data,
+                model: model
+              });
+            }
+          }
+        }
+      }
+    } finally {
+      reader.cancel();
+    }
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('OpenRouter provider is not enabled');
+    }
+
+    const { model, stream, timeout, ...restOptions } = options;
+    if (!model) {
+      throw new Error('Model is required');
+    }
+    
+    // openrouter doesn't support timeout option
+
+    try {
+      // Remove openrouter/ prefix for API request
+      const baseModel = this.getBaseModelName(model);
+
+      // Clean up options and messages
+      const cleanOptions = restOptions;
+      const cleanMessages = messages.map(msg => ({
+        role: msg.role,
+        content: msg.content
+      }));
+
+      // Setup request headers
+      const headers = {
+        'Authorization': `Bearer ${this.getCurrentKey()}`,
+        'Content-Type': 'application/json'
+      };
+
+      // Add Accept header for streaming
+      if (stream) {
+        headers['Accept'] = 'text/event-stream';
+      }
+
+      // Make request to OpenRouter API
+      const response = await fetch(`${this.baseUrl}/chat/completions`, {
+        method: 'POST',
+        headers,
+        body: JSON.stringify({
+          model: baseModel, // Use unprefixed model name
+          messages: cleanMessages,
+          stream: Boolean(stream),
+          ...cleanOptions
+        })
+      });
+
+      if (!response.ok) {
+        let errorText = `${response.status} ${response.statusText}`;
+        try {
+          const errorData = await response.json();
+          errorText = errorData.error?.message || errorText;
+        } catch {}
+
+        const error = new Error(`OpenRouter API error: ${errorText}`);
+        error.status = response.status;
+
+        // Handle rate limits and auth errors by rotating keys
+        if (response.status === 429 || response.status === 402 || response.status === 401 || response.status === 403) {
+          this.handleKeyError(error);
+          return this.chat(messages, options); // Retry with next key
+        }
+
+        throw error;
+      }
+
+      if (stream) {
+        // Get the reader for streaming
+        const reader = response.body?.getReader();
+        if (!reader) {
+          throw new Error('Response body is not readable');
+        }
+        return this.handleStream(reader, baseModel);
+      }
+
+      // Handle regular response
+      const completion = await response.json();
+      
+      // Log raw response for debugging
+      if (process.env.DEBUG_MODE === 'true') {
+        logger.debug(`Raw response from ${baseModel}:`, {
+          id: completion.id,
+          model: completion.model,
+          finish_reason: completion.choices?.[0]?.finish_reason,
+          content: completion.choices?.[0]?.message?.content?.slice(0, 100) +
+                  (completion.choices?.[0]?.message?.content?.length > 100 ? '...' : '') || 'null',
+          usage: completion.usage || null
+        });
+      }
+
+      try {
+        return this.transformResponse(completion, baseModel);
+      } catch (parseError) {
+        throw new Error(`Failed to parse model response: ${parseError.message}\nRaw response: ${JSON.stringify(completion)}`);
+      }
+    } catch (error) {
+      if (process.env.DEBUG_MODE === 'true') {
+        logger.error('OpenRouter API error:', {
+          error: error.message,
+          status: error.status,
+          model: baseModel,
+          details: error.details || null,
+          response: error.response || null
+        });
+      }
+
+      // Handle errors not already caught by response.ok check
+      if (error.status === 429 || error.status === 402 || error.status === 401 || error.status === 403) {
+        const currentKey = this.apiKeys[this.activeKeyIndex];
+        if (currentKey) {
+          currentKey.lastError = Date.now();
+        }
+
+        const failedKeys = this.apiKeys.filter(k => k.lastError &&
+          (Date.now() - k.lastError < this.keyRotationDelay));
+
+        if (failedKeys.length === this.apiKeys.length) {
+          this.lastProviderError = {
+            timestamp: Date.now(),
+            error: error
+          };
+        } else {
+          this.activeKeyIndex = (this.activeKeyIndex + 1) % this.apiKeys.length;
+          if (this.getCurrentKey()) {
+            return this.chat(messages, options); // Retry with next key
+          }
+        }
+      }
+
+      if (error.status === 402) {
+        this.disableModel(model);
+      }
+
+      throw error;
+    }
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+}
+
+export default new OpenRouterProvider();
\ No newline at end of file
diff --git a/src/providers/perplexity.js b/src/providers/perplexity.js
new file mode 100644
index 0000000..662ee4b
--- /dev/null
+++ b/src/providers/perplexity.js
@@ -0,0 +1,241 @@
+// Perplexity provider - implements Perplexity's API via OpenAI compatibility
+import OpenAI from 'openai';
+
+class PerplexityProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_PERPLEXITY !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.PERPLEXITY_API_KEY) {
+      this.apiKeys.push({ key: process.env.PERPLEXITY_API_KEY, lastError: null });
+    }
+    if (process.env.PERPLEXITY_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.PERPLEXITY_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.PERPLEXITY_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.PERPLEXITY_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+
+    // Hardcode models as Perplexity doesn't have a /models endpoint
+    this.perplexityModels = [
+      { id: 'sonar-deep-research', context_length: 128000 },
+      { id: 'sonar-reasoning-pro', context_length: 128000 },
+      { id: 'sonar-reasoning', context_length: 128000 },
+      { id: 'sonar-pro', context_length: 200000 },
+      { id: 'sonar', context_length: 128000 },
+      { id: 'r1-1776', context_length: 128000 }
+    ];
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    // Initialize models from hardcoded list
+    this.updateAvailableModels();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('Perplexity initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.perplexity.ai'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('perplexity/') ? modelId : `perplexity/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^perplexity\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled) return [];
+
+    this.models.clear();
+    this.perplexityModels.forEach(model => {
+      const modelId = this.formatModelName(model.id);
+      if (!this.disabledModels.has(modelId)) {
+        this.models.set(modelId, {
+          id: modelId,
+          object: 'model',
+          created: Date.now(),
+          owned_by: 'https://perplexity.ai',
+          permission: [],
+          root: model.id,
+          parent: null,
+          context_length: model.context_length,
+          capabilities: { 
+            text: true,
+            images: false,
+            audio: false,
+            video: false
+          }
+        });
+      }
+    });
+
+    this.lastUpdate = Date.now();
+    return Array.from(this.models.keys());
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+    return this.models.has(this.formatModelName(model));
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('perplexity provider not enabled');
+    }
+
+    if (!Array.isArray(messages) || messages.length === 0) {
+      throw new Error('messages must be a non-empty array');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+    if (!this.models.has(this.formatModelName(baseModel))) {
+      throw new Error(`unsupported model: ${baseModel}`);
+    }
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No Perplexity API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('Perplexity API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All Perplexity API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new PerplexityProvider();
\ No newline at end of file
diff --git a/src/providers/together.js b/src/providers/together.js
new file mode 100644
index 0000000..e67f283
--- /dev/null
+++ b/src/providers/together.js
@@ -0,0 +1,266 @@
+// Together.ai provider - implements Together's API via OpenAI compatibility
+import OpenAI from 'openai';
+
+class TogetherProvider {
+  constructor() {
+    this.enabled = process.env.ENABLE_TOGETHER !== 'false';
+    
+    // setup api keys
+    this.apiKeys = [];
+    if (process.env.TOGETHER_API_KEY) {
+      this.apiKeys.push({ key: process.env.TOGETHER_API_KEY, lastError: null });
+    }
+    if (process.env.TOGETHER_BACKUP_KEY_1) {
+      this.apiKeys.push({ key: process.env.TOGETHER_BACKUP_KEY_1, lastError: null });
+    }
+    if (process.env.TOGETHER_BACKUP_KEY_2) {
+      this.apiKeys.push({ key: process.env.TOGETHER_BACKUP_KEY_2, lastError: null });
+    }
+
+    this.activeKeyIndex = 0;
+    this.keyRotationDelay = 60000; // 1 minute cooldown for failed keys
+
+    this.supportsStreaming = true;
+    this.disabledModels = new Set();
+    this.models = new Map();
+    this.lastUpdate = 0;
+    this.updateInterval = 5 * 60 * 1000; // 5 minutes
+
+    // Known problematic models
+    this.unreliableModels = new Set([
+      'togethercomputer/MoA-1',
+      'togethercomputer/MoA-1-Turbo'
+    ]);
+
+    // Create OpenAI client for active key
+    this.updateClient();
+
+    if (this.enabled && process.env.DEBUG_MODE === 'true') {
+      console.log('Together.ai initialized with:', {
+        primaryKey: this.apiKeys[0]?.key ? '****' + this.apiKeys[0].key.slice(-4) : 'none',
+        backupKey1: this.apiKeys[1]?.key ? '****' + this.apiKeys[1].key.slice(-4) : 'none',
+        backupKey2: this.apiKeys[2]?.key ? '****' + this.apiKeys[2].key.slice(-4) : 'none'
+      });
+    }
+  }
+
+  updateClient() {
+    const currentKey = this.getActiveKey();
+    if (currentKey) {
+      this.client = new OpenAI({
+        apiKey: currentKey,
+        baseURL: 'https://api.together.xyz/v1'
+      });
+    } else {
+      this.client = null;
+    }
+  }
+
+  getActiveKey() {
+    const now = Date.now();
+    for (let i = 0; i < this.apiKeys.length; i++) {
+      const keyIndex = (this.activeKeyIndex + i) % this.apiKeys.length;
+      const keyInfo = this.apiKeys[keyIndex];
+      
+      if (keyInfo.lastError && now - keyInfo.lastError < this.keyRotationDelay) {
+        continue;
+      }
+      
+      this.activeKeyIndex = keyIndex;
+      return keyInfo.key;
+    }
+    return null;
+  }
+
+  handleKeyError(error) {
+    const keyInfo = this.apiKeys[this.activeKeyIndex];
+    if (keyInfo) {
+      if (error.status === 401 || error.status === 403) {
+        keyInfo.lastError = Infinity;
+      } else if (error.status === 429) {
+        keyInfo.lastError = Date.now();
+      }
+    }
+    this.updateClient();
+  }
+
+  formatModelName(modelId) {
+    return modelId.startsWith('together/') ? modelId : `together/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    return model.replace(/^together\//, '');
+  }
+
+  async updateAvailableModels() {
+    if (!this.enabled || !this.client) return [];
+
+    const now = Date.now();
+    if (now - this.lastUpdate < this.updateInterval && this.models.size > 0) {
+      return Array.from(this.models.keys());
+    }
+
+    try {
+      const response = await this.client.models.list();
+      this.models.clear();
+
+      // Filter for free models only and exclude unreliable ones
+      response.data.filter(model => 
+        model.pricing && 
+        model.pricing.input === 0 && 
+        model.pricing.output === 0 &&
+        model.pricing.hourly === 0 &&
+        model.type === 'chat' &&
+        !this.unreliableModels.has(model.id)
+      ).forEach(model => {
+        const modelId = this.formatModelName(model.id);
+        if (!this.disabledModels.has(modelId)) {
+          this.models.set(modelId, {
+            id: modelId,
+            object: 'model',
+            created: model.created || Date.now(),
+            owned_by: model.link || model.organization || 'https://api.together.xyz',
+            permission: [],
+            root: model.id,
+            parent: null,
+            context_length: model.context_length || model.config?.context_length || 4096,
+            capabilities: {
+              text: true,
+              images: model.id.toLowerCase().includes('vision')
+            }
+          });
+        }
+      });
+
+      this.lastUpdate = now;
+      return Array.from(this.models.keys());
+    } catch (error) {
+      console.error('Failed to fetch Together.ai models:', error);
+      return Array.from(this.models.keys());
+    }
+  }
+
+  async getModels() {
+    if (!this.enabled) return [];
+    
+    await this.updateAvailableModels();
+    return Array.from(this.models.values());
+  }
+
+  async canHandle(model) {
+    if (!this.enabled) return false;
+
+    try {
+      const models = await this.updateAvailableModels();
+      return models.includes(this.formatModelName(model));
+    } catch {
+      return false;
+    }
+  }
+
+  disableModel(model) {
+    const fullName = this.formatModelName(model);
+    this.disabledModels.add(fullName);
+    console.error(`model disabled due to error: ${fullName}`);
+  }
+
+  async chat(messages, options = {}) {
+    if (!this.enabled) {
+      throw new Error('together.ai provider not enabled');
+    }
+
+    const { model, stream, temperature, max_tokens, ...otherOptions } = options;
+    
+    if (!model) {
+      throw new Error('model is required');
+    }
+
+    const baseModel = this.getBaseModelName(model);
+
+    // Skip unreliable models
+    if (this.unreliableModels.has(baseModel)) {
+      throw new Error(`Model ${model} is currently unreliable and has been disabled`);
+    }
+
+    // Try with all available keys
+    for (let attempt = 0; attempt < this.apiKeys.length; attempt++) {
+      if (!this.client) {
+        throw new Error('No Together.ai API keys available');
+      }
+
+      try {
+        const completion = await this.client.chat.completions.create({
+          model: baseModel,
+          messages,
+          stream,
+          temperature,
+          max_tokens,
+          ...otherOptions
+        });
+
+        // Handle streaming
+        if (stream) {
+          return this._handleStream(completion, model);
+        }
+
+        // Add our model prefix to the response
+        if (completion.model) {
+          completion.model = this.formatModelName(completion.model);
+        }
+        return completion;
+
+      } catch (error) {
+        if (process.env.DEBUG_MODE === 'true') {
+          console.error('Together.ai API error:', error);
+        }
+
+        this.handleKeyError(error);
+
+        // Check if error indicates server issues
+        if (error.status === 500 || 
+            (error.message && error.message.includes('Internal server error'))) {
+          this.disableModel(model);
+          throw new Error(`Model ${model} is experiencing server errors and has been disabled`);
+        }
+
+        // Try next key if rate limited
+        if (error.status === 429 && attempt < this.apiKeys.length - 1) {
+          if (process.env.DEBUG_MODE === 'true') {
+            console.log(`${this.constructor.name}: Retrying with backup key`);
+          }
+          continue;
+        }
+
+        throw error;
+      }
+    }
+
+    throw new Error('All Together.ai API keys failed or were rate limited');
+  }
+
+  async *_handleStream(stream, model) {
+    try {
+      for await (const chunk of stream) {
+        if (!chunk || !chunk.choices?.[0]?.delta) continue;
+        
+        // Add our model prefix
+        if (chunk.model) {
+          chunk.model = this.formatModelName(chunk.model);
+        }
+        yield chunk;
+      }
+    } catch (error) {
+      if (error.status === 401 || error.status === 403) {
+        this.handleKeyError(error);
+        throw new Error('unauthorized: check your api key');
+      }
+      if (error.status === 429) {
+        this.handleKeyError(error);
+        throw new Error('rate limit exceeded');
+      }
+      throw error;
+    }
+  }
+}
+
+export default new TogetherProvider();
\ No newline at end of file
diff --git a/src/providers/voids.js b/src/providers/voids.js
new file mode 100644
index 0000000..630cc26
--- /dev/null
+++ b/src/providers/voids.js
@@ -0,0 +1,207 @@
+// voids.js provider - the free one we all love
+class VoidsProvider {
+  constructor() {
+    this.baseUrl = 'https://api.voids.top/v1';
+    this.supportsStreaming = true;
+    this.lastRequest = 0;
+    this.requestDelay = 1000; // 1 second between requests
+    this.models = new Map();
+    this.lastModelUpdate = 0;
+    this.modelUpdateInterval = 5 * 60 * 1000; // 5 minutes
+  }
+
+  formatModelName(modelId) {
+    // if it already has our prefix, use it as-is
+    if (modelId.startsWith('voids/')) {
+      return modelId;
+    }
+    return `voids/${modelId}`;
+  }
+
+  getBaseModelName(model) {
+    // if it starts with our prefix, remove it
+    if (model.startsWith('voids/')) {
+      return model.replace('voids/', '');
+    }
+    // if it's not our model, return as-is
+    return model;
+  }
+
+  async throttleRequest() {
+    const now = Date.now();
+    const timeSinceLastRequest = now - this.lastRequest;
+    
+    if (timeSinceLastRequest < this.requestDelay) {
+      await new Promise(resolve => 
+        setTimeout(resolve, this.requestDelay - timeSinceLastRequest)
+      );
+    }
+    
+    this.lastRequest = Date.now();
+  }
+
+  async makeRequest(url, options = {}) {
+    await this.throttleRequest();
+
+    try {
+      const response = await fetch(url, {
+        ...options,
+        headers: {
+          'Content-Type': 'application/json',
+          ...options.headers
+        }
+      });
+
+      // Check for rate limiting
+      if (response.status === 429) {
+        const retryAfter = parseInt(response.headers.get('Retry-After')) || 5;
+        this.requestDelay = Math.max(this.requestDelay, retryAfter * 1000);
+        throw new Error(`Rate limited. Please wait ${retryAfter} seconds.`);
+      }
+
+      // Check for other errors
+      if (!response.ok) {
+        throw new Error(`HTTP error! status: ${response.status}`);
+      }
+
+      const text = await response.text();
+      try {
+        return JSON.parse(text);
+      } catch (e) {
+        console.error('Raw response:', text);
+        throw new Error(`Invalid JSON response: ${e.message}`);
+      }
+    } catch (error) {
+      console.error('Request failed:', error);
+      throw error;
+    }
+  }
+
+  async canHandle(model) {
+    const baseModel = this.getBaseModelName(model);
+    const models = await this.getModels();
+    return models.some(m => m.id === this.formatModelName(baseModel));
+  }
+
+  transformResponse(response, model) {
+    if (response && typeof response === 'object') {
+      if (response.model) {
+        response.model = this.formatModelName(model);
+      }
+      if (response.choices && Array.isArray(response.choices)) {
+        response.choices.forEach(choice => {
+          if (choice.model) {
+            choice.model = this.formatModelName(model);
+          }
+        });
+      }
+    }
+    return response;
+  }
+
+  async chat(messages, options = {}) {
+    const { model, stream } = options;
+    const baseModel = this.getBaseModelName(model);
+
+    try {
+      const response = await this.makeRequest(`${this.baseUrl}/chat/completions`, {
+        method: 'POST',
+        body: JSON.stringify({
+          messages,
+          model: baseModel,
+          stream,
+          ...options
+        })
+      });
+
+      if (stream) {
+        return this._handleStream(response, model);
+      }
+
+      return this.transformResponse(response, model);
+    } catch (error) {
+      if (error.message.includes('Rate limited')) {
+        // Wait and retry once
+        await new Promise(resolve => setTimeout(resolve, this.requestDelay));
+        return this.chat(messages, options);
+      }
+      throw error;
+    }
+  }
+
+  async *_handleStream(response, model) {
+    const reader = response.body.getReader();
+    const decoder = new TextDecoder();
+    let buffer = '';
+
+    try {
+      while (true) {
+        const { done, value } = await reader.read();
+        if (done) break;
+
+        buffer += decoder.decode(value, { stream: true });
+        const lines = buffer.split('\n');
+        buffer = lines.pop() || '';
+
+        for (const line of lines) {
+          if (line.trim() === '') continue;
+          if (line === 'data: [DONE]') return;
+
+          try {
+            const data = JSON.parse(line.slice(6)); // remove 'data: '
+            yield this.transformResponse(data, model);
+          } catch (e) {
+            console.error('Streaming parse error:', e);
+          }
+        }
+      }
+    } catch (error) {
+      console.error('Stream error:', error);
+      throw error;
+    } finally {
+      reader.releaseLock();
+    }
+  }
+
+  async getModels() {
+    const now = Date.now();
+    if (now - this.lastModelUpdate < this.modelUpdateInterval && this.models.size > 0) {
+      return Array.from(this.models.values());
+    }
+
+    try {
+      const data = await this.makeRequest(`${this.baseUrl}/models`);
+      
+      if (!data || !data.data || !Array.isArray(data.data)) {
+        throw new Error('Invalid model data format');
+      }
+
+      this.models = new Map(data.data.map(model => [
+        this.formatModelName(model.id),
+        {
+          id: this.formatModelName(model.id),
+          object: 'model',
+          created: Date.now(),
+          owned_by: 'https://voids.top',
+          permission: [],
+          root: model.id,
+          parent: null,
+          context_length: model.context_length || 8192,
+          capabilities: {
+            text: true,
+            images: model.id.includes('vision')
+          }
+        }
+      ]));
+
+      this.lastModelUpdate = now;
+      return Array.from(this.models.values());
+    } catch (error) {
+      console.error('Model fetch failed:', error);
+      // Return cached models if available, otherwise empty array
+      return Array.from(this.models.values());
+    }
+  }
+}
+
+export default VoidsProvider;
\ No newline at end of file
diff --git a/src/server.js b/src/server.js
new file mode 100644
index 0000000..cdfb15c
--- /dev/null
+++ b/src/server.js
@@ -0,0 +1,346 @@
+// server.js
+import express from 'express';
+import cors from 'cors';
+import rateLimit from 'express-rate-limit';
+import morgan from 'morgan';
+import path from 'path';
+import { fileURLToPath } from 'url';
+import 'dotenv/config';
+
+import providers from './providers/index.js';
+import logger from './utils/logger.js';
+import protectRoute from './middleware/protect.js';
+import health from './utils/health.js';
+import maintenanceMiddleware from './middleware/maintenance.js';
+
+const __dirname = path.dirname(fileURLToPath(import.meta.url));
+const port = process.env.PORT || 3000;
+
+// Initialize express
+const app = express();
+
+// Apply maintenance middleware first
+app.use(maintenanceMiddleware);
+
+// Basic middleware
+app.use(express.json({ limit: process.env.MAX_PAYLOAD_SIZE || '10mb' }));
+app.use(express.urlencoded({ extended: true, limit: process.env.MAX_REQUEST_SIZE || '10mb' }));
+
+// CORS
+app.use(cors({
+    origin: process.env.ALLOWED_ORIGINS?.split(',') || '*'
+}));
+
+// Request logging
+if (process.env.ENABLE_LOGGING !== 'false') {
+    morgan.token('log-date', () => new Date().toISOString());
+    app.use(morgan((tokens, req, res) => {
+        const log = {
+            method: tokens.method(req, res),
+            url: tokens.url(req, res),
+            status: tokens.status(req, res),
+            responseTime: tokens['response-time'](req, res) + 'ms'
+        };
+        
+        if (req.path !== '/health') { // Don't log health checks
+            logger.debug('API Request', log);
+        }
+        
+        return process.env.DEBUG_MODE === 'true' ? 
+            `${log.method} ${log.url} ${log.status} ${log.responseTime}` : 
+            null;
+    }));
+}
+
+// Rate limiting
+if (process.env.DISABLE_RATE_LIMIT !== 'true') {
+    const limiter = rateLimit({
+        windowMs: parseInt(process.env.RATE_LIMIT_WINDOW) || 60000,
+        max: parseInt(process.env.RATE_LIMIT_REQUESTS) || 60,
+        // skip rate limiting for local ips cuz like why would you even rate limit yourself lmao
+        skip: (req) => {
+            const ip = req.ip;
+            return ip === '127.0.0.1' || ip === '::1' || ip === 'localhost';
+        }
+    });
+    app.use(limiter);
+    logger.info('Rate limiting enabled (local IPs exempt)', {
+        window: `${(parseInt(process.env.RATE_LIMIT_WINDOW) || 60000) / 1000}s`,
+        max: parseInt(process.env.RATE_LIMIT_REQUESTS) || 60
+    });
+}
+
+// Config endpoint
+app.get('/v1/config', (req, res) => {
+    const publicConfig = {
+        require_api_key: process.env.REQUIRE_API_KEY === 'true',
+        rate_limiting: {
+            enabled: process.env.DISABLE_RATE_LIMIT !== 'true',
+            requests: parseInt(process.env.RATE_LIMIT_REQUESTS) || 60,
+            window_ms: parseInt(process.env.RATE_LIMIT_WINDOW) || 60000
+        },
+        features: {
+            health_checks: process.env.ENABLE_HEALTH_CHECKS !== 'false',
+            logging: process.env.ENABLE_LOGGING !== 'false'
+        },
+        limits: {
+            max_tokens: process.env.MAX_TOKENS || 'infinite',
+            max_request_size: process.env.MAX_REQUEST_SIZE || '10mb',
+            max_concurrent: parseInt(process.env.MAX_CONCURRENT) || 50,
+            request_timeout: parseInt(process.env.REQUEST_TIMEOUT) || 120000
+        },
+        defaults: {
+            model: process.env.DEFAULT_MODEL || false,
+            temperature: parseFloat(process.env.DEFAULT_TEMPERATURE) || 0.7
+        },
+        providers: providers.getProviderStatus()
+    };
+    res.json(publicConfig);
+});
+
+// Routes
+app.get('/v1', (req, res) => {
+    res.redirect('/docs');
+});
+
+app.get('/docs', (req, res) => {
+    res.sendFile(path.join(__dirname, '../public/docs.html'));
+});
+
+app.get('/models', (req, res) => {
+    res.sendFile(path.join(__dirname, '../public/models.html'));
+});
+
+// Static files
+app.use(express.static('public', {
+    extensions: ['html']
+}));
+
+// API routes
+app.get('/v1/models', async (req, res) => {
+    try {
+        const models = await providers.getAvailableModels();
+        
+        // Add health status to models
+        const healthStatus = health.getStatus();
+        const healthMap = new Map(healthStatus.map(h => [h.model, h]));
+
+        // Group models by provider and count them
+        // Count models per provider using owned_by field
+        const providerCounts = new Map();
+        models.forEach(model => {
+            const provider = model.owned_by || 'Unknown';
+            providerCounts.set(provider, (providerCounts.get(provider) || 0) + 1);
+        });
+// Sort models by provider's model count (ascending) and then provider name
+const sortedModels = [...models].sort((a, b) => {
+    const aCount = providerCounts.get(a.owned_by) || 0;
+    const bCount = providerCounts.get(b.owned_by) || 0;
+    
+    // First sort by model count (ascending)
+    if (aCount !== bCount) {
+        return aCount - bCount;
+    }
+    
+    // Then sort alphabetically by provider name for equal counts
+    const aProvider = a.owned_by || 'Unknown';
+    const bProvider = b.owned_by || 'Unknown';
+    return aProvider.localeCompare(bProvider);
+});
+
+
+        const modelsWithHealth = sortedModels.map(model => {
+            const healthData = healthMap.get(model.id) || {
+                status: 'unknown',
+                latency: null,
+                lastCheck: null
+            };
+            return {
+                ...model,
+                health: {
+                    status: healthData.status,
+                    latency: healthData.latency,
+                    lastCheck: healthData.lastCheck
+                }
+            };
+        });
+
+        res.json({
+            object: 'list',
+            data: modelsWithHealth
+        });
+    } catch (error) {
+        logger.error('Error getting models:', error);
+        res.status(500).json({
+            error: {
+                message: error.message,
+                type: 'server_error'
+            }
+        });
+    }
+});
+
+app.post('/v1/chat/completions', protectRoute, async (req, res) => {
+    try {
+        const { model, messages, stream, ...options } = req.body;
+
+        if (!model) {
+            return res.status(400).json({
+                error: {
+                    message: 'model is required',
+                    type: 'invalid_request_error',
+                    param: 'model',
+                    code: 'model_required' 
+                }
+            });
+        }
+
+        if (!Array.isArray(messages) || messages.length === 0) {
+            return res.status(400).json({
+                error: {
+                    message: 'messages must be a non-empty array',
+                    type: 'invalid_request_error',
+                    param: 'messages',
+                    code: 'invalid_messages'
+                }
+            });
+        }
+
+        const requestStartTime = Date.now();
+        const response = await providers.chat(model, messages, { stream, ...options });
+        const latency = Date.now() - requestStartTime;
+
+        if (stream) {
+            res.setHeader('Content-Type', 'text/event-stream');
+            res.setHeader('Cache-Control', 'no-cache');
+            res.setHeader('Connection', 'keep-alive');
+
+            for await (const chunk of response) {
+                res.write(`data: ${JSON.stringify(chunk)}\n\n`);
+            }
+            res.write('data: [DONE]\n\n');
+            res.end();
+        } else {
+            res.json(response);
+        }
+
+    } catch (error) {
+        logger.error('Chat error:', error);
+        
+        const status = error.status || 500;
+        const errorResponse = {
+            error: {
+                message: error.message,
+                type: error.type || 'server_error',
+                param: error.param,
+                code: error.code
+            }
+        };
+
+        res.status(status).json(errorResponse);
+    }
+});
+
+// Health check endpoint
+app.get('/health', (req, res) => {
+    const providerStatus = providers.getProviderStatus();
+    const modelHealthStatus = health.getStatus();
+    
+    res.json({ 
+        status: 'ok',
+        providers: providerStatus,
+        models: modelHealthStatus
+    });
+});
+
+// 404 handler
+app.use((req, res, next) => {
+    // Only handle HTML requests with 404, let API requests go to error handler
+    if (req.accepts('html')) {
+        res.status(404).sendFile(path.join(__dirname, '../public/404.html'));
+    } else {
+        next();
+    }
+});
+
+// Error handler
+app.use((err, req, res, next) => {
+    logger.error('Unhandled error:', err);
+    res.status(500).json({
+        error: {
+            message: 'Internal server error',
+            type: 'server_error'
+        }
+    });
+});
+
+// Start server
+async function startServer() {
+    try {
+        logger.startup('Server');
+
+        // Start HTTP server
+        const server = app.listen(port, () => {
+            logger.info(`Server running on port ${port}`);
+        });
+
+        // Check maintenance mode
+        if (process.env.MAINTENANCE_MODE === 'true') {
+            logger.warn('Server started in maintenance mode', {
+                message: process.env.MAINTENANCE_MESSAGE || 'Service is temporarily under maintenance.'
+            });
+            return;
+        }
+
+        // Initialize providers
+        const status = providers.getProviderStatus();
+        logger.debug(`Providers initialized`, {
+            available: status.available,
+            total: status.available.length
+        });
+
+        if (status.errors && Object.keys(status.errors).length > 0) {
+            logger.warn('Provider initialization errors:', status.errors);
+        }
+
+        // Start health checks
+        if (process.env.ENABLE_HEALTH_CHECKS !== 'false') {
+            const interval = parseInt(process.env.HEALTH_CHECK_INTERVAL) * 1000 || 7200000;
+            
+            health.startHealthChecks((model, result) => {
+                const status = result.status === 'operational' ? 'healthy' : 'unhealthy';
+                logger.debug(`[Health] Model ${model} ${status}:`, {
+                    latency: result.latency,
+                    error: result.error,
+                    attempts: result.attempts,
+                    response: result.response?.slice(0, 100) + (result.response?.length > 100 ? '...' : '') || 'null'
+                });
+            }, interval);
+
+            logger.debug('Health checks initialized', {
+                interval: `${interval/1000}s`,
+                delay: `${parseInt(process.env.HEALTH_CHECK_DELAY) || 2600}ms`
+            });
+        }
+
+        // Cleanup function
+        function cleanup() {
+            logger.debug('Shutting down...');
+            health.stopHealthChecks();
+            server.close(() => {
+                logger.info('Server closed');
+                setTimeout(() => process.exit(0), 100);
+            });
+        }
+
+        // Handle graceful shutdown
+        process.on('SIGTERM', cleanup);
+        process.on('SIGINT', cleanup);
+
+    } catch (error) {
+        logger.error('Failed to start server:', error);
+        process.exit(1);
+    }
+}
+
+startServer();
\ No newline at end of file
diff --git a/src/utils/bootstrap.js b/src/utils/bootstrap.js
new file mode 100644
index 0000000..f1bca82
--- /dev/null
+++ b/src/utils/bootstrap.js
@@ -0,0 +1,433 @@
+// bootstrap.js - initialize everything with sane defaults
+import path from 'path';
+import fs from 'fs';
+import { fileURLToPath } from 'url';
+
+const __filename = fileURLToPath(import.meta.url);
+const __dirname = path.dirname(__filename);
+
+// track any configuration warnings
+const warningMessages = [];
+let warnings = 0;
+
+function warnConfig(message) {
+  warningMessages.push(message);
+  warnings++;
+  console.warn('âš ï¸  WARNING:', message);
+}
+
+function debugConfig(name, value) {
+  if (process.env.DEBUG_MODE === 'true') {
+    const maskedValue = value ? '****' + value.slice(-4) : 'undefined';
+    console.log(`ðŸ” DEBUG: ${name} = ${maskedValue}`);
+  }
+}
+
+function validateApiKey(key, name) {
+  if (!key) return false;
+  key = key.trim();
+  if (key === '""' || key === "''") return false;
+  debugConfig(name, key);
+  return key.length > 0;
+}
+
+function validateApiKeySet(primaryKey, backupKey1, backupKey2, providerName) {
+  const validKeys = [];
+  if (validateApiKey(primaryKey, `${providerName}_API_KEY`)) {
+    validKeys.push(primaryKey.trim());
+  }
+  if (validateApiKey(backupKey1, `${providerName}_BACKUP_KEY_1`)) {
+    validKeys.push(backupKey1.trim());
+  }
+  if (validateApiKey(backupKey2, `${providerName}_BACKUP_KEY_2`)) {
+    validKeys.push(backupKey2.trim());
+  }
+  return validKeys;
+}
+
+function getDefaultRateLimit() {
+  return {
+    window: 60000,    // 1 minute in ms
+    max: 50,          // 50 requests per minute
+    message: 'Too many requests, please try again later'
+  };
+}
+
+function getSizeLimit(value, defaultValue) {
+  if (!value) return defaultValue;
+  if (typeof value === 'number') return value;
+  
+  const match = value.match(/^(\d+)(mb|kb|b)?$/i);
+  if (!match) return defaultValue;
+
+  const num = parseInt(match[1]);
+  const unit = (match[2] || 'b').toLowerCase();
+
+  switch (unit) {
+    case 'mb': return num * 1024 * 1024;
+    case 'kb': return num * 1024;
+    case 'b': return num;
+    default: return defaultValue;
+  }
+}
+
+function readApiKeysFromFile(filePath) {
+  try {
+    const content = fs.readFileSync(filePath, 'utf8');
+    return content.split('\n')
+      .map(line => line.trim())
+      .filter(line => line && !line.startsWith('#')); // skip empty lines and comments
+  } catch (error) {
+    warnConfig(`failed to read api keys file ${filePath}: ${error.message}`);
+    return [];
+  }
+}
+
+function validateEnvironment() {
+  console.log('ðŸ”„ Validating environment configuration...');
+  
+  const config = {
+    // Authentication
+    apiKey: process.env.API_KEY,
+    apiKeys: [],
+    adminKey: process.env.ADMIN_KEY,
+    requireApiKey: process.env.REQUIRE_API_KEY === 'true',
+
+    // Provider Configuration
+    providers: {
+      openrouter: {
+        enabled: process.env.ENABLE_OPENROUTER !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.OPENROUTER_API_KEY,
+          process.env.OPENROUTER_BACKUP_KEY_1,
+          process.env.OPENROUTER_BACKUP_KEY_2,
+          'OPENROUTER'
+        )
+      },
+      google: {
+        enabled: process.env.ENABLE_GOOGLE !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.GOOGLE_API_KEY,
+          process.env.GOOGLE_BACKUP_KEY_1,
+          process.env.GOOGLE_BACKUP_KEY_2,
+          'GOOGLE'
+        )
+      },
+      groq: {
+        enabled: process.env.ENABLE_GROQ !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.GROQ_API_KEY,
+          process.env.GROQ_BACKUP_KEY_1,
+          process.env.GROQ_BACKUP_KEY_2,
+          'GROQ'
+        )
+      },
+      glama: {
+        enabled: process.env.ENABLE_GLAMA !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.GLAMA_API_KEY,
+          process.env.GLAMA_BACKUP_KEY_1,
+          process.env.GLAMA_BACKUP_KEY_2,
+          'GLAMA'
+        )
+      },
+      huggingchat: {
+        enabled: process.env.ENABLE_HUGGINGCHAT !== 'false',
+        email: process.env.HUGGINGFACE_EMAIL?.trim(),
+        password: process.env.HUGGINGFACE_PASSWORD?.trim()
+      },
+      hackclub: {
+        enabled: process.env.ENABLE_HACKCLUB !== 'false'
+      },
+      voids: {
+        enabled: process.env.ENABLE_VOIDS !== 'false'
+      },
+      openai: {
+        enabled: process.env.ENABLE_OPENAI !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.OPENAI_API_KEY,
+          process.env.OPENAI_BACKUP_KEY_1,
+          process.env.OPENAI_BACKUP_KEY_2,
+          'OPENAI'
+        )
+      },
+      claude: {
+        enabled: process.env.ENABLE_CLAUDE !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.ANTHROPIC_API_KEY,
+          process.env.ANTHROPIC_BACKUP_KEY_1,
+          process.env.ANTHROPIC_BACKUP_KEY_2,
+          'ANTHROPIC'
+        )
+      },
+      deepseek: {
+        enabled: process.env.ENABLE_DEEPSEEK !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.DEEPSEEK_API_KEY,
+          process.env.DEEPSEEK_BACKUP_KEY_1,
+          process.env.DEEPSEEK_BACKUP_KEY_2,
+          'DEEPSEEK'
+        )
+      },
+      together: {
+        enabled: process.env.ENABLE_TOGETHER !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.TOGETHER_API_KEY,
+          process.env.TOGETHER_BACKUP_KEY_1,
+          process.env.TOGETHER_BACKUP_KEY_2,
+          'TOGETHER'
+        )
+      },
+      perplexity: {
+        enabled: process.env.ENABLE_PERPLEXITY !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.PERPLEXITY_API_KEY,
+          process.env.PERPLEXITY_BACKUP_KEY_1,
+          process.env.PERPLEXITY_BACKUP_KEY_2,
+          'PERPLEXITY'
+        )
+      },
+      cerebras: {
+        enabled: process.env.ENABLE_CEREBRAS !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.CEREBRAS_API_KEY,
+          process.env.CEREBRAS_BACKUP_KEY_1,
+          process.env.CEREBRAS_BACKUP_KEY_2,
+          'CEREBRAS'
+        )
+      },
+      fireworks: {
+        enabled: process.env.ENABLE_FIREWORKS !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.FIREWORKS_API_KEY,
+          process.env.FIREWORKS_BACKUP_KEY_1,
+          process.env.FIREWORKS_BACKUP_KEY_2,
+          'FIREWORKS'
+        )
+      },
+      mistral: {
+        enabled: process.env.ENABLE_MISTRAL !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.MISTRAL_API_KEY,
+          process.env.MISTRAL_BACKUP_KEY_1,
+          process.env.MISTRAL_BACKUP_KEY_2,
+          'MISTRAL'
+        )
+      },
+      deepinfra: {
+        enabled: process.env.ENABLE_DEEPINFRA !== 'false',
+        apiKeys: validateApiKeySet(
+          process.env.DEEPINFRA_API_KEY,
+          process.env.DEEPINFRA_BACKUP_KEY_1,
+          process.env.DEEPINFRA_BACKUP_KEY_2,
+          'DEEPINFRA'
+        )
+      }
+    },
+
+    // Security Settings
+    trustProxy: process.env.TRUST_PROXY === 'true',
+    allowedOrigins: process.env.ALLOWED_ORIGINS?.split(',') || ['*'],
+    maintenanceMode: process.env.MAINTENANCE_MODE === 'true',
+    maintenanceMessage: process.env.MAINTENANCE_MESSAGE || 'System is down for maintenance',
+
+    // Rate Limiting
+    rateLimit: {
+      enabled: process.env.DISABLE_RATE_LIMIT !== 'true',
+      requests: parseInt(process.env.RATE_LIMIT_REQUESTS) || 50,
+      window: parseInt(process.env.RATE_LIMIT_WINDOW) || 60000,
+      type: process.env.RATE_LIMIT_TYPE || 'ip'
+    },
+
+    // Size Limits
+    maxRequestSize: process.env.MAX_REQUEST_SIZE || '50mb',
+    maxHeaderSize: process.env.MAX_HEADER_SIZE || '10kb',
+    maxPayloadSize: process.env.MAX_PAYLOAD_SIZE || '50mb',
+
+    // Feature Flags
+    debugMode: process.env.DEBUG_MODE === 'true',
+    logging: {
+      enabled: process.env.ENABLE_LOGGING !== 'false',
+      requests: process.env.LOG_REQUESTS !== 'false',
+      responses: process.env.LOG_RESPONSES !== 'false',
+      errors: process.env.LOG_ERRORS !== 'false'
+    },
+    stats: {
+      enabled: process.env.ENABLE_STATS !== 'false'
+    },
+
+    // Performance
+    requestTimeout: parseInt(process.env.REQUEST_TIMEOUT) || 30000,
+    maxConcurrent: parseInt(process.env.MAX_CONCURRENT) || 100,
+    cacheDuration: parseInt(process.env.CACHE_DURATION) || 300,
+
+    // Health Check Configuration
+    health: {
+      enabled: process.env.ENABLE_HEALTH_CHECKS !== 'false',
+      delay: parseInt(process.env.HEALTH_CHECK_DELAY) || 2600,
+      interval: parseInt(process.env.HEALTH_CHECK_INTERVAL) || 7200,
+      timeout: parseInt(process.env.HEALTH_CHECK_TIMEOUT) || 30000,
+      retries: parseInt(process.env.HEALTH_CHECK_RETRIES) || 2
+    },
+
+    // Monitoring
+    metrics: {
+      enabled: process.env.ENABLE_METRICS === 'true',
+      port: parseInt(process.env.METRICS_PORT) || 9090
+    },
+
+    // Defaults
+    defaults: {
+      model: process.env.DEFAULT_MODEL === 'false' ? null : (process.env.DEFAULT_MODEL || 'hackclub/llama-3.3-70b-versatile'),
+      temperature: process.env.DEFAULT_TEMPERATURE?.toLowerCase() === 'def' ||
+                  process.env.DEFAULT_TEMPERATURE?.toLowerCase() === 'default' ? null :
+                  parseFloat(process.env.DEFAULT_TEMPERATURE) || 0.7,
+      maxTokens: process.env.MAX_TOKENS?.toLowerCase() === 'inf' ||
+                process.env.MAX_TOKENS?.toLowerCase() === 'infinite' ? null :
+                parseInt(process.env.MAX_TOKENS) || 8192
+    }
+  };
+
+  console.log('ðŸ” Checking provider configurations...');
+
+  // Validate each provider's configuration
+  Object.entries(config.providers).forEach(([name, provider]) => {
+    // Skip providers that are explicitly disabled
+    if (process.env[`ENABLE_${name.toUpperCase()}`] === 'false') {
+      console.log(`ðŸ‘Ž Provider ${name} disabled by config`);
+      provider.enabled = false;
+      return;
+    }
+
+    // Validate enabled providers
+    if (name === 'huggingchat') {
+      if (!provider.email || !provider.password) {
+        provider.enabled = false;
+        console.log(`âŒ Provider ${name} missing credentials`);
+      } else {
+        console.log(`âœ… Provider ${name} credentials found`);
+      }
+    } else if (!['hackclub', 'voids'].includes(name)) {
+      if (!provider.apiKeys?.length) {
+        provider.enabled = false;
+        console.log(`âŒ Provider ${name} missing API key(s)`);
+      } else {
+        console.log(`âœ… Provider ${name} has ${provider.apiKeys.length} API key(s)`);
+      }
+    }
+  });
+
+  // Validate Authentication
+  if (process.env.API_KEYS_FILE) {
+    const keys = readApiKeysFromFile(process.env.API_KEYS_FILE);
+    if (keys.length > 0) {
+      config.apiKeys = keys;
+      // if API_KEY is also set, add it to the list
+      if (config.apiKey) {
+        config.apiKeys.push(config.apiKey);
+      }
+      config.apiKey = null; // we're using the keys array instead
+    } else {
+      warnConfig('API_KEYS_FILE specified but no valid keys found');
+    }
+  }
+
+  if (config.requireApiKey && !config.apiKey && config.apiKeys.length === 0) {
+    warnConfig('Neither API_KEY nor valid API_KEYS_FILE provided');
+  }
+
+  if (!config.adminKey) {
+    warnConfig('ADMIN_KEY not set - admin endpoints will be disabled');
+  }
+
+  // Parse Size Limits
+  config.parsedMaxRequestSize = getSizeLimit(config.maxRequestSize, 50 * 1024 * 1024);
+  config.parsedMaxHeaderSize = getSizeLimit(config.maxHeaderSize, 10 * 1024);
+  config.parsedMaxPayloadSize = getSizeLimit(config.maxPayloadSize, 50 * 1024 * 1024);
+
+  // Validate Performance Settings
+  if (config.requestTimeout < 1000) {
+    warnConfig('Request timeout too low, setting to 1000ms minimum');
+    config.requestTimeout = 1000;
+  }
+  if (config.maxConcurrent < 1) {
+    warnConfig('Max concurrent requests too low, setting to 1 minimum');
+    config.maxConcurrent = 1;
+  }
+
+  // Create Required Directories
+  if (config.logging.enabled) {
+    const logsDir = path.join(__dirname, '../../logs');
+    if (!fs.existsSync(logsDir)) {
+      try {
+        fs.mkdirSync(logsDir, { recursive: true });
+      } catch (error) {
+        warnConfig(`Failed to create logs directory: ${error.message}`);
+        config.logging.enabled = false;
+      }
+    }
+  }
+
+  if (config.stats.enabled) {
+    const dataDir = path.join(__dirname, '../../data');
+    if (!fs.existsSync(dataDir)) {
+      try {
+        fs.mkdirSync(dataDir, { recursive: true });
+      } catch (error) {
+        warnConfig(`Failed to create data directory: ${error.message}`);
+        config.stats.enabled = false;
+      }
+    }
+  }
+
+  // Validate Health Check Configuration
+  if (config.health.enabled) {
+    if (config.health.interval < 60) {
+      warnConfig('Health check interval too low, setting to 60s minimum');
+      config.health.interval = 60;
+    }
+    if (config.health.delay < 5000) {
+      warnConfig('Health check delay too low, setting to 5000ms minimum');
+      config.health.delay = 5000;
+    }
+    if (config.health.timeout < 5000) {
+      warnConfig('Health check timeout too low, setting to 5000ms minimum');
+      config.health.timeout = 5000;
+    }
+    if (config.health.retries < 1) {
+      warnConfig('Health check retries too low, setting to 1 minimum');
+      config.health.retries = 1;
+    }
+  }
+
+  console.log('âœ¨ Environment configuration validated');
+  return config;
+}
+
+function initialize() {
+  try {
+    const config = validateEnvironment();
+    
+    // Set validated config values back to process.env
+    process.env.PORT = process.env.PORT || '3000';
+    process.env.RATE_LIMIT_REQUESTS = config.rateLimit.requests.toString();
+    process.env.RATE_LIMIT_WINDOW = config.rateLimit.window.toString();
+    process.env.DEBUG_MODE = config.debugMode.toString();
+    process.env.ENABLE_LOGGING = config.logging.enabled.toString();
+    process.env.ENABLE_STATS = config.stats.enabled.toString();
+    process.env.ENABLE_HEALTH_CHECKS = config.health.enabled.toString();
+
+    return { warnings, warningMessages, config };
+  } catch (error) {
+    console.error('âŒ ERROR:', error.message);
+    process.exit(1);
+  }
+}
+
+export {
+  initialize,
+  validateEnvironment,
+  getDefaultRateLimit,
+  getSizeLimit,
+  readApiKeysFromFile
+};
\ No newline at end of file
diff --git a/src/utils/health.js b/src/utils/health.js
new file mode 100644
index 0000000..9a70dd9
--- /dev/null
+++ b/src/utils/health.js
@@ -0,0 +1,350 @@
+// health check utilities - sequential model testing with proper delays
+import providers from '../providers/index.js';
+import logger from './logger.js';
+
+class HealthCheck {
+    constructor() {
+        this.models = new Map();
+        this.lastCheck = 0;
+        this.checkIntervalId = null;
+        
+        // concurrency control
+        this.isChecking = false;
+        this.checkQueue = [];
+    }
+
+    getRandomPrompt() {
+        const prompts = [
+            "If you had to choose, what's the worst way to eat a banana?",
+
+            "How many pancakes would it take to build a ladder to the moon?",
+        
+            "What would happen if you tried to fry ice?",
+        
+            "Can you explain the concept of time using only emojis?",
+        
+            "If colors could scream, which one would be the loudest?",
+        
+            "How would you describe a smartphone to a medieval knight?",
+        
+            "What's the most useless superpower you can think of?",
+        
+            "If animals could talk, which would be the rudest?",
+        
+            "How many chickens would it take to win a fight against a lion?",
+        
+            "What would happen if you replaced all the water in the ocean with ketchup?",
+        
+            "If you could only eat 5 food2 for the rest of your life, but it had to be a crayon color, which would you pick?",
+        
+            "How would you survive a zombie apocalypse using only office supplies?",
+        
+            "What's the weirdest way to greet someone?",
+        
+            "If you could turn any object into a musical instrument, what would it be and why?",
+        
+            "How would you explain the internet to a caveman?",
+        
+            "What would happen if gravity stopped working for 5 seconds?",
+        
+            "If you could combine any two animals, what would be the most ridiculous combo?",
+        
+            "How many spoons of sugar does it take to make a coffee undrinkable?",
+        
+            "What's the most absurd way to propose to someone?",
+        
+            "If you could only communicate using movie quotes, which movie would you choose?",
+        
+            "How would you describe a rainbow to a blind alien?",
+        
+            "What's the worst possible name for a pet goldfish?",
+        
+            "If you had to replace your hands with everyday objects, what would you pick?",
+        
+            "How would you convince a cat to take a bath?",
+        
+            "What's the most unnecessary invention you can think of?",
+        
+            "If you could only wear 5 outfits for the rest of your life, what would it be?",
+        
+            "How would you explain memes to someone from the 1800s?",
+        
+            "What would happen if you tried to microwave a microwave?",
+        
+            "If you could make any food spicy by default, which would it be?",
+        
+            "How would you survive in the wilderness with only a roll of duct tape?",
+        
+            "What's the weirdest thing you could say to a stranger on an elevator?",
+        
+            "If you had to replace all the trees with something else, what would it be?",
+        
+            "How would you describe a smartphone to a potato?",
+        
+            "What's the most confusing way to give directions?",
+        
+            "If you could only listen to five songs for the rest of your life, but it had to be sung by a goat, which would it be?",
+        
+            "How would you explain TikTok to your great-grandparents?",
+        
+            "What's the worst possible pizza topping combination?",
+        
+            "If you could turn any body part into a kitchen appliance, what would it be?",
+        
+            "How would you teach a robot to dance using only metaphors?",
+        
+            "What's the most ridiculous way to cool down on a hot day?",
+        
+            "If you could only use 5 emojis for the rest of your life, which would it be?",
+        
+            "How would you describe a rainbow to someone who's never seen color?",
+        
+            "What's the weirdest thing you could order at a restaurant?",
+        
+            "If you had to replace all the wheels in the world with something else, what would it be?",
+        
+            "How would you explain a computer virus to a 5-year-old using only toys?",
+        
+            "What's the most pointless rule you could enforce at a workplace?",
+        
+            "If you could make any animal the size of a skyscraper, which would it be?",
+        
+            "How would you describe the taste of water to an alien?",
+        
+            "What's the worst possible way to start a conversation?",
+        
+            "If you could only eat food that's the color blue, what would your diet consist of?"
+        ];
+        return prompts[Math.floor(Math.random() * prompts.length)];
+    }
+
+    validateResponse(response, prompt) {
+        if (!response || typeof response !== 'string') {
+            // Log full response for Google API format errors
+            if (response?.error === 'Invalid response format from Google API') {
+                logger.debug('Full Google API response:', response);
+            }
+            return false;
+        }
+
+        const words = response.trim().split(/\s+/).filter(w => w.length > 0);
+        const minWords = parseInt(process.env.HEALTH_CHECK_MIN_WORDS) || 2;
+        return words.length >= minWords;
+    }
+
+    async testModel(model, attempt = 1) {
+        const maxRetries = parseInt(process.env.HEALTH_CHECK_RETRIES) || 2;
+        const prompt = this.getRandomPrompt();
+        
+        try {
+            const provider = await providers.getProviderForModel(model);
+            if (!provider) {
+                return { status: 'error', error: 'No provider available' };
+            }
+
+            const startTime = Date.now();
+            const response = await provider.chat([{
+                role: 'user',
+                content: prompt
+            }], {
+                model,
+                temperature: 0.3,
+                max_tokens: 50
+            });
+            const latency = Date.now() - startTime;
+
+            // Extract content and log full response for debugging
+            let content = '';
+            
+            if (response.choices?.[0]?.message?.content) {
+                content = response.choices[0].message.content;
+            } else if (response.choices?.[0]?.delta?.content) {
+                content = response.choices[0].delta.content;
+            } else if (typeof response === 'string') {
+                content = response;
+            } else {
+                logger.debug(`Invalid response format from ${model}:`, {
+                    type: typeof response,
+                    structure: response ? Object.keys(response) : 'null',
+                    raw: response
+                });
+            }
+
+            if (!this.validateResponse(content, prompt)) {
+                // Log the invalid response for debugging
+                logger.debug(`Model ${model} invalid response (attempt ${attempt}/${maxRetries}):`, {
+                    prompt,
+                    response: content || 'null',
+                    latency
+                });
+
+                // Retry if we haven't hit the limit
+                if (attempt < maxRetries) {
+                    logger.debug(`Retrying ${model}...`);
+                    await new Promise(resolve => setTimeout(resolve, 5000)); // Wait 5s before retry
+                    return this.testModel(model, attempt + 1);
+                }
+                
+                return {
+                    status: 'error',
+                    latency,
+                    error: `Response invalid: "${content || 'null'}" (${typeof content})`,
+                    prompt,
+                    response: content,
+                    attempts: attempt,
+                    rawResponse: response
+                };
+            }
+
+            logger.debug(`Model ${model} healthy (attempt ${attempt}/${maxRetries}):`, {
+                latency,
+                response: content.slice(0, 100) + (content.length > 100 ? '...' : ''),
+                prompt
+            });
+
+            return {
+                status: 'operational',
+                latency,
+                error: null,
+                response: content,
+                attempts: attempt
+            };
+
+        } catch (error) {
+            // Retry on potentially temporary errors
+            if (attempt < maxRetries && (
+                error.message.includes('timeout') ||
+                error.message.includes('rate limit') ||
+                error.status === 429 ||
+                error.status === 500 ||
+                error.status === 502 ||
+                error.status === 503 ||
+                error.status === 504
+            )) {
+                logger.debug(`Model ${model} error (attempt ${attempt}/${maxRetries}), retrying...`, {
+                    error: error.message,
+                    status: error.status
+                });
+                await new Promise(resolve => setTimeout(resolve, 5000)); // Wait 5s before retry
+                return this.testModel(model, attempt + 1);
+            }
+
+            const status = error.message === 'Timeout' ? 'unknown' : 'error';
+            const statusCode = error.status || error.statusCode || error.code;
+            
+            // Log full error details for debugging
+            logger.debug(`Model ${model} failed after ${attempt} attempts:`, {
+                error: error.message,
+                status: statusCode,
+                response: error.response || null,
+                details: error.details || null,
+                prompt: prompt,
+                rawError: typeof error === 'object' ? JSON.stringify(error, null, 2) : error
+            });
+            return {
+                status,
+                latency: null,
+                error: `${statusCode ? `[${statusCode}] ` : ''}${error.message}`,
+                prompt,
+                attempts: attempt
+            };
+        }
+    }
+
+    getStatus() {
+        return Array.from(this.models.entries()).map(([model, data]) => ({
+            model,
+            ...data
+        }));
+    }
+
+    async checkAllModels(callback) {
+        // dont start a new check if one is already running
+        if (this.isChecking) {
+            logger.debug('Health check already in progress, skipping');
+            return;
+        }
+
+        this.isChecking = true;
+        this.lastCheck = Date.now();
+
+        try {
+            const allModels = await providers.getAvailableModels();
+            if (!allModels?.length) {
+                logger.warn('No models available for health check');
+                return;
+            }
+
+            logger.info(`Starting health checks for ${allModels.length} models`);
+            
+            // Process models one at a time with proper delay
+            const delayMs = parseInt(process.env.HEALTH_CHECK_DELAY) || 26000;
+            
+            for (const model of allModels) {
+                if (!model?.id) continue;
+
+                try {
+                    // Wait for delay before testing next model
+                    await new Promise(resolve => setTimeout(resolve, delayMs));
+
+                    const result = await this.testModel(model.id);
+                    this.models.set(model.id, {
+                        ...result,
+                        lastCheck: Date.now()
+                    });
+
+                    // Log the result with attempt count
+                    const status = result.status === 'operational' ? 'healthy' : 'unhealthy';
+                    logger.debug(`Model ${model.id} ${status} after ${result.attempts} attempt(s) - latency: ${result.latency}ms`);
+
+                    if (callback) {
+                        callback(model.id, result);
+                    }
+                } catch (error) {
+                    logger.debug(`Health check failed for ${model.id}:`, {
+                        error: error.message,
+                        stack: error.stack,
+                        status: error.status || error.code,
+                        response: error.response || null,
+                        details: error.details || null,
+                        rawError: typeof error === 'object' ? JSON.stringify(error, null, 2) : error
+                    });
+                }
+            }
+
+            logger.info('Health check cycle completed');
+        } catch (error) {
+            logger.error('Health check cycle failed:', error);
+        } finally {
+            this.isChecking = false;
+        }
+    }
+
+    stopHealthChecks() {
+        if (this.checkIntervalId) {
+            clearInterval(this.checkIntervalId);
+            this.checkIntervalId = null;
+            logger.info('Health checks stopped');
+        }
+    }
+
+    startHealthChecks(callback) {
+        if (this.isChecking) return;
+        
+        this.stopHealthChecks();
+        
+        // Get interval from env (in seconds) and convert to milliseconds
+        const intervalSecs = parseInt(process.env.HEALTH_CHECK_INTERVAL) || 7200;
+        const intervalMs = intervalSecs * 1000;
+        
+        // Run initial check
+        this.checkAllModels(callback);
+
+        // Set up recurring checks
+        this.checkIntervalId = setInterval(() => {
+            this.checkAllModels(callback);
+        }, intervalMs);
+    }
+}
+
+export default new HealthCheck();
diff --git a/src/utils/index.js b/src/utils/index.js
new file mode 100644
index 0000000..bfc9498
--- /dev/null
+++ b/src/utils/index.js
@@ -0,0 +1,4 @@
+// utils/index.js - all the random helper stuff packaged up nice
+export { logMessage, getLogDates, getLogs } from './logger.js';
+export { trackInteraction, getStats } from './stats.js';
+export { initialize, validateEnvironment, readApiKeysFromFile } from './bootstrap.js';
\ No newline at end of file
diff --git a/src/utils/logger.js b/src/utils/logger.js
new file mode 100644
index 0000000..bb18e6a
--- /dev/null
+++ b/src/utils/logger.js
@@ -0,0 +1,100 @@
+// Logger utility with color support and consistent formatting
+import chalk from 'chalk';
+import fs from 'fs';
+import path from 'path';
+
+class Logger {
+    constructor() {
+        // separate debug mode from debug logging
+        this.DEBUG = process.env.DEBUG_MODE === 'true';
+        this.DEBUG_LOGGING = process.env.DEBUG_LOGGING !== 'false';
+        
+        // colors for different log levels
+        this.colors = {
+            debug: chalk.blue,
+            info: chalk.green,
+            warn: chalk.yellow,
+            error: chalk.red
+        };
+
+        // ensure logs directory exists
+        this.logsDir = path.join(process.cwd(), 'logs');
+        if (!fs.existsSync(this.logsDir)) {
+            fs.mkdirSync(this.logsDir, { recursive: true });
+        }
+    }
+
+    format(level, message, data = {}, forFile = false) {
+        const timestamp = new Date().toISOString();
+        const prefix = forFile ?
+            `[${level.toUpperCase()}]` :
+            this.colors[level](`[${level.toUpperCase()}]`);
+
+        // Format data if present
+        const details = Object.keys(data).length > 0
+            ? '\n' + JSON.stringify(data, null, 2).split('\n').map(line => '  ' + line).join('\n')
+            : '';
+
+        // for file logging we want the timestamp
+        return forFile ?
+            `${timestamp} ${prefix} ${message}${details}` :
+            `${prefix} ${message}${details}`;
+    }
+
+    // write to log file
+    async writeToFile(level, message, data = {}) {
+        try {
+            const logFile = path.join(this.logsDir, `${level}.log`);
+            const logMessage = this.format(level, message, data, true) + '\n';
+            await fs.promises.appendFile(logFile, logMessage);
+        } catch (error) {
+            console.error('Failed to write to log file:', error);
+        }
+    }
+
+    debug(message, data = {}) {
+        // always write debug logs to file
+        this.writeToFile('debug', message, data);
+        
+        // only show in terminal if debug logging is enabled
+        if (this.DEBUG_LOGGING) {
+            console.log(this.format('debug', message, data));
+        }
+    }
+
+    info(message, data = {}) {
+        this.writeToFile('info', message, data);
+        console.log(this.format('info', message, data));
+    }
+
+    warn(message, data = {}) {
+        this.writeToFile('warn', message, data);
+        console.warn(this.format('warn', message, data));
+    }
+
+    error(message, data = {}) {
+        this.writeToFile('error', message, data);
+        console.error(this.format('error', message, data));
+    }
+
+    // Clean startup logging
+    startup(component, status, data = {}) {
+        const message = `Starting ${component}...`;
+        
+        // write to startup.log
+        this.writeToFile('startup', message, { status, ...data });
+        
+        if (status === 'success') {
+            this.info(`${message} OK`, data);
+        } else if (status === 'warning') {
+            this.warn(`${message} WARNING`, data);
+        } else if (status === 'error') {
+            this.error(`${message} FAILED`, data);
+        } else {
+            this.info(message, data);
+        }
+    }
+}
+
+// create a singleton instance
+export default new Logger();
\ No newline at end of file
diff --git a/src/utils/stats.js b/src/utils/stats.js
new file mode 100644
index 0000000..6a67c56
--- /dev/null
+++ b/src/utils/stats.js
@@ -0,0 +1,160 @@
+// stats.js - tracking stuff without going crazy
+import fs from 'fs';
+import path from 'path';
+import { fileURLToPath } from 'url';
+
+// setup dirname for esm
+const __filename = fileURLToPath(import.meta.url);
+const __dirname = path.dirname(__filename);
+
+// stats file location
+const statsFile = path.join(__dirname, '../../data/stats.json');
+
+// make sure data directory exists
+const dataDir = path.dirname(statsFile);
+if (!fs.existsSync(dataDir)) {
+  fs.mkdirSync(dataDir, { recursive: true });
+}
+
+// initialize stats object
+let stats = {
+  startTime: Date.now(),
+  totalRequests: 0,
+  successfulRequests: 0,
+  failedRequests: 0,
+  providers: {}
+};
+
+// load stats if they exist
+try {
+  if (fs.existsSync(statsFile)) {
+    const data = fs.readFileSync(statsFile, 'utf8');
+    stats = JSON.parse(data);
+    // always update start time on load
+    stats.startTime = Date.now();
+  }
+} catch (error) {
+  console.error('failed to load stats:', error);
+}
+
+// save stats to disk
+const saveStats = () => {
+  if (process.env.ENABLE_STATS !== 'true') return;
+  
+  try {
+    fs.writeFileSync(statsFile, JSON.stringify(stats, null, 2));
+  } catch (error) {
+    console.error('failed to save stats:', error);
+  }
+};
+
+// auto-save every 5 minutes
+const SAVE_INTERVAL = 5 * 60 * 1000;
+setInterval(saveStats, SAVE_INTERVAL);
+
+// save on process exit
+process.on('SIGINT', () => {
+  saveStats();
+  process.exit();
+});
+
+process.on('SIGTERM', () => {
+  saveStats();
+  process.exit();
+});
+
+// track api interaction
+export const trackInteraction = async ({
+  requestId,
+  provider,
+  model,
+  messages,
+  response,
+  error,
+  startTime
+}) => {
+  if (process.env.ENABLE_STATS !== 'true') return;
+
+  stats.totalRequests++;
+
+  // initialize provider stats
+  if (!stats.providers[provider]) {
+    stats.providers[provider] = {
+      totalRequests: 0,
+      successfulRequests: 0,
+      failedRequests: 0,
+      models: {},
+      lastRequest: null,
+      averageLatency: 0
+    };
+  }
+
+  // initialize model stats
+  if (!stats.providers[provider].models[model]) {
+    stats.providers[provider].models[model] = {
+      totalRequests: 0,
+      successfulRequests: 0,
+      failedRequests: 0,
+      totalTokens: 0,
+      averageLatency: 0,
+      lastRequest: null
+    };
+  }
+
+  const providerStats = stats.providers[provider];
+  const modelStats = providerStats.models[model];
+
+  providerStats.totalRequests++;
+  modelStats.totalRequests++;
+
+  // calculate latency
+  const latency = Date.now() - startTime;
+  modelStats.averageLatency = (modelStats.averageLatency * (modelStats.totalRequests - 1) + latency) / modelStats.totalRequests;
+  providerStats.averageLatency = (providerStats.averageLatency * (providerStats.totalRequests - 1) + latency) / providerStats.totalRequests;
+
+  if (error) {
+    stats.failedRequests++;
+    providerStats.failedRequests++;
+    modelStats.failedRequests++;
+  } else {
+    stats.successfulRequests++;
+    providerStats.successfulRequests++;
+    modelStats.successfulRequests++;
+
+    // try to get token counts
+    if (response?.usage?.total_tokens) {
+      modelStats.totalTokens += response.usage.total_tokens;
+    }
+  }
+
+  // update timestamps
+  const now = Date.now();
+  providerStats.lastRequest = now;
+  modelStats.lastRequest = now;
+
+  // save after each interaction
+  saveStats();
+};
+
+// get current stats
+export const getStats = () => {
+  if (process.env.ENABLE_STATS !== 'true') {
+    return { enabled: false };
+  }
+
+  const uptime = Date.now() - stats.startTime;
+
+  return {
+    enabled: true,
+    uptime,
+    stats: {
+      ...stats,
+      successRate: stats.totalRequests > 0
+        ? (stats.successfulRequests / stats.totalRequests * 100).toFixed(2) + '%'
+        : '0%',
+      requestsPerMinute: stats.totalRequests > 0
+        ? ((stats.totalRequests / uptime) * 60000).toFixed(2)
+        : '0'
+    }
+  };
+};
\ No newline at end of file
diff --git a/vercel.json b/vercel.json
index 5419284..c9d628e 100644
--- a/vercel.json
+++ b/vercel.json
@@ -2,21 +2,19 @@
   "version": 2,
   "builds": [
     {
-      "src": "server.js",
+      "src": "src/server.js",
       "use": "@vercel/node"
     }
   ],
   "routes": [
-    {
-      "src": "/swagger-ui/(.*)",
-      "dest": "/server.js"
-    },
     {
       "src": "/(.*)",
-      "dest": "/server.js"
+      "dest": "src/server.js"
     }
   ],
   "env": {
-    "NODE_ENV": "production"
+    "OPENROUTER_API_KEY": "@openrouter_api_key",
+    "GOOGLE_AI_KEY": "@google_ai_key",
+    "DEBUG_MODE": "@debug_mode"
   }
 }
